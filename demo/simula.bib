
@Book{Simula.SE.295,
  editor = {Arisholm, Erik and Briand, Lionel C and Anda, Bente C. D},
  title = {Proceedings of the First Workshop on Empirical Studies of Model-Driven Engineering (ESMDE)},
  year = {2008},
  abstract = {It is often difficult to rigorously evaluate Model-Driven Engineering (MDE) technologies. Performing empirical studies require skills, experience and tacit knowledge that are in many ways very different from the {\textquotedblleft}core{\textquotedblright} MDE research. Furthermore, empirical studies often entail large investments in terms of human resources, time and money. Nevertheless, evaluations of MDE technologies are needed in order to demonstrate the soundness, applicability, and cost effectiveness of proposed technologies in various development contexts. The aim of this workshop is to exemplify and discuss ways in which proposed modeldriven engineering (MDE) technologies should be evaluated, with a specific emphasis on how to plan, conduct, analyze and report the results of empirical studies. The workshop will have focus on the challenges of empirical studies involving human users, since MDE technologies are typically expected to be used by software engineers to improve various quality aspects of software systems and the productivity of software development. More detailed topics include: What are the main obstacles and potential remedies when performing empirical studies of MDE? What are the main threats to validity of empirical studies of MDE, and how should they be dealt with? For example, since MDE often represent new and complex technology, the selection and training of human subjects who participate in empirical studies often become critical factors. What are the most important outcome variables of the costs and benefits of MDE? How can quality be measured in the context of MDE? And can we define an unambiguous set of (benchmark) outcome measures to facilitate metaanalyses across subjects, systems, tasks and technologies? The goal of the workshop is to pave the way for the development of a MDE-specific framework for empirical evaluation of MDE technologies, or at least provide a minimum standard for evaluation that published work in the MDE community should abide by.},
  publisher = {http://CEUR-WS.org/Vol-392},
  volume = {392},
  isbn = {1613-0073}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2004.a,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M and Tanilkan, S S and Gallis, H and Lien, A C and Hove, S E},
  title = {A Survey on Software Estimation in the Norwegian Industry},
  year = {2004},
  abstract = {This paper seeks to provide an overview of the estimation methods that software companies apply to estimate their projects, why those methods are chosen, and how accurate they are. In order to improve software estimation accuracy, such knowledge is essential. We conducted an in-depth survey, where information was collected through structured interviews with senior managers from 18 different companies and project managers of 52 different projects. We collected and analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, completeness of delivered functionality and other estimation related information. Our results suggest, for example, that average effort overruns are 41\%, that the software estimation performance has not changed much the last 10-20 years, that expert estimation is the dominating estimation method, that estimation accuracy is not much impacted by use of formal estimation models, and that software managers tend to believe that the estimation accuracy of their company is better than it actually is.},
  booktitle = {10th International Software Metrics Symposium (METRICS 2004)},
  pages = {208--219},
  publisher = {IEEE Computer Society Press},
  address = {Chicago, USA}
}

@Inproceedings{SE.5.Li.2004.a,
  author = {Li, J and Bj{\o}rnson, F O and Conradi, R and Kampenes, V B},
  title = {An empirical Study of Variations in COTS-based Software Development Processes in Norwegian IT Industry},
  year = {2004},
  abstract = {More and more software projects use Commercial- 
Off-The-Shelf (COTS) software. Although previous 
studies have proposed specific COTS-based development 
processes, there are few empirical studies to investigate 
how to use and customize them to different project 
contexts. This paper describes an exploratory study of 
state-of-the-practice of COTS-based development 
processes. 16 software projects in Norwegian IT 
companies have been studied by structured interviews. 
The results are that COTS-specific activities can be 
successfully incorporated in most traditional development 
processes (such as waterfall or incremental), given 
proper guidelines to reduce risks and provide specific 
assistance. We have identified three COTS-specific 
activities {\textendash} the build vs. buy decision, COTS component 
selection, and COTS component integration {\textendash} and one 
new role, that of a knowledge keeper. We have also found 
a special COTS component selection activity for 
unfamiliar components, combining Internet searches with 
hands-on experimentation. The process guidelines are 
expressed as scenarios and lessons learned, and can be 
used to customize the actual development processes, e.g. 
in which lifecycle phase to put the new activities. Such 
customization crucially depends on project context, such 
as previous familiarity with possible COTS components 
and flexibility of requirements.},
  booktitle = {Proceedings of 10th IEEE International Metrics Symposium (Metrics'04)},
  pages = {72--83},
  publisher = {-},
  address = {Chicago, USA}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2004.b,
  author = {Mol{\o}kken-{\O}stvold, K J and Lien, A C and J{\o}rgensen, M and Tanilkan, S S and Gallis, H and Hove, S E},
  title = {Does Use of Development Model Affect Estimation Accuracy and Bias?},
  year = {2004},
  abstract = {Objective. To investigate how the use of incremental and evolutionary development models affects the accuracy and bias of effort and schedule estimates of software projects. Rationale. Advocates of incremental and evolutionary development models often claim that use of these models results in improved estimation accuracy. Design of study. We conducted an in-depth survey, where information was collected through structured interviews with 22 software project managers in 10 different companies. We collected and analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, completeness of delivered functionality and other estimation related information. Results. We found no impact from the development model on the estimation approach. However, we found that incremental and evolutionary projects were less prone to effort overruns. The degree of delivered functionality and schedule estimation accuracy, on the other hand, were seemingly independent of development model. Conclusion. The use of incremental and evolutionary development models may reduce the chance of effort overruns.},
  booktitle = {Product Focused Software Process                      Improvement: 5th International Conference                      (PROFES 2004)},
  pages = {17--29},
  publisher = {Springer-Verlag},
  address = {Kansai Science City, Japan},
  series = {Lecture Notes in Computer Science},
  note = {ISBN: 3-540-21421-6}
}

@Inproceedings{SE.5s.Mohagheghi.2004,
  author = {Mohagheghi, P and Conradi, R and Killi, O M and Schwarz, H},
  title = {An Empirical Study of Software Reuse vs. Defect-Density and Stability},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the International Conference on Software Engineering (ICSE'04)},
  pages = {282--292},
  publisher = {IEEE Computer Society Press},
  address = {Edinburgh, Scotland, UK}
}

@Inproceedings{SE.5.Kitchenham.2004,
  author = {Kitchenham, B and Dyb{\r a}, T and J{\o}rgensen, M},
  title = {Evidence-based Software Engineering},
  year = {2004},
  abstract = {Objective: Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. 
Method: We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). 
Results: EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. 
Conclusions: Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
  booktitle = {International Conference on Software Engineering (ICSE'04)},
  pages = {273--281},
  publisher = {IEEE Computer Society, Washington DC, USA},
  address = {Edinburgh}
}

@Inproceedings{SE.5.Joergensen.2004.b,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Eliminating Over-Confidence in Software Development Effort Estimates},
  year = {2004},
  abstract = {Minimum-maximum effort intervals are applied in the planning of
software development projects in order to, among other things, determine the
contingency buffer. Several studies suggest that judgment-based minimummaximum
intervals are based on a systematic over-confidence in the accuracy
of the effort estimates. In this paper, we investigate whether the possession by
estimators of information about previous estimation error for similar projects
reduces this over-confidence. Nineteen realistically composed estimation teams
provided minimum-maximum effort intervals for the same software project.
Ten of the teams (Group A) received no instructions about the uncertainty
assessment process. The remaining nine teams (Group B) were instructed to
begin the minimum-maximum effort interval assessment by recalling the
distribution of estimation error for similar projects. We found that the recall of
the error distribution of the Group B teams did have an impact, but mainly on
the assessment of the estimated minimum effort, not on the maximum effort.
We discuss reasons for this finding and provide recommendations.},
  booktitle = {Conference on Product Focused Software                      Process Improvement},
  pages = {174--184},
  publisher = {Springer-Verlag},
  address = {Japan},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Mohagheghi.2004,
  author = {Mohagheghi, P and Conradi, R},
  title = {Mining of Software Repositories in Empirical Research},
  year = {2004},
  abstract = {},
  booktitle = {8th Workshop on Quantitative Approaches in                      Object-Oriented Software Engineering (QAOOSE)},
  publisher = {-}
}

@Inproceedings{SE.5.Joergensen.2004.c,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Generalization and Theory-Building in Software Engineering Research},
  year = {2004},
  abstract = {The main purpose of this paper is to generate discussions
which may improve how we conduct empirical software
engineering studies. Our position is that statistical
hypothesis testing plays a too large role in empirical
software engineering studies. The problems of applying
statistical hypothesis testing in empirical software
engineering studies is illustrated by the finding: Only 3
out of the 47 studies in Journal of Empirical Software
Engineering which applied statistical hypothesis testing,
were able to base their statistical testing on well-defined
populations and random samples from those populations.
The frequent use of statistical hypothesis testing may also
have had unwanted consequences on the study designs,
e.g., it may have contributed to a too low focus on theory
building. We outline several steps we believe are useful
for a change in focus from {\textquotedblleft}generalizing from a random
sample to a larger population{\textquotedblright} to {\textquotedblleft}generalizing across
populations through theory-building{\textquotedblright}},
  booktitle = {Empirical Assessment in Software Engineering (EASE2004)},
  editor = {unknown},
  pages = {29--36},
  publisher = {IEE Proceedings},
  isbn = {0 86341 435 4}
}

@Inproceedings{SE.5.Li.2004,
  author = {Li, J and Conradi, R and Mohagheghi, P and S{\ae}hle, O A and Wang, {\O} and Naalsund, E and Walseth, O A},
  title = {A study of developer attitude to component reuse in three IT companies},
  year = {2004},
  abstract = {},
  booktitle = {Product Focused Software Process                      Improvement: 5th International Conference                      (PROFES 2004)},
  pages = {538--552},
  publisher = {Springer-Verlag},
  address = {Kansai Science City, Japan},
  note = {ISBN: 3-540-21421-6}
}

@Inproceedings{SE.5.Li.2004.b,
  author = {Li, J and Bj{\o}rnson, F O and Conradi, R and Kampenes, V B},
  title = {An Empirical Study on COTS Component Selection Process in Norwegian IT Companies},
  year = {2004},
  abstract = {The use of Commercial-Off-The-Shelf (COTS) 
software has become more and more important in 
software development. In COTS-based development, 
COTS component selection is the most crucial phase. 
Although some selection processes have been proposed, 
empirical studies are necessary to assess these processes. 
This paper describes an exploratory study by structured 
interviews of 16 COTS-based development projects in 
Norwegian IT companies. The results indicate that 
successful COTS component selection can be 
implemented without using formal processes, and projects 
with different contexts may use different selection 
processes. If members in new project has enough 
practical experience with actual COTS components, such 
experience can be the dominant factor in selection. In the 
case of using a new COTS component in the project, 
hands-on experimentation is needed as an effective way on evaluating the component.},
  booktitle = {International workshop on models and processes for the evaluation of COTS component (MPEC'04)},
  publisher = {-},
  address = {Edinburgh}
}

@Techreport{SE.7.Moloekken-Oestvold.2004,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M and Tanilkan, S S and Gallis, H and Lien, A C and Hove, S E},
  title = {Project Estimation in the Norwegian Software Industry -- A Summary},
  year = {2004},
  abstract = {This report provides an overview of the results obtained from a survey on project estimation in Norwegian software companies. The survey was conducted between February and November 2003. The main results are:


{\textbullet} We observed that 76\% of the projects used more effort than estimated, while 19\% used less. The average effort overrun was 41\%. 


{\textbullet} Average effort overrun was 67\% in projects with a public client, compared to an average effort overrun of 21\% for projects with a private client. 


{\textbullet} Projects that used an incremental or evolutionary development approach had an average effort overrun of 24\%, as opposed to the average 55\% overrun for projects that used a waterfall-based development approach. 


The frequency and magnitude of effort overruns found in this survey seems to be similar to results reported from surveys conducted in other countries in the past 20 years. The observed differences in effort overruns between private and public projects may be caused by differences in bidding procedures, level of client involvement or acceptance procedures. 

In order to reduce the risk of effort overruns, software companies should: 


{\textbullet} Focus on analyzing their own estimation performance, and invest in estimation improvement (for instance through experience databases or work breakdown structures). 


{\textbullet} Differentiate risk buffers based on the type of the customer, development approach and the size of the project. 


{\textbullet} Try to establish an {\textquotedblleft}as close as possible{\textquotedblright} dialogue with the customers (e.g. through an incremental development approach). 


All companies should analyse completed projects, in order to benchmark their performance. This allows for improvement efforts to be identified.},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-03},
  note = {Results from the BEST-Pro (Better Estimation
                      of Software Tasks and Process Improvement)
                      survey}
}

@Misc{SE.7.Glattetre.2004,
  author = {Glattetre, J and Vok{\a\'a}c, M},
  title = {Generating service descriptions and code from reusable models},
  year = {2004},
  abstract = {Data models have a longer lifetime than the storage platform or technology they are implemented on at any given time. The models tend to change relatively slowly, and are usually described by some form of platform-in{\textlnot}de{\textlnot}pendent dictionary, though this may be in a proprietary form. It is then desirable to generate platform- and technology specific data definitions and persistence layers from such a description, as technology changes over time. 
We have implemented a code generator in C\# that works using text substitution. It takes a plain-text template as input, and combines it with model data to produce a specific output. Within the generator, the dictionary is represented by a thin object layer. C\# attributes are used to denote the roles of the dictionary objects, and reflection is used to load and activate these objects as substitution tags. Occurrences of the tags in the templates are then replaced with actual values from the dictionary by the generator. In this way, we have successfully generated a C\# persistence layer for a legacy database, using its proprietary dictionary and newly written template files. 
The principle can equally be applied to descriptions of high-level functional services. Service access technology, for instance COM, CORBA or Web Services varies on a shorter timescale than do the services themselves, analogous to the variation of storage technology relative to data models. 
The same code generator has been used to generate service descriptions, based on platform-independent definitions in UML. Rational XDE was used to create the service definitions, and a th{\textlnot}in object layer encapsulated the XDE data model for the code generator. By creating the proper template files, we can generate both WSDL and COM IDL descriptions for our services, and even C\# implementation shells. Platform dependency is thus introduced through the templates, and the results may be reverse-engineered to derive an updated, platform-dependent model if needed. 
This approach achieves a high level of reuse, by concentrating on reusing descriptions instead of concrete, platform-dependent implementations. We have successfully used it as the basis for multiple commercial products within the same product family/line.},
  howpublished = {Practitioner's Report. ECOOP 2004, Oslo, Norway}
}

@Misc{SE.8.Moloekken-Oestvold.2004.a,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Best-Pro estimeringsseminar},
  year = {2004},
  abstract = {},
  howpublished = {Presentasjon for ledere og prosjektledere i norske IT-bedrifter},
  note = {60-70 deltakere. Arrang{\o}r: Simula}
}

@Misc{SE.8.Moloekken-Oestvold.2004.b,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {e-kontaktgruppen},
  year = {2004},
  abstract = {},
  howpublished = {Foredrag for e-kontaktgruppen, med representanter for 6 forskjellige departementer},
  note = {12-15 deltakere. Arrang{\o}r: e-kontaktgruppen}
}

@Misc{SE.8.Moloekken-Oestvold.2004.c,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Steria lederseminar},
  year = {2004},
  abstract = {},
  howpublished = {Seminar for Sterias internasjonale ledergruppe},
  note = {12-15 deltakere. Arrang{\o}r: Steria}
}

@Misc{SE.8.Moloekken-Oestvold.2004.d,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Genovus},
  year = {2004},
  abstract = {},
  howpublished = {Presentasjon for Generas kunder},
  note = {50-60 deltakere. Arrang{\o}r: Genera}
}

@Misc{SE.8.Moloekken-Oestvold.2004.e,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Riksrevisjonen},
  year = {2004},
  abstract = {},
  howpublished = {Presentasjon av v{\r a}r forskning for Riksrevisjonen},
  note = {12-15 deltakere. Arrang{\o}r: Riksrevisjonen}
}

@Misc{SE.8.Moloekken-Oestvold.2004.f,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Effin-seminar},
  year = {2004},
  abstract = {},
  howpublished = {Prosjektledere og softwareutviklere for offentlige IT-prosjekter, spesialister innen Human Computer Interaction, brukerrepresentanter og forskere},
  note = {25-30 deltakere. Arrang{\o}r: SINTEF}
}

@Misc{SE.8.Moloekken-Oestvold.2004.g,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Steria lynkurs},
  year = {2004},
  abstract = {},
  howpublished = {Seminar for Sterias kunder},
  note = {80-90 deltakere. Arrang{\o}r: Steria}
}

@Misc{SE.8.Moloekken-Oestvold.2004.h,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {SPIKE-seminar},
  year = {2004},
  abstract = {},
  howpublished = {Presentasjon for forskere og bedrifter tilknyttet SPIKE},
  note = {30-40 deltakere. Arrang{\o}r: Simula}
}

@Misc{SE.8.Moloekken-Oestvold.2004.i,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {IKT-2010},
  year = {2004},
  abstract = {},
  howpublished = {Presentasjon for deltakere p{\r a} IKT-2010},
  note = {70-80 deltakere. Arrang{\o}r: NFR}
}

@Misc{SE.8.Moloekken-Oestvold.2004.j,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M and S{\o}rgaard, P},
  title = {Offentlig fare},
  year = {2004},
  abstract = {},
  howpublished = {Newspaper article in Computerworld}
}

@Misc{SE.8.Moloekken-Oestvold.2004.k,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {LCSC kick-off},
  year = {2004},
  abstract = {},
  howpublished = {Presentasjon p{\r a} LCSC kick-off},
  note = {15 deltakere. Arrang{\o}r: DNV}
}

@Misc{SE.8.Joergensen.2004.a,
  author = {J{\o}rgensen, M},
  title = {Uncertainty assessments in software development projects},
  year = {2004},
  abstract = {},
  howpublished = {Presentation at: Causality,                      Uncertainty \& Ignorance, 3rd International                      Summer School 2004, University of Konstanz,                      Germany}
}

@Misc{SE.8.Joergensen.2004.b,
  author = {J{\o}rgensen, M},
  title = {Software cost overruns - how large are they and how should they be measured?},
  year = {2004},
  abstract = {},
  howpublished = {Invited talk at: 60th seminar of the Software Reliability and Metrics Club, London}
}

@Misc{SE.8.Joergensen.2004.c,
  author = {J{\o}rgensen, M},
  title = {Lav pris gir lav kvalitet},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.d,
  author = {J{\o}rgensen, M},
  title = {Erfaring er ikke ekspertise},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.e,
  author = {J{\o}rgensen, M},
  title = {Hvor gode er v{\r a}re it-akt{\o}rer},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.f,
  author = {J{\o}rgensen, M},
  title = {Kunsten {\r a} m{\r a}le},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.g,
  author = {J{\o}rgensen, M},
  title = {Simpsons paradoks},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.h,
  author = {J{\o}rgensen, M},
  title = {Optimisme: styrke og svakhet},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.i,
  author = {J{\o}rgensen, M},
  title = {Produktet som ikke kan m{\r a}lbindes},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.k,
  author = {J{\o}rgensen, M},
  title = {Overskridelser i offentlige IT-prosjekter},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld: Kronikk}
}

@Misc{SE.8.Joergensen.2004.l,
  author = {J{\o}rgensen, M},
  title = {Hvordan f{\r a} tak i den reelle usikkerheten i IT-prosjekter},
  year = {2004},
  abstract = {},
  howpublished = {Den Norske Dataforening: Presentasjon p{\r a} seminar}
}

@Misc{SE.8.Joergensen.2004.m,
  author = {J{\o}rgensen, M},
  title = {IT-sprekk skyldes urealistiske kunder},
  year = {2004},
  abstract = {},
  howpublished = {digi.no: Innlegg}
}

@Misc{SE.8.Joergensen.2004.n,
  author = {J{\o}rgensen, M},
  title = {Estimering av IT-prosjekter},
  year = {2004},
  abstract = {},
  howpublished = {Simula-seminar: Presentasjon}
}

@Misc{SE.8.Joergensen.2004.o,
  author = {J{\o}rgensen, M},
  title = {Software Cost Overruns - How Large Are They? Critical comments on the CHAOS Report},
  year = {2004},
  abstract = {The Standish Group (www.standishgroup.com) claims that the results of their CHAOS research, i.e., their large-scaled surveys conducted in 1994, 1996, 1998, 2000 and 2002, are the most widely quoted statistics in the IT industry. This may very well be true. Quoted with particular frequency are the results described in the 1994 CHAOS report [1], probably because the 1994 CHAOS report is free and easily can be downloaded from the web. The results of that report have been used in several recent governmental reports, project reviews, and research studies. Examples are the PITAC 1999 report [2] and the cost estimation study described in [3]. An important result from the 1994 CHAOS research is the reported 189\% average cost overrun of so-called challenged projects, i.e., projects not on time, on cost, and with all specified functionality. In this paper we argue that the 189\% average cost overrun number, as it is commonly interpreted, is not consistent with results of other cost accuracy surveys and probably far too high to reflect the average cost overrun in that period. The measures and the research method of the CHAOS survey are insufficiently described to evaluate the quality of the results, e.g., there are many possible interpretations of what is meant by {\textquoteleft}cost overrun{\textquoteright} in the CHAOS reports. We should therefore cease to trust the 189\% average cost overrun as a reference point for performance of software projects until such time as the Standish Group disclose how they measure cost overrun and how they conduct their research.},
  howpublished = {SPIKE-seminar: Presentasjon}
}

@Misc{SE.8.Joergensen.2004.p,
  author = {Karahasanovic, A and Hinkel, U N},
  title = {A Controlled Experiment to Evaluate the Reactivity and Usefulness of the Think-Aloud Tool},
  year = {2004},
  abstract = {},
  howpublished = {Presentation at the Oslo University College},
  note = {20 deltakere. Arrang{\o}r: HiO}
}

@Misc{SE.8.Karahasanovic.2004.b,
  author = {Karahasanovic, A},
  title = {A Think-Aloud Tool as a Means for Understanding Processes of Programming and Process Evaluation},
  year = {2004},
  abstract = {},
  howpublished = {Presentation at the COOL seminar},
  note = {10-15 deltakere. Arrang{\o}r: Intermedia}
}

@Article{Vokac.2004.2,
  author = {Vok{\a\'a}c, Marek and Tichy, W and Sj{\o}berg, Dag I. K and Arisholm, Erik and Aldrin, M},
  title = {A Controlled Experiment Comparing the Maintainability of Programs Designed with and without Design Patterns -- a Replication in a real Programming Environment},
  year = {2004},
  abstract = {Software "Design Patterns" seek to package proven solutions to design prob- 
lems in a form that makes it possible to find, adapt and reuse them. To support the 
industrial use of Design Patterns, this research investigates when, and how, using patterns is beneficial, and whether some patterns are more di{\textpm}cult to use than others. This paper describes a replication of an earlier controlled experiment on Design Patterns in maintenance, with major extensions. Experimental realism was increased by using a real programming environment instead of pen and paper, and paid professionals from multiple major consultancy companies as subjects. 


Measurements of elapsed time and correctness were analyzed using regression models and an estimation method that took into account the correlations present in the raw data. Together with on-line logging of the subjects' work, this made possible a better qualitative understanding of the results. 


The results indicate quite strongly that some patterns are much easier to understand and use than others. In particular, the Visitor pattern caused much confusion. Conversely, the patterns Observer and, to a certain extent, Decorator were grasped and used intuitively, even by subjects with little or no knowledge of patterns. 


The implication is that Design Patterns are not universally good or bad, but must be used in a way that matches the problem and the people. When approaching a program with documented Design Patterns, even basic training can improve both the speed and quality of maintenance activities
},
  journal = {Empirical Software Engineering},
  volume = {9},
  number = {3},
  pages = {149--195}
}

@Inproceedings{Benestad.2005.1,
  author = {Benestad, Hans Christian and Arisholm, Erik and Sj{\o}berg, Dag I. K},
  title = {How to Recruit Professionals as Subjects in Software Engineering Experiments},
  year = {2005},
  abstract = {Controlled experiments are the classical scientific method for identifying cause-effect relationships, and are complementary to case studies and surveys as a means to empirically evaluate information systems development methods and practices. Most controlled experiments that evaluate software development methods and practices use students as subjects. Using students as subjects is convenient. However, a common criticism of controlled experiments in which students are used as subjects is that it is difficult to generalize the results to industrial settings. Consequently, Simula Research Laboratory has included professionals as subjects in software engineering experiments. At present, more than 750 professional software developers from 46 software development organizations have participated in our experiments. From this experience we have identified three important principles for research groups that want to include professional software developers as subjects in controlled experiments. First, practical constraints must be considered when defining the target population of software developers.  Second, the participating organizations must be offered flexibility and value using a planned communication strategy, in order to ensure adequately sized representative samples of organizations and individuals. Third, to ensure long-term, relationships with the organizations, high professional and ethical standards must be employed.},
  booktitle = {IRIS (Information Systems Research in Scandinavia), August 6-9, Kristiansand Norway},
  editor = {Hustad, E., Munkvold, B.E., Rolland, K. and Flak, L.S.},
  publisher = {Department of Information Systems, Agder University College},
  isbn = {0000-0000}
}

@Inproceedings{Jorgensen.2005.1,
  author = {J{\o}rgensen, Magne and Gruschke, Tanja Milijana},
  title = {Industrial Use of Formal Software Cost Estimation Models: Expert Estimation in Disguise?},
  year = {2005},
  abstract = {The goal of this paper is to propose and evaluate the hypothesis that software cost estimates based on formal estimation models are frequently expert estimation in disguise, i.e., that the cost estimates are not as mechanically derived as prescribed and assumed. We test implications of the hypothesis through discussion of related work and an empirical study of function point-based effort estimation of software projects. The actual effort estimates of the projects were compared with the effort estimates one would expect if the formal function point model was applied as prescribed. We observed several large deviations between the actual and the mechanically derived effort estimates, which we interpret as indications of a strong impact from expert judgment. Important limitations of our study are that the hypothesis is formulated vaguely, that there is not much evidence available, and, that we may have had a tendency to bias our search towards supporting evidence. More studies are therefore needed, preferably from independent researchers. If our hypothesis is correct, implementation of formal software cost estimation models should include means to avoid unwanted effects of initial beliefs and irrelevant information.},
  booktitle = {Proceedings of EASE, Keele, UK, April 11-13},
  pages = {1-7},
  publisher = {Keele University}
}

@Inproceedings{Molokken-ostvold.2005.1,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and J{\o}rgensen, Magne and S{\o}rgaard, P{\r a}l and Grimstad, Stein},
  title = {Management of Public Software Projects: Avoiding Overruns},
  year = {2005},
  abstract = {Avoiding overruns is one of the major challenges in public software projects. In order to reduce overruns and, in turn, increase the chances of project success, it is essential to have an unbiased overview of the frequency and magnitude of overruns, as well as knowledge about factors which may prevent overruns. In a recent survey of the software industry it was found that the average magnitude of overruns in work-hours was 67\% for projects with a public client, compared to 21\% for projects with a private client. This paper presents results from two surveys of software companies and professionals that investigate the possible differences between public and private clients, and a review of research on public IT-projects. The results indicate that there are several properties of public projects on the political, organizational and individual level which may contribute to the large overruns. These include artificial deadlines based on political terms, regulations on procurement and development processes, and lack of technology and project management skills. The challenges appear to be common for several countries in the OECD region which have investigated this topic. In order to reduce overruns of public projects, both contractors and government officials have to address these issues.},
  booktitle = {Hawaiian International Conference on Business, May 25-28, Hawaii, USA},
  publisher = {Hawaiian International Conference on Business}
}

@Inproceedings{Mohagheghi.2005.1,
  author = {Mohagheghi, Parastoo and Anda, Bente Cecilie Dahlum and Conradi, Reidar},
  title = {Effort Estimation of Use Cases for Incremental Large-Scale Software Development},
  year = {2005},
  abstract = {This paper describes an industrial study of an effort estimation method based on use cases, the Use Case Points method. The original method was adapted to incremental development and evaluated on a large industrial system with modification of software from the previous release. We modified the following elements of the original method: a) complexity assessment of actors and use cases, and b) the handling of non-functional requirements and team factors that may affect effort. For incremental development, we added two elements to the method: c) counting both all and the modified actors and transactions of use cases, and d) effort estimation for secondary changes of software not reflected in use cases. We finally extended the method to: e) cover all development effort in a very large project. The method was calibrated using data from one release and it produced an estimate for the successive release that was only 17\% lower than the actual effort. The study identified factors affecting effort on large projects with incremental development. It also showed how these factors can be calibrated for a specific context and produce relatively accurate estimates.},
  booktitle = {ICSE'05},
  editor = {Mohagheghi, B Anda, R Conradi},
  pages = {303--311},
  publisher = {The 27th International Conference on Software Engineering, 15-21 May, St Louis, Missouri, USA},
  series = {ISBN 1-59593-963-2, ACM Order Number 592050},
  isbn = {1-59593-963-2}
}

@Inproceedings{Molokken-ostvold.2005.2,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Ethical Concerns when Increasing Realism in Controlled Experiments with Industrial Participants},
  year = {2005},
  abstract = {The emerging interest for realistic controlled 
experiments in computer science has created a need for 
focus on related research ethics. Increased realism and 
scale in experimental studies pose new challenges which 
have not been debated to a sufficient extent. Specifically, 
there can be conflicts between the ethical principles of 
scientific value and informed consent. This paper 
provides an account of related previous work in computer 
science research ethics. To illustrate, two large scale 
software engineering experiments with industrial 
participants are described. Challenges and solutions in 
these experiments are discussed in light of current ethical 
guidelines. Interviews and debriefing sessions with 
industrial participants from these, and other, experiments 
are also provided. These reveal that there not necessarily 
will be ethical problems with increased realism, given 
that the researchers respect the principles of informed 
consent, benefice and confidentiality.},
  booktitle = {38th Annual Hawaii International Conference on System Sciences (HICSS'05), January 3-6, Hawaii, USA},
  publisher = {Computer Society Press}
}

@Article{Anda.2005.2,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag I. K},
  title = {Investigating the Role of Use Cases in the Construction of Class Diagrams},
  year = {2005},
  abstract = {Several approaches have been proposed for the transition from functional requirements to object-oriented design. In a use case-driven development process, the use cases are important input to the identification of classes and their methods. There is, however, no established, empirically validated technique for the transition from use cases to class diagrams. One recommended technique is to derive classes by analyzing the use cases. It has, nevertheless, been reported that this technique leads to problems, such as the developers missing requirements and mistaking requirements for design. An alternative technique is to identify classes from a textual requirements specification and subsequently apply the use case model to validate the resulting class diagram. This paper describes two controlled experiments conducted to investigate these two approaches to applying a use case model in an object-oriented design process. The first experiment was conducted with 53 students as subjects. Half of the subjects used a professional modelling tool; the other half used pen and paper. The second experiment was conducted with 22 professional software developers as subjects, all of whom used one of several modelling tools. The first experiment showed that applying use cases to validate class diagrams constructed from textual requirements led to more complete class diagrams than did the derivation of classes from a use case model. In the second experiment, however, we found no such difference between the two techniques. In both experiments, deriving class diagrams from the use cases led to a better structure of the class diagrams. The results of the experiments therefore show that the technique chosen for the transition from use cases to class diagrams affect the quality of the class diagrams, but also that the effects of the techniques depend on the categories of developer applying it and on the tool with which the technique is applied.},
  journal = {Empirical Software Engineering},
  volume = {10},
  number = {3},
  pages = {285-309}
}

@Article{Dyba.2005.1,
  author = {Dyb{\r a}, Tore and Kitchenham, Barbara and J{\o}rgensen, Magne},
  title = {Evidence-based Software Engineering for Practitioners},
  year = {2005},
  abstract = {Objective: Our objective is to explain how practitioners in industry can use evidencebased
software engineering (EBSE) to assist decisions concerning the adoption of new
techniques. Rationale: Practitioners may make incorrect technology adoption decisions if
they do not consider scientific evidence about the efficacy of techniques. Method: We
adapt procedures used for evidence-based medicine to software engineering and discuss
how these procedures map to software process improvement. Results: We found EBSE
fitted well with current ideas concerning process improvement and that it could be an
important means for closing the gap between research and practice. However EBSE
presents difficulties for practitioners because current software engineering research is
limited and not reported in a manner that assists accumulation and evaluation of evidence.
Conclusions: EBSE has potential benefits for software engineering practitioners, but will
be hindered without changes to software engineering research.
Keywords: Evidence, empirical software engineering, evaluation.},
  journal = {IEEE Software},
  volume = {22},
  number = {1},
  pages = {58-65}
}

@Inproceedings{Jorgensen.2005.2,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Over-optimism in Software Development Projects: {\textquotedblleft}The winner{\textquoteright}s curse{\textquotedblright}},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of IEEE CONIELECOMP, Puebla, Mexico, February 28-March 2},
  pages = {280--285 },
  publisher = {IEEE Computer Society}
}

@Article{Jorgensen.2005.3,
  author = {J{\o}rgensen, Magne},
  title = {Practical guidelines for better support of expert judgement-based software effort estimation},
  year = {2005},
  abstract = {},
  journal = {IEEE Software},
  volume = {22},
  number = {3},
  pages = {57--63}
}

@Article{Teigen.2005.1,
  author = {Teigen, Karl Halvor and J{\o}rgensen, Magne},
  title = {When 90\% confidence intervals are only 50\% certain: On the credibility of credible intervals},
  year = {2005},
  abstract = {Estimates of confidence intervals for general knowledge items are usually too narrow. We report five experiments showing that people have much less confidence in these intervals than dictated by the assigned level of confidence. For instance, 90\% intervals can be associated with an estimated confidence of 50\% or less (and still lower hit rates). Moreover, interval width appears to remain stable over a wide range of instructions (high and low numeric and verbal confidence levels). This leads to a high degree of overconfidence for 90\% intervals, but less for 50\% intervals or for free choice intervals (without an assigned degree of confidence). To increase interval width one may have to ask exclusion rather than inclusion questions, for instance by soliciting {\textquotedblleft}improbable{\textquotedblright} upper and lower values (Experiment 4), or by asking separate {\textquotedblleft}more than{\textquotedblright} and {\textquotedblleft}less than{\textquotedblright} questions (Experiment 5). We conclude that interval width and degree of confidence have different determinants, and cannot be regarded as equivalent ways of expressing uncertainty.},
  journal = {Applied Cognitive Psychology},
  volume = {19},
  number = {4},
  pages = {455--475}
}

@Inproceedings{Thomas.2005.1,
  author = {Thomas, Richard and Karahasanovic, Amela and Kennedy, Gregor},
  title = {An Investigation into Keystroke Metrics as an Indicator of Programming Performance},
  year = {2005},
  abstract = {Typing has long been studied in psychology and HCI, and strong 
cognitive models for transcription typing exist. However work on 
keystroke latencies has not yielded a stand alone technique for identity authentication. We present the results from two very different experiments with computer science students. Keystroke timings were recorded while they worked on 
Ada or Java source code. Programming quality of their finished work was 
measured mainly in terms of completeness. In the controlled experiment, 
39 students undertook two tasks over more than 2 hours to modify an 
application consisting of 6000 lines of Java. In the field study 
experiment, data was collected over 6 weeks from 141 students while 
they worked unsupervised on Ada programming in first year laboratories. In 
both cases there were highly significant (P=0.001), but weak, negative 
correlations between speed and coding performance. With additional 
development, these techniques may have promise for user modelling and 
assessment as well as in educational diagnostics.},
  booktitle = {Proceedings of the Australian Computing Education Conference 2005, Conferences in Research and Practice in Information Technology, Vol. 42, Newcastle, Australia, January 31-February 3},
  editor = {Thomas, Karahasanovic, Kennedy},
  pages = {127-134},
  publisher = {Australian Computer Society, Inc},
  isbn = {1-920682-24-4}
}

@Phdthesis{Vokac.2005.2,
  author = {Vok{\a\'a}c, Marek},
  title = {On the practical use of design patterns},
  year = {2005},
  abstract = {This thesis investigates the empirical evidence for the usefulness of design patterns, 
in the context of software maintenance. Advocates of design patterns claim general 
benefits in the form of higher flexibility, easier maintenance and reduced development 
time, though there is mostly only anecdotal evidence to support these claims. 
The thesis reports on empirical research that has been conducted in three related areas: 
the effects of design patterns on maintenance effort and correctness; the connection 
between design patterns and corrective maintenance needs, and the utility of design 
patterns in organizing the development process. 
The presence of design patterns in the code can have a significant effect on the effort 
needed to perform maintenance tasks, and the correctness of the results. The effects 
are strongly dependent on the expertise of the developers{\textemdash}a pattern that is beneficial 
to an expert, may cause a sharp increase in effort and a reduced correctness for a 
novice or intermediate developer. However, some patterns are simple and intuitive 
enough that they provide benefits to all types of developers, even those who lack 
knowledge of the particular pattern, or even design patterns in general. 
The presence of design patterns can also be correlated with the need for corrective 
maintenance, i.e., the error rates in the parts of the code conforming to the patterns, 
relative to the code at large. Some patterns are correlated with complex program 
structures that are prone to errors, while other patterns result in simple, less connected 
structures with lower error rates. Assuming that the patterns are used appropriately, 
this correlation can be used as a predictive or diagnostic tool to determine areas of the 
code that need particular design attention from expert developers. 
Design patterns can also be used in the organization of the development team. Teams 
are often organized around functional areas or around layers in the code (such as a 
Web front end or a database-related back end). An alternative, when the architecture 
of the system is expressed using patterns, is to organize the team around the patterns 
themselves, so that individual developers work with all the code involved in a pattern, 
across other boundaries. This can aid in the implementation of complex architectures, 
for example J2EE applications. 
Empirical evidence for these contributions was gathered through a combination of 
controlled experiments and case studies. Realism was ensured by using paid, professional 
developers as experimental subjects. In the case studies, industrial projects 
were examined and a large scale analysis of industrial C++ code performed. Research 
methods in the field were advanced both by the scale, design and realism of the experiment, 
and by the creation and validation of a tool for extracting design patterns 
from C++ code at a high precision and speed.},
  school = {University of Oslo}
}

@Article{Molokken-ostvold.2005.4,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and J{\o}rgensen, Magne},
  title = {Expert Estimation of the Effort of Web-Development Projects: Are Software Professionals in Technical Roles More Optimistic Than Those in Non-Technical Roles?},
  year = {2005},
  abstract = {},
  journal = {Journal of Empirical Software Engineering},
  volume = {10},
  number = {1},
  pages = {7-30}
}

@Article{Karahasanovic.2005.2,
  author = {Karahasanovic, Amela and Anda, Bente Cecilie Dahlum and Arisholm, Erik and Hove, Siw Elisabeth and J{\o}rgensen, Magne and Sj{\o}berg, Dag I. K and Welland, Ray},
  title = {Collecting Feedback during Software Engineering Experiments},
  year = {2005},
  abstract = {Objective: To improve the qualitative data obtained from software engineering experiments by gathering feedback during experiments. Rationale: Existing techniques for collecting quantitative and qualitative data from software engineering experiments do not provide sufficient information to validate or explain all our results. Therefore, we would like a cost-effective and unobtrusive method of collecting feedback from subjects during an experiment to augment other sources of data. Design of study: We formulated a set of qualitative questions that might be answered by collecting feedback during software engineering experiments. We then developed a tool to collect such feedback from experimental subjects. This feedback-collection tool was used in four different experiments and we evaluated the usefulness of the feedback obtained in the context of each experiment. The feedback data was triangulated with other sources of quantitative and qualitative data collected for the experiments. Results: We have demonstrated that the collection of feedback during experiments provides useful additional data to: validate the data obtained from other sources about solution times and quality of solutions; check process conformance; understand problem solving processes; identify problems with experiments; and understand subjects{\textquoteright} perception of experiments. Conclusions: Feedback collection has proved useful in four experiments and we intend to use the feedback-collection tool in a range of other experiments to further explore the cost-effectiveness and limitations of this technique. It is also necessary to carry out a systematic study to more fully understand the impact of the feedback-collecting tool on subjects{\textquoteright} performance in experiments.},
  journal = {Empirical Software Engineering},
  volume = {10},
  number = {2},
  pages = {113-147}
}

@Misc{Jorgensen.2004.1,
  author = {J{\o}rgensen, Magne},
  title = {Uncertainty Assessments in Situations With Unknown Probabilities, Unknown Events and Partial Impact of Outcome},
  year = {2004},
  abstract = {},
  howpublished = {Misc},
  note = {Presentation at conference on "Causality, Uncertainty and Ignorance", Koblenz, August, 2004}
}

@Article{Jorgensen.2005.5,
  author = {J{\o}rgensen, Magne},
  title = {Evidence-Based Guidelines for Assessment of Software Development Cost Uncertainty},
  year = {2005},
  abstract = {},
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {11},
  pages = {942-954}
}

@Article{Molokken-ostvold.2005.5,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and J{\o}rgensen, Magne},
  title = {A Comparison of Software Project Overruns {\textendash} Flexible vs. Sequential Development Models},
  year = {2005},
  abstract = {Flexible software development models, e.g., evolutionary and incremental models, have become increasingly popular. Advocates of these models claim that among the benefits of these models are reduced software project overruns, which is one of the main challenges of software project management. This paper describes an in-depth survey of software development projects. The results support the claim that projects which employ a flexible development model experience less effort overruns than do those who employ a sequential model. The reason for the difference is not obvious. We found, for example, no variation in project size, estimation process, or delivered proportion of planned functionality between projects applying different types of development model. When the managers were asked to provide reasons for software overruns and/or estimation accuracy, the largest difference were that more of flexible projects cited good requirement specifications and good collaboration/communication with clients as contributing to accurate estimates.
},
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {9},
  pages = {754-766},
  note = {Project questions are dowloadable}
}

@Inproceedings{Gruschke.2005.1,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {Assessing Uncertainty of Software Development Effort Estimates:The Learning From Outcome Feedback},
  year = {2005},
  abstract = {To enable properly sized software projects budgets and
plans it is important to be able to assess the uncertainty of
the estimates of most likely effort required to complete the
projects. Previous studies show that software professionals
tend to be too optimistic about the uncertainty of their effort
estimates. This paper reports the results from a preliminary
study on the role of outcome feedback in the learning
process on effort estimation uncertainty assessment.
Software developers were given repeated and immediate
outcome feedback, i.e. feedback about the discrepancy
between the estimated most likely effort and the actual
effort, for the purpose of investigating how much, and
how, they improve (learn). We found that a necessary
condition for improvement of uncertainty assessments of
effort estimates may be the use of explicitly formulated
uncertainty assessment strategies. By contrast, intuitionbased
uncertainty assessment strategies may lead to no or
little learning.},
  booktitle = {11th International Software Metrics Symposium (METRICS 2005), Como, Italy, September 19-22},
  pages = {p.4 (1-10)},
  publisher = {IEEE},
  note = {Proceedings not published on paper, only on CD-ROM. Therefore, the page numbers are displayed differently than usual.}
}

@Article{Sjoberg.2005.1,
  author = {Sj{\o}berg, Dag I. K and Hannay, Jo E and Hansen, Ove and Kampenes, Vigdis By and Karahasanovic, Amela and Liborg, Nils-Kristian and Rekdal, Anette C},
  title = {A Survey of Controlled Experiments in Software Engineering},
  year = {2005},
  abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research. },
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {9},
  pages = {733-753}
}

@Inproceedings{Grimstad.2005.2,
  author = {Grimstad, Stein and J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {The Clients' Impact on Effort Estimation Accuracy in Software Development Projects},
  year = {2005},
  abstract = {It seems clear that there is no simple solution to improved predictability of software development projects. Over several decades various aspects of software development and their relationship to estimation accuracy have been investigated. This paper focus on one such relationship; the clients' impact on estimation accuracy in software development projects. Factors contributing to overruns as well as factors preventing overruns are investigated. Based on a literature review and a survey of 300 software professionals we find that: 1) It is software professionals' perception that clients impact estimation accuracy. Changed and new requirements are perceived as the clients' most frequent contribution to overruns, while overruns are prevented by the availability of competent clients and capable decision makers. 2) Survey results should not be used in estimation accuracy improvement initiatives without further analysis. Surveys typically identify direct and project specific causes for overruns, while substantial improvement is only possible when the underlying causes are understood. 

Keywords: software estimation, survey, client-vendor relationship, overrun causes},
  booktitle = {11th IEEE International Software Metrics Symposium (METRICS 2005), Como, Italy, September 19-22},
  pages = {3},
  publisher = {IEEE},
  edition = {0},
  series = {0}
}

@Inproceedings{Hove.2005.1,
  author = {Hove, Siw Elisabeth and Anda, Bente Cecilie Dahlum},
  title = {Experiences from Conducting Semi-Structured Interviews in Empirical Software Engineering Research},
  year = {2005},
  abstract = {Many phenomena related to software development are qualitative in nature. Relevant measures of such phenomena are often collected using semi-structured interviews. Such interviews involve high costs, and the quality of the collected data is related to how the interviews are conducted. Careful planning and conducting of the interviews are therefore necessary, and experiences from interview studies in software engineering should consequently be collected and analyzed to provide advice to other researchers. We have brought together experiences from 12 software engineering studies, in which a total of 280 interviews were conducted. Four areas were particularly challenging when planning and conducting these interviews; estimating the necessary effort, ensuring that the interviewer had the needed skills, ensuring good interaction between interviewer and interviewees, and using the appropriate tools and project artifacts. The paper gives advice on how to handle these areas and suggests what information about the interviews should be included when reporting studies where interviews have been used in data collection. Knowledge from other disciplines is included. By sharing experience, knowledge about the accomplishments of software engineering interviews is increased and hence, measures of high quality can be achieved.},
  booktitle = {11th IEEE International Software Metrics Symposium (METRICS 2005), 19-22 September, Como, Italy},
  editor = {Hove, Anda},
  pages = {23(1-10)},
  publisher = {IEEE},
  isbn = {0-7695-2371-4}
}

@Inproceedings{Jorgensen.2005.4,
  author = {J{\o}rgensen, Magne and Kitchenham, Barbara and Dyb{\r a}, Tore},
  title = {Teaching Evidence-Based Software Engineering to University Students },
  year = {2005},
  abstract = {Evidence-based software engineering (EBSE) describes a process of identifying, understanding and evaluating findings from research and practice-based experience. This process aims at improving software engineering decisions. For the last three years, EBSE has been taught to university students at Hedmark University College, Rena, Norway. The motivation for the EBSE-course is that it is essential for the students, as future practitioners, to learn how to base important software engineering decisions on the systematic and critical evaluation of the best available evidence. The main purpose of this paper is to inspire and support other universities in their work on developing their own EBSE-courses. For this purpose we report on how our course has been organized and what lessons have been learned. There are currently no studies available on the effects of teaching EBSE and, as far as we know, only we have gained practice-based experience. To acquire more knowledge about the costs and benefits of teaching EBSE we hope that other universities will develop their own EBSE-courses and report their experience.},
  booktitle = {11th IEEE International Software Metrics Symposium, Como, Italy, September 19-22},
  publisher = { }
}

@Misc{Molokken-ostvold.2005.3,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Kan {\textquotedblright}moderne{\textquotedblright} utviklingsmetodikker redusere  overskridelser i IT-prosjekter?},
  year = {2005},
  abstract = {},
  howpublished = {Presentation for Abelia seminar on agile development}
}

@Misc{Molokken-ostvold.2005.7,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Lynkurs},
  year = {2005},
  abstract = {},
  howpublished = {Lynkurs i estimering for Sterias kunder},
  note = {Lynkurs for Sterias kunder}
}

@Techreport{Karahasanovic.2004.1,
  author = {Karahasanovic, Amela and Hinkel, Unni Nyhamar and Sj{\o}berg, Dag and Levine, Annette Kristin and Thomas, Richard},
  title = {A Controlled Experiment to Evaluate Feedback-Collection Tool in Software Engineering Research, Technical Report 2004-8, Simula Research Laboratory},
  year = {2004},
  abstract = {Objective: To compare the feedback-collection method with think-aloud methods regarding effects on subjects{\textquoteright} performance and type of collected information in software engineering experiments. Rationale: Think-aloud methods provide a rich set of data about experiments but are time-consuming in data collection.  Alternatively, subjects can be interrupted during experiments and asked for written feedback.  However, there has been no systematic comparison of these methods. Design of study: We conducted a controlled experiment to evaluate three methods for qualitative data collection: feedback-collection, concurrent think-aloud and retrospective think-aloud. Results: There is no significant difference in subjects{\textquoteright} performance for the three data collection methods.  The types of information gathered by these three methods differed; in particular, the feedback collection method provided more explanations about the problem-solving process and more information about subjects{\textquoteright} perceptions of the experiment. Conclusions: This research confirms that the feedback-collection method is an appropriate data collection method in software engineering experiments.},
  institution = {Simula Research Laboratory},
  number = {2004-8},
  note = {Technical Report 2004-8, Simula Research Laboratory}
}

@Techreport{Karahasanovic.2003.3,
  author = {Karahasanovic, Amela and Anda, Bente and Arisholm, Erik and Hove, Siw Elisabeth and J{\o}rgensen, Magne and Sj{\o}berg, Dag},
  title = {A Think-Aloud Support Tool for Collecting Feedback in Large-Scale Software Engineering Experiments},
  year = {2003},
  abstract = {The think-aloud method has proven useful for studying how a technology is used. However, it is labour-intensive and makes demands on resources for studies with many subjects. This paper describes a tool, called the think-aloud tool, that we have developed to support the collection of feedback from subjects in large-scale software engineering experiments. This tool instructs the subjects, at regular intervals, to write down their thoughts on a web-based screen and collects their feedback. An empirical evaluation of this tool is reported. The results from four controlled experiments suggest that the think-aloud tool is a valuable means for increasing the quality of large-scale software engineering experiments. The results also indicate that the participants were mostly positive to the tool and had no significant problems in verbalising their thoughts. The tool we proposed adds to an available set of tools for data collection in software engineering experiments. Whereas we have experience with the think-aloud tool in software engineering experiments, there is also potential for using such a tool in other areas in which the think-aloud method is used in studies involving a computer system. },
  institution = {Simula Research Laboratory},
  number = {2003-7}
}

@Inproceedings{Jorgensen.2005.7,
  author = {J{\o}rgensen, Magne},
  title = {The "Magic Step" of Judgment-Based Software Effort Estimation},
  year = {2005},
  abstract = {},
  booktitle = {International Conference on Cognitive Economics},
  pages = {105--114},
  publisher = {New Bulgarian University, Sofia, August 5-8}
}

@Inproceedings{Kampenes.2005.1,
  author = {Kampenes, Vigdis By},
  title = {Effect size in empirical software engineering experiments},
  year = {2005},
  abstract = {},
  booktitle = {Guidelines for Empirical Work in Software Engineering. 3rd International Workshop, WSESE2005, Oulu, Finland, June 13-16},
  editor = {Andreas Jedlitschka, Marcus Ciolkowski},
  pages = {14-21},
  publisher = {A publication by Fraunhofer IESE},
  isbn = {iese-report no. 053.05/E}
}

@Misc{Dzidek.2004.1,
  author = {Dzidek, W. James},
  title = {Automated Contract Driven Development with UML{\textquoteright}s OCL},
  year = {2004},
  abstract = {Design by Contract (DbC) allows for more powerful assertions since with the DbC framework assertions take the form of preconditions, postconditions, and class invariants. Preconditions specify something that must be true before a specific method executes, postconditions specify something that must be true after a specific method executes, and class invariants state something that must be true within a context of a specific class before and after any public method belonging to class executes. For example, in a Person class we may want to specify the invariant that the age attribute is never negative. If this is ever broken we ask the system to let us know by, for example, throwing an unchecked exception. Developing software with such a framework allows for developers to build better software by organizing the communication between software elements through specifying, the mutual obligations and benefits that are involved in those communications, where the specifications are the contracts. The benefit of DbC is said to be better designed software with improved reliability where the documentation is better, the debugging is easier, and support for reuse is better. The DbC concept is taken further in UML via OCL as contracts are expressed in higher-order logic and the contracts can be specified during analysis.},
  howpublished = {JavaZone website}
}

@Article{Vokac.2004.1,
  author = {Vok{\a\'a}c, Marek},
  title = {Defect Frequency and Design Patterns: An Empirical Study of Industrial Code},
  year = {2004},
  abstract = { Software ``Design Patterns' seek to package proven solutions to design
problems in a form that makes it possible to find, adapt and reuse them. A
common claim is that a design based on properly applied patterns will have
fewer defects than more ad hoc solutions.

This case study analyzes the weekly evolution and maintenance of a
large commercial product (C++, 500,000 LOC) over three years,
comparing defect rates for classes that participated in selected
Design Patterns to the code at large. We found that there are
significant differences in defect rates among the Patterns,
ranging from 63,\\% to 154,\\% of the average rate. We developed a
new set of tools able to extract design pattern information at a
rate 3$ imes$10$^6$ lines of code per hour, with relatively high
precision.

Based on a qualitative analysis of the code and the nature of the
Patterns, we conclude that the Observer and Singleton patterns
tend to be used in complex parts, and so can serve as indicators
of code that requires special attention. Conversely, code designed
with the Factory pattern is less complex or less central, and
consequently has lower defect rates. Template Method was used in
both simple and complex situations, leading to no clear tendency.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {12},
  pages = {904-917}
}

@Inproceedings{Anda.2005.6,
  author = {Anda, Bente Cecilie Dahlum and Benestad, Hans Christian and Hove, Siw Elisabeth},
  title = {A Multiple-Case Study of Effort Estimation based on Use Case Points},
  year = {2005},
  abstract = {There is much interest in industry for the estimation of software development effort based on use cases, but little scientific evaluation of applying use cases in estimation has been reported. We investigated one particular method, the use case points method, in a multiple-case study. The Software Engineering Department at Simula issued a tender for a system, and 35 companies responded, with estimates ranging from 78 to 654 hours. We chose four companies to develop the system. They all implemented the same functionality, but their development processes varied, ranging from a light, mainly code-and-fix process, to a heavy process with much emphasis on analysis and design. 
The use case points method estimated this project to 430 hours. This was equal to the actual effort spent on implementing the system by the company with the lightest development process. In our opinion, the results from this study may represent a basis for measuring size of the use cases and choosing productivity factor (hours per use case point) when estimating based on use cases. The three other companies spent 587, 829 and 943 hours respectively, showing that a heavier development process and more emphasis on non-functional requirements may increase effort by more than 100\%.
},
  booktitle = {ISESE'2005 (Fourth International Symposium on Empirical Software Engineering)},
  pages = {407--416},
  publisher = {IEEE Computer Society, Noosa, Australia, November 17-18}
}

@Inproceedings{Conradi.2005.1,
  author = {Conradi, Reidar and Li, Jingyue and Slyngstad, Odd Petter and Kampenes, Vigdis By and Bunse, Christian and Morisio, Maurizio and Torchiano, Marco},
  title = {Reflections on conducting an international survey of CBSE in ICT industry},
  year = {2005},
  abstract = {},
  booktitle = {ISESE05 (Fourth International Symposium on Empirical Software Engineering)},
  pages = {214-223},
  publisher = {IEEE Computer Society},
  address = {Noosa, Australia, November 17-18}
}

@Inproceedings{Dyba.2005.3,
  author = {Dyb{\r a}, Tore and Moe, Nils B and Arisholm, Erik},
  title = {Measuring Software Methodology Usage: Challenges of Conceptualization and Operationalization},
  year = {2005},
  abstract = {Most software engineering research implicitly assumes that development methodologies are useful and that there is a direct relationship between software methodologies and their effects on organizational performance. However, a methodology cannot have an impact if it is not used. The purpose of this paper is, thus, to raise a number of challenges related to the conceptualization and operationalization of methodology usage and to report on a study that compared subjective and objective operationalizations of usage. Results of regression analyses show that these operationalizations do not appear to be strongly related. While self-reported usage is related to self-reported measures of the independent variables of methodology acceptance in the study, the objective and computer-recorded measures show different and distinctly weaker links. There are several explanations to these seemingly contradictory results. Most importantly, the results of this study suggest a need for reconceptualization and better validation of methodology usage constructs in future, empirical software engineering research.},
  booktitle = {International Symposium on Empirical Software Engineering (ISESE'05), Noosa, Australia, November 17-18},
  pages = {447-458},
  publisher = {IEEE Computer Society}
}

@Misc{Molokken-ostvold.2005.8,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and Gallis, Hans Enger and Grimstad, Stein},
  title = {Software Estimation},
  year = {2005},
  abstract = {},
  howpublished = {Presentation for Norgesgruppen}
}

@Inproceedings{Briand.2005.1,
  author = {Briand, Lionel Claude and Dzidek, W. James and Labiche, Yvan},
  title = {Instrumenting Contracts with Aspect-Oriented Programming to Increase Observability and Support Debugging},
  year = {2005},
  abstract = {Analysis and design by contract allows the definition of a formal agreement between a class and its clients, expressing each party{\textquoteright}s rights and obligations. Operation contracts and class invariants are known to be a useful technique to specify the precondition and postcondition of operations and the legal states of class instances in an object-oriented context, making the definition of object-oriented analysis or design elements more precise. Furthermore, it is also useful to check such contracts and invariants at run time in order to help testing and debugging during corrective maintenance. Indeed, experiments report a substantial gain when relying on instrumented contracts during those two activities. However, the instrumentation of such contracts is a time consuming activity. In this paper we report on how Aspect-Oriented Programming (AOP), using AspectJ, can be employed to automatically and efficiently instrument contracts and invariants in Java. The paper focuses on (1) the AspectJ templates to instrument preconditions, postconditions, and class invariants, (2) the necessary instrumentation for compliance-checking to the Liskov Substitution Principle.},
  booktitle = {21st IEEE International Conference on Software Maintenance (ICSM), Budapest, Hungary, September 25-30 },
  editor = {IEEE Computer Society},
  pages = {687-690},
  publisher = {IEEE},
  isbn = {0-7695-2368-4},
  note = {Use of the attached PDF is subject to the IEEE terms found here: http://www.computer.org/portal/pages/csdl/content/Terms.html}
}

@Misc{Gallis.2005.1,
  author = {Gallis, Hans Enger and Arisholm, Erik},
  title = {The Pair Programming Experiment},
  year = {2005},
  abstract = {},
  howpublished = {Presentation of the international pair programming experiment, SPIKE seminar, Oslo,  June 16th 2005.}
}

@Techreport{Arisholm.2005.2,
  author = {Arisholm, Erik and Briand, Lionel Claude L and Hove, Siw Elisabeth and Labiche, Yvan},
  title = {The Impact of UML Documentation on Software Maintenance: An Experimental Evaluation},
  year = {2005},
  abstract = {The Unified Modeling Language (UML) is becoming the de-facto standard for software analysis and design modeling. However, there is still a significant resistance to model-driven development in many software organizations as it is perceived to be expensive and not necessarily cost-effective. It is therefore important to investigate the benefits obtained through modeling. As a first step in this direction, this paper reports on controlled experiments, spanning across two locations, which investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and passed a certain learning curve, the availability UML documentation may result into significant improvements of the functional correctness of changes as well as their design quality. On the other hand, there does not seem to be any resulting time saving. For simpler tasks, the time needed to update the UML documentation may be substantial compared with the potential benefits, thus motivating the need for UML tools with better support for performing model changes. },
  institution = {Simula Research Laboratory},
  type = {Technical Report},
  number = {2005-14}
}

@Article{articlereference.2005-08-30.0974791857,
  author = {No names specified},
  title = {},
  year = {},
  abstract = {}
}

@Inproceedings{Vokac.2005.3,
  author = {Vok{\a\'a}c, Marek and Glattetre, Jens M},
  title = {Using a Domain-Specific Language and custom tools to model a multi-tier service-oriented application---experiences and challenges},
  year = {2005},
  abstract = {A commercial Customer Relationship Management application of
approx. 1.5 MLOC of C++ code is being reimplemented, in stages, as
a service-oriented, multi-tier application in C\\# on Microsoft
.NET. We have chosen to use a domain-specific language both to
model the external service-oriented interfaces, and to manage the
transition to the internal, object-oriented implementation.
Generic UML constructs such as class diagrams do not capture
enough semantics to model these concepts. By defining a UML
Profile that incorporates the concepts we wish to model, we have
in effect created a Domain-Specific Language for our application.
The models are edited using Rational XDE, but we have substituted
our own code generator. This generator is a relatively generic
text-substitution engine, which takes a template text and performs
substitutions based on the model. The generator uses reflection to
convert the UML and Profile concepts into substitution tags, which
are in turn used in the template text. In this way, we can
translate the semantics of the model into executable code, WSDL or
other formats in a flexible way. We have successfully used this
approach on a prototype scale, and are now transitioning to
full-scale development.},
  booktitle = {Models 2005, Montego Bay, Jamaica October 2-7},
  editor = {Lionel Briand, Clay Williams },
  pages = {492--506},
  publisher = {Springer-Verlag GmbH},
  address = {Heidelberg},
  series = {LNCS 3713},
  note = {ISSN: 0302-9743
ISBN: 3-540-29010-9
DOI: 10.1007/11557432\_37 }
}

@Misc{Jorgensen.2005.10,
  author = {J{\o}rgensen, Magne and Karl Halvor, Teigen},
  title = {Kan vi unng{\r a} at "s{\r a} {\r a} si helt sikkert" bare betyr "60\% sikkert"?},
  year = {2005},
  abstract = {},
  howpublished = {Prosjektledelse (norsk fagtidsskrift for prosjektledere), Nr 2, s. 29-31}
}

@Misc{Jorgensen.2006.2,
  author = {J{\o}rgensen, Magne},
  title = {Software Cost Overruns - How Large Are They and How Should They be Measured? A critique of the Standish Group Chaos report.},
  year = {2006},
  abstract = {},
  howpublished = {Invited talk at the conference: OOP (Object oriented programming), M{\"u}nchen, January,}
}

@Misc{Jorgensen.2005.11,
  author = {J{\o}rgensen, Magne and Grimstad, Stein and Gruschke, Tanja Milijana},
  title = {The magic step in expert estimation of software cost. Why does 1000 feel more right than 1500?},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at: JavaZone, 2005}
}

@Techreport{Kasbo.2002.1,
  author = {Kasbo, J P and Gallis, Hans Enger and Herstad, J},
  title = {Walking away from the PDA},
  year = {2002},
  abstract = {This article presents our study of medical students in their medical practice and their use of mobile terminals, serving as information resources, in relation to the KNOWMOBILE project. The main objective is to shed light upon conditions for the possibility for use of mobile terminals, serving as an information resource, in contexts related to the medical practice. The title of the this article refers to our most distinct discovery: We observed a very limited use of the mobile terminal in the medical students' practical training. We observed that they preferred to use other resources instead, and often left the mobile terminal at home or in the cloakroom at the practice location. When moving away from the medical contexts and the medical tasks, the use increased. Through literature studies, prototyping and empirical findings, we state that several aspects affect the use or more correctly: the non-use of the mobile terminal. We state that the use is dependent upon the characteristics and resources of the mobile terminal as well as the other information resources available in the context. In addition, we believe that because the use of the mobile terminal was not percieved as a part of the community of practice, the medical students did not find the mobile terminal useful in their medical practice period.},
  institution = {KNOWMOBILE - Knowledge access in distributed                   training. Mobile opportunities for medical students.                   InterMedia report 5, University of Oslo}
}

@Misc{Gallis.2004.1,
  author = {Gallis, Hans Enger},
  title = {Agile Software Development and eXtreme Programming (XP)},
  year = {2004},
  abstract = {},
  howpublished = {Presentation to ABB, Oslo, August 20}
}

@Misc{Gallis.2005.2,
  author = {Gallis, Hans Enger},
  title = {Prosess og metode i programvareutvikling},
  year = {2005},
  abstract = {},
  howpublished = {Presentasjon for Bouvet, Oslo, 2. Juni}
}

@Misc{Gallis.2005.3,
  author = {Gallis, Hans Enger},
  title = {Prosess og metode i programvareutvikling - Hva er byggeklossene?},
  year = {2005},
  abstract = {},
  howpublished = {Presentasjon for Den Norske Dataforenings SQM nettverksm{\o}te, Oslo, 7. juni}
}

@Misc{Gallis.2003.1,
  author = {Gallis, Hans Enger},
  title = {Hva er likheten mellom Nils Arne Eggens godfoten filosofi og Kent Becks eXtreme Programming?},
  year = {2003},
  abstract = {},
  howpublished = {Presentation to the Telenor COS project, Oslo, December}
}

@Misc{Gallis.2003.2,
  author = {Gallis, Hans Enger},
  title = {Prosessforbedring gjennom samhandling: Nils Arne Eggens godfoten filosofi m{\o}ter Kent Becks eXtreme Programming},
  year = {2003},
  abstract = {},
  howpublished = {Presentation to Objectnet, Oslo, December}
}

@Misc{Gallis.2005.4,
  author = {Gallis, Hans Enger},
  title = {Prosess og metode i programvareutvikling - Balanse mellom smidighet og disiplin},
  year = {2005},
  abstract = {},
  howpublished = {Presentasjon for USIT (Universitetets Senter for Informasjonsteknologi), Oslo, 20. juni}
}

@Misc{Gallis.2004.2,
  author = {Gallis, Hans Enger},
  title = {Pair Programming},
  year = {2004},
  abstract = {},
  howpublished = {Presentation to the bAmbie project, VTT Electronics, Oulu, Finland, February 23}
}

@Misc{Gallis.2003.3,
  author = {Gallis, Hans Enger and Arisholm, Erik and Dyb{\r a}, Tore and Sj{\o}berg, Dag},
  title = {Pair Programming versus Individual Programming on Maintenance Tasks with Professional Subjects: A Controlled Experiment},
  year = {2003},
  abstract = {},
  howpublished = {Scientific presentation to ISERN, October 4th}
}

@Misc{Sjoberg.2005.2,
  author = {Sj{\o}berg, Dag},
  title = {Use of Professionals in Experiments},
  year = {2005},
  abstract = {},
  howpublished = {Letter to Editor, IEEE Software, Sep/Oct, pp. 9-10}
}

@Misc{Sjoberg.2002.1,
  author = {Sj{\o}berg, Dag and S{\o}r{\r a}sen, Oddvar and Holden, Lars},
  title = {Nekrolog - Kristen Nygaard, Aftenposten og Uniforum 13.8.2002},
  year = {2002},
  abstract = {},
  howpublished = {Misc}
}

@Misc{Dzidek.2005.2,
  author = {Dzidek, W. James},
  title = {Visual Struts: Modeling Struts-Based Applications using UML and Advanced Tool Support},
  year = {2005},
  abstract = {The Jakarta Struts framework is the de facto standard for web applications development in Java. Unfortunately, one of the primary difficulties with Struts-based development is that a lot of relationships are defined in a declarative manner, which leads to errors not being found at compilation-time. Furthermore, the system is developed in three different types of files: Java class files, Java ServerPages (JSPs), and XML configuration files. It is difficult to comprehend how these relationships interact, both, statically and dynamically (especially for a person not involved with the original development of a system). Modeling can greatly help in the understand of these relationships: (1) From a static point of view, page flow diagrams show how JSPs, user-defined classes, and Struts constructs (e.g. action mappings) relate (e.g. shows which JSP uses which action mapping); and (2) From a dynamic point of view, sequence diagrams show when/how specific action objects are invoked and when/how objects are placed in a scope. The technique discussed in this session has been discussed with over twenty professional Struts developers and actively used by ten on a real-word project. 

The talk will also cover the advantages and disadvantages of proprietary (non-UML based) Struts tools, mainly M7{\textquoteright}s NitroX for Eclipse.},
  howpublished = {JavaZone website}
}

@Article{Freimut.2005.1,
  author = {Freimut, Bernd and Briand, Lionel Claude L and Vollei, Ferdinand},
  title = {Determining Inspection Cost-Effectiveness by Combining Project Data and Expert Opinion},
  year = {2005},
  abstract = {There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worthwhile. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is proposed and a method to determine the cost-effectiveness by combining project data and expert opinion is described. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented and an initial validation is performed.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {12},
  pages = {1074-1092}
}

@Inproceedings{Hansen.2005.5,
  author = {Hansen, Kai and Anda, Bente Cecilie Dahlum and Gullesen, Ingolf},
  title = {Experiences with a UML-based Development Method for SIL products},
  year = {2005},
  abstract = {This article describes the results of an investigation into the suitability of methodology based on UML in an IEC 61508 development project that developed SIL products. Feedback was gathered from project members through interviews and questionnaires. The results show that the method led to improvements in some aspects of the development process that are particularly important in the development of SIL products. These improvements were related to documentation, traceability from requirements to code, and the quality of the code. The results also show, however, that the opinions of the project members differed a lot, and consequently that it is difficult to define one method with which a large number of developers with different backgrounds and mind sets will be content.},
  booktitle = {CSDUML 2005, The 4th International Workshop on Critical Systems Development Using Modeling Languages, 27th September, Fredrikstad, Norway},
  editor = {Siv Hilde Houmb, Jan J{\"u}rjens, Robert France},
  pages = {87--95},
  publisher = {Technical report, Institut f{\"u}r Informatik, Technische Universit{\"a}t M{\"u}nchen},
  isbn = {0}
}

@Inbook{Karahasanovic.2006.1,
  author = {Karahasanovic, Amela and Holmboe, C},
  title = {Challenges of learning object-oriented analysis and design through a modelling-first approach},
  year = {2006},
  abstract = {},
  booktitle = {Comprehensive Object-Oriented Learning. The Learner{\textquoteright}s Perspective},
  editor = {A. Fjuk, A. Karahasanovic, J. Kaasb{\o}ll},
  publisher = {Informing Science Press},
  address = {California, USA},
  chapter = {4},
  pages = {49--66},
  isbn = {83-922337-4-3}
}

@Inbook{Karahasanovic.2006.2,
  author = {Karahasanovic, Amela and Fjuk, A and Sj{\o}berg, Dag and Thomas, R},
  title = {Revealing object-oriented comprehension by feedback collection},
  year = {2006},
  abstract = {},
  booktitle = {Comprehensive Object-Oriented Learning. The Learner{\textquoteright}s Perspective},
  editor = {A. Fjuk, A. Karahasanovic, J. Kaasb{\o}ll},
  publisher = {Informing Science Press},
  address = {California, USA},
  chapter = {7},
  pages = {111--130},
  isbn = {83-922337-4-3}
}

@Book{Karahasanovic.2006.3,
  editor = {Fjuk, Annita and Karahasanovic, Amela and Kaasb{\o}ll, Jens},
  title = {Comprehensive Object-Oriented Learning: The Learners Perspective},
  year = {2006},
  abstract = {The object-oriented paradigm is increasingly being integrated in computer
science education as well as in industry. There is a high demand for
understanding the learner{\textquoteright}s actions, strategies and thoughts while solving
object-oriented problems. Little is known about these processes. Aimed at
both teachers and researchers involved in teaching object-oriented
technologies, this book reports findings from four case studies, two
design experiments and three controlled experiments, with a total of 187
subjects. It provides new insight into knowledge-acquiring processes and
shows how to successfully integrate the empirically based findings into
pedagogical design.

{\textquotedblleft}The area addressed in this book is interesting. I haven{\textquoteright}t found many
texts on this issue and there is clearly need for more knowledge. It is
very useful to get new perspectives on problems in learning
object-oriented concepts. Advices given in the book are concise and
concrete{\textquotedblright} Svein Erik Bratsberg, Norwegian University of Science and
Technology.

{\textquotedblleft}The book, interestingly recommends that the object-oriented education
starts with using tools that hide the compilation and execution steps.
Only later may the learners switch to more professional tools. This
approach should be considered in any introductory course on
object-oriented programming.{\textquotedblright} Gerhard Skagestein, University of Oslo.
},
  publisher = {Informing Science Press},
  address = {California, USA},
  isbn = {83-922337-4-3}
}

@Inbook{Kaasboll.2006.1,
  author = {Kaasb{\o}ll, J and Fjuk, A and Karahasanovic, Amela and Groven, A-K},
  title = {Improvements of teaching and tools for learning object-orientation},
  year = {2006},
  abstract = {},
  booktitle = {Comprehensive Object-Oriented Learning: The Learner's Perspective},
  editor = {A. Fjuk, A. Karahasanovic and J. Kaasb{\o}ll},
  publisher = {Informing Science Press},
  address = {California, USA},
  chapter = {11},
  pages = {205--220},
  isbn = {83-922337-4-3}
}

@Misc{Karahasanovic.2005.5,
  author = {Karahasanovic, Amela},
  title = {Object-Oriented Comprehension during Software Maintenance},
  year = {2005},
  abstract = {},
  howpublished = {Research Seminar at the Department of Computer Science and Software Engineering, University of Western Australia, Perth},
  note = {19.8.2005}
}

@Misc{Karahasanovic.2005.6,
  author = {Karahasanovic, Amela},
  title = {Empirical Studies of Object-Oriented Comprehension},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at the Australian Defence Institute, Perth, Western Australia}
}

@Misc{Karahasanovic.2005.9,
  author = {Karahasanovic, Amela},
  title = {Research, People and Economic Growth},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at the Conference Economic Growth and Diversity, Drammen }
}

@Inproceedings{Gruschke.2005.2,
  author = {Gruschke, Tanja Milijana},
  title = {Empirical Studies of Software Cost Estimation: Training of Effort Estimation Uncertainty Assessment Skills},
  year = {2005},
  abstract = {This research abstract describes my proposed doctorial work within the fi eld of software project cost and effort estimation. My work focuses on assessment of uncertainty of software development cost or effort estimates. In particular, the work focuses on to which degree this assessment is a skill that can be improved with better training. Work completed includes one small scale experiment with student participants. A followup, larger, experiment with professional software developers is currently in progress. Studies targeted towards better understanding of the mental processes of development of work effort estimates and uncertainty assessments will be the next step. This work aims at the development of effective training processes of estimation and uncertainty assessment skills. Through the METRICS05 dissertation forum I hope to receive feedback on the viability and relevance of the proposed work within the software cost and effort estimation field. },
  booktitle = {11th International Software Metrics Symposium (METRICS 2005), Como, Italy, September 19-22},
  pages = {48 (1-3)},
  publisher = {IEEE}
}

@Misc{Sjoberg.2006.1,
  author = {Sj{\o}berg, Dag I. K},
  title = {The Simula approach to experimentation in software engineering},
  year = {2006},
  abstract = {The ultimate goal of software engineering research is to support the private and public software industry in developing higher quality systems with improved timeliness in a more cost-effective and predictable way. One contribution of the empirical software engineering community to this overall goal is the conducting of experiments to evaluate and compare technologies (processes, methods, techniques, languages and tools) for planning, building and maintaining software. However, the applicability of the experimental results to industrial practice is, in most cases, hampered by the experiments{\textquoteright} lack of realism and scale regarding subjects, tasks, systems and environments. In this talk, I will discuss Simula Research Laboratory{\textquoteright}s strategy for addressing this challenge: (1) About 25\% of our budget is used for hiring software consultants as experimental subjects, mainly at the expense of employing a larger number of researchers. In the last five years, about 800 professionals from 60 companies in several countries have participated in 25 experiments (some of them very large, in order to identify the variances between sub-populations) in which the professionals worked under various controlled circumstances, such as the complexity of tasks and systems, the tools used, whether they worked in pairs, and so on. (2) A large investment in infrastructures and apparatus has been made to support the logistics of running large experiments and surveys, and to collect and organise data with minimal overhead. (3) A senior project manager has been employed to organise the experiments and the resulting data. To increase flexibility and save administrative overhead, Simula hires people on a short-term basis for assistance with, for example, particularly large or complex experiments. They could be students for clerical work or consultants who are particularly qualified for certain tasks, for example, a statistician. (4) Active collaboration with industry (in addition to hiring consultants), such as taking part in industry-managed research projects on software process improvement, and giving seminars and courses, has been considered important. The focus on publicising our research in the media and disseminating it through teaching has also resulted in Simula becoming well known in the Norwegian software industry. (5) Software engineering is typically performed by humans in organisations. Hence, we have established research collaborations with other disciplines, such as psychology, sociology and management.},
  howpublished = {Keynote address at EASE'2006}
}

@Mastersthesis{Hinkel.2005.1,
  author = {Hinkel, Unni Nyhamar},
  title = {Evaluating Methods for Data Collection during Software Engineering Experiments},
  year = {2005},
  abstract = {},
  school = {Department of Informatics, University of Oslo},
  note = {Supervisors Amela Karahasanovic and Dag Sj{\o}berg}
}

@Mastersthesis{Levine.2005.1,
  author = {Levine, Annette Kristin},
  title = {A Study of Comprehension Strategies and Difficulties by Novice Programmers Performing Maintenance Tasks on Object-Oriented Systems},
  year = {2005},
  abstract = {},
  school = {Department of Informatics, University of Oslo},
  note = {Supervisors Amela Karahasanovic and Dag Sj{\o}berg}
}

@Mastersthesis{Taij.2005.1,
  author = {Taij, Junisilver},
  title = {Controlled Experiment to Investigate the Correlation between Keystroke Latency and Programming Performance},
  year = {2005},
  abstract = {},
  school = {School of Computer Science and Software Engineering, The University of Western Australia},
  note = {Supervisors Richard Thomas and Amela Karahasanovic.
Amela Karahasanovic had support from the University of Western Australia Gledden Visiting Senior Fellowship}
}

@Mastersthesis{Vassdokken.2005.1,
  author = {Vassdokken, Rolf},
  title = {The background information on subjects in program comprehension studies},
  year = {2005},
  abstract = {},
  school = {Department of Informatics, University of Oslo},
  note = {Supervisor Amela Karahasanovic}
}

@Misc{Jorgensen.2005.15,
  author = {J{\o}rgensen, Magne},
  title = {Valg av IT-leverand{\o}r basert p{\r a} laveste pris:VINNERENS FORBANNELSE},
  year = {2005},
  abstract = {},
  howpublished = {Presentation at the DnD conference Software 2005}
}

@Misc{Jorgensen.2005.16,
  author = {J{\o}rgensen, Magne},
  title = {Teaching evidence-based software engineering},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at EASE 2005}
}

@Misc{Jorgensen.2005.17,
  author = {J{\o}rgensen, Magne},
  title = {Hvordan unng{\r a} VINNERENS FORBANNELSE i anbudsrunder med mange tilbydere og sterk fokusering p{\r a} pris? },
  year = {2005},
  abstract = {},
  howpublished = {Presenation at SPIKE seminar, June}
}

@Misc{Jorgensen.2005.18,
  author = {J{\o}rgensen, Magne and Grimstad, Stein and Gruschke, Tanja Milijana},
  title = {"The magic step" i ekspertestimering av IT-utviklingskostnader. Hvorfor f{\o}les 1000 timeverk mer riktig enn 1500?},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at JavaZone 2005}
}

@Misc{Jorgensen.2005.19,
  author = {J{\o}rgensen, Magne},
  title = {Overoptimisme i IT-prosjekter: Hvorfor l{\ae}rer vi aldri?},
  year = {2005},
  abstract = {},
  howpublished = {Simula seminar with approx. 100 software professionals}
}

@Misc{Jorgensen.2005.20,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Hvorfor er vi overoptimistiske igjen og igjen og igjen ...},
  year = {2005},
  abstract = {},
  howpublished = {Presentation IT-Fornebu halvtimen}
}

@Misc{Jorgensen.2005.21,
  author = {J{\o}rgensen, Magne},
  title = {Practical Guidelines for Expert-Judgment-Based Software Effort Estimation},
  year = {2005},
  abstract = {},
  howpublished = {Invited article to SPIKE newsletter IMPROVE}
}

@Misc{Molokken-ostvold.2005.9,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Research on software estimation - have we made progress the last 30 years?},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at Brunel University}
}

@Misc{Molokken-ostvold.2005.10,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = { Bedre estimater i IT-prosjekter. N{\o}kkelen ligger i grenselandet mellom kunde og leverand{\o}r},
  year = {2005},
  abstract = {},
  howpublished = {Hovedinnledning p{\r a} Abelias seminar om endringsvennlig prosjektstyring}
}

@Inproceedings{Jorgensen.2006.1,
  author = {J{\o}rgensen, Magne and Faugli, Bj{\o}rn},
  title = {Prediction of Optimistic Expert Judgment-Based Predictions},
  year = {2006},
  abstract = {(Paper selected for publication in JSS)},
  booktitle = {EASE 2006},
  editor = {B. Kitchenham},
  pages = {40-49},
  publisher = {BCS},
  address = {UK},
  isbn = {ISSN 1477-9358}
}

@Inproceedings{Briand.2006.1,
  author = {Briand, Lionel Claude L and Labiche, Yvan and Sowka, Michal},
  title = {Automated, Contract-based User Testing of Commercial-Off-The-Shelf Components},
  year = {2006},
  abstract = {Commercial-off-the-Shelf (COTS) components provide a means to construct software (component-based) systems in reduced time and cost. In a COTS component software market there exist component vendors (original developers of the component) and component users (developers of the component-based systems). The former provide the component to the user without source code or design documentation, and as a result it is difficult for the latter to adequately test the component when deployed in their system. In this article we propose a framework that clarifies the roles and responsibilities of both parties so that the user can adequately test the component in a deployment environment and the vendor does not need to release proprietary details. Then, based on this framework we combine and adapt two specification-based testing techniques and describe (and implement) a method for the automated generation of adequate test sets. An evaluation of our approach on a case study demonstrates that it is possible to automatically generate cost effective test sequences and that these test sequences are effective at detecting complex errors.},
  booktitle = {ACM/IEEE International Conference on Software Engineering (ICSE)},
  editor = {Ken Anderson},
  publisher = {ACM Press},
  isbn = {1-59593-375-1}
}

@Inproceedings{Garousi.2006.1,
  author = {Garousi, Vahid and Briand, Lionel Claude L and Labiche, Yvan},
  title = {Traffic-aware Stress Testing of Distributed Systems Based on UML Models},
  year = {2006},
  abstract = {A stress test methodology aimed at increasing chances of discovering faults related to network traffic in distributed systems is presented. The technique uses the UML 2.0 model of the distributed system under test, augmented with timing information and is based on an analysis of the control flow in sequence diagrams. It yields stress test requirements that are made of specific control flow paths along with time values indicating when to trigger them. Different variants of our stress testing technique already exist (they stress different aspects of a distributed system) and we focus here on one variant that is designed to identify and to stress test the system at the instant when data traffic on a network is maximal. Using a real-world distributed system specification, we design and implement a prototype distributed system and describe, for that particular system, how the stress test cases are derived and executed using our methodology. The stress test results indicate that the technique is significantly more effective at detecting network traffic-related faults when compared to test cases based on an operational profile. },
  booktitle = {ACM/IEEE International Conference on Software Engineering (ICSE)},
  editor = {Ken Anderson},
  pages = {391 - 400},
  publisher = {ACM Press},
  isbn = {1-59593-375-1}
}

@Misc{Molokken-ostvold.2005.11,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Lynkurs II},
  year = {2005},
  abstract = {},
  howpublished = {Lynkurs i estimering for Sterias kunder},
  note = {Lynkurs for Steria-kunder}
}

@Misc{Molokken-ostvold.2005.12,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Nyskapning basert p{\r a} forskning innenfor kostnadsberegning og gjennomf{\o}ring av IT-prosjekter},
  year = {2005},
  abstract = {},
  howpublished = {Presentation for Stortingets N{\ae}ringskomit{\a\'e}}
}

@Inproceedings{Benestad.2006.1,
  author = {Benestad, Hans Christian and Anda, Bente Cecilie Dahlum and Arisholm, Erik},
  title = {Assessing Software Product Maintainability Based on Class-Level Structural Measures},
  year = {2006},
  abstract = {A number of structural measures have been suggested to support the assessment and prediction of software quality attributes. The aim of our study is to investigate how class-level measures of structural properties can be used to assess the maintainability of a software product as a whole. We survey, structure and discuss current practices on this topic, and apply alternative strategies on four functionally equivalent systems that were constructed as part of a multi-case study. In the absence of historical data needed to build statistically based prediction models, we apply elements of judgment in the assessment. We show how triangulation of alternative strategies as well as sensitivity analysis may increase the confidence in assessments that contain elements of judgment. This paper contributes to more systematic practices in the application of structural measures. Further research is needed to evaluate and improve the accuracy and precision of judgment-based strategies. },
  booktitle = {7th International Conference on Product-focused Software Process Improvement (PROFES)},
  editor = {J{\"u}rgen M{\"u}nch},
  pages = {94-111},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science},
  isbn = {0302-9743}
}

@Inproceedings{Anda.2006.1,
  author = {Anda, Bente Cecilie Dahlum and Hansen, Kai},
  title = {A Case Study on the Application of UML in Legacy Development},
  year = {2006},
  abstract = {Model-driven development with UML is becoming a de facto standard in industry, but although much of today{\textquoteright}s software development is about enhancing existing systems, there is no well-defined process for model-driven development in the context of legacy systems. To ensure the relevance of research on model-driven development with UML, there is a need for studies of actual use of UML in software development companies. As part of a software process initiative, we conducted a case study in a large development project where some of the development teams enhanced existing components, while other teams developed software from scratch. The results from this case study showed that those who applied UML in modelling and enhancing legacy software experienced more challenges and fewer benefits from the use of UML than did the developers who modelled and developed new software. Overall our results show a need for better methodological support on applying UML in legacy development.},
  booktitle = {ISESE'2006 (Fifth ACM-IEEE International Symposium on Empirical Software Engineering)},
  editor = {Jose Maldonado, Claes Wohlin},
  pages = {124-133},
  publisher = {ACM Press, Rio de Janiero, Brasil, September 21-22},
  isbn = {1-59593-218-6}
}

@Inbook{Dzidek.2006.1,
  author = {Dzidek, W. James and Briand, Lionel Claude and Labiche, Yvan},
  title = {Lessons Learned from Developing a Dynamic OCL Constraint Enforcement Tool for Java},
  year = {2006},
  abstract = {Analysis and design by contract allows the definition of a formal agreement between a class and its clients, expressing each party{\textquoteright}s rights and obligations. Contracts written in the Object Constraint Language (OCL) are known to be a useful technique to specify the precondition and postcondition of operations and class invariants in a UML context, making the definition of object-oriented analysis or design elements more precise while also helping in testing and debugging. In this article, we report on the experiences with the development of ocl2j, a tool that automatically instruments OCL constraints in Java programs using aspect-oriented programming (AOP). The approach strives for automatic and efficient generation of contract code, and a non-intrusive instrumentation technique. A summary of our approach is given along with the results of an initial case study, the discussion of encountered problems, and the necessary future work to resolve the encountered issues.},
  booktitle = {Satellite Events at the MoDELS 2005 Conference: MoDELS 2005 International Workshops OCLWS, MoDeVA, MARTES, AOM, MTiP, WiSME, MODAUI, NfC, MDD, WUsCAM, Montego Bay, Jamaica, October 2-7, 2005, Revised Selected Papers},
  editor = {Bruel, Jean-Michel},
  publisher = {Springer-Verlag GmbH},
  series = {Lecture Notes in Computer Science, Vol. 3844},
  pages = {10--19},
  isbn = {3-540-31780-5}
}

@Inproceedings{Mair.2005.2,
  author = {Mair, Carolyn and Shepperd, Martin and J{\o}rgensen, Magne},
  title = {An analysis of data sets used to train and validate cost prediction systems},
  year = {2005},
  abstract = {},
  booktitle = {International Conference on Software Engineering: Proceedings of the 2005 workshop on Predictor models in software engineering },
  pages = {1--6},
  publisher = {ACM Press},
  address = {New York, NY, USA },
  note = {ISBN:-159593-125-2 
}
}

@Article{Vokac.2006.1,
  author = {Vok{\a\'a}c, Marek},
  title = {An efficient tool for recovering Design Patterns from C++ Code},
  year = {2006},
  abstract = {Design Patterns are informal descriptions of tested solutions to 
recurring problems. Most design tools have little or no support 
for documenting the presence and usage of patterns in code. 
Reverse engineering is therefore often required to recover Design 
Patterns from code in existing projects. Knowledge of what Design 
Patterns have been used can aid in code comprehension, as well as 
support research. 

Since pattern descriptions are abstract and informal, they offer 
no algorithmic translation into concrete code. Some patterns 
prescribe class structures that are easy to recognize, while 
others lead to structures that are difficult or impossible to 
recognize. 

This work presents a tool that can recover five different design 
patterns from C++ code with high precision and at a speed of 
3x10^6 LOC/hr. This makes it suitable for analysis of 
large (multi-millon LOC) systems.},
  journal = {Journal of Object Technology},
  volume = {5},
  number = {1},
  pages = {139--157}
}

@Book{Conradi.2006.3,
  editor = {Conradi, Reidar and Dyb{\r a}, Tore and Sj{\o}berg, Dag I. K and Ulsund, Tor},
  title = {Software Process Improvement: Results and Experience from the Field},
  year = {2006},
  abstract = {For over a decade, software process improvement (SPI) has been promoted as an approach to improve systematically the way software is developed and managed. Mostly this research and the relevant experience reports have been focussed on large software companies.

Conradi and his co-authors have collected the main results from four Norwegian industrial research and development projects on SPI carried out between 1996 and 2005, which, in contrast to other treatments, concentrated on small- and medium-sized companies, typically characterized by fast-changing environments and processes. The presentation is organized in five sections: general principles and methods of SPI, knowledge management for SPI, process modelling and electronic process guides, estimation methods, and object-oriented and component-based systems. A spectrum of empirical methods has been used, e.g. case studies, large-scale experiments, surveys and interviews, and action research.

The book mainly targets researchers and graduate students in (empirical) software engineering, and software professionals working in development or quality assurance
},
  publisher = {Springer},
  isbn = {3-540-32178-0}
}

@Article{Grimstad.2006.1,
  author = {Grimstad, Stein and J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Software Effort Estimation Terminology: The Tower of Babel},
  year = {2006},
  abstract = {This paper provides a review of how software development effort estimation terms are used in software engineering textbooks and research papers. We found that the term {\textquoteleft}effort estimate{\textquoteright} frequently is applied without any clarification of its meaning. It is therefore difficult to determine whether the authors{\textquoteright} intended interpretation is an estimate of {\textquoteleft}most likely effort{\textquoteright}, {\textquoteleft}planned effort{\textquoteright}, {\textquoteleft}budgeted effort{\textquoteright}, or something else. This is problematic as these terms are not equivalent and are used for different purposes. The lack of clarity of {\textquoteleft}effort estimate{\textquoteright} lowers the quality and interpretability of surveys on software effort estimation accuracy, i.e., it is not clear what the estimation accuracy results really mean. This reduces the estimation evaluation and learning possibilities. We suggest guidelines on how to reduce this terminology ambiguity. To the authors{\textquoteright} knowledge, this is the first published review of software cost estimation terminology.},
  journal = {Journal of Information and Software Technology},
  volume = {48},
  number = {4},
  pages = {302-310}
}

@Article{Jorgensen.2006.3,
  author = {J{\o}rgensen, Magne},
  title = {The Effects of the Format of Software Project Bidding Processes},
  year = {2006},
  abstract = {This study investigates how differences in format of the bidding process affect companies' bids for software projects. Thirty outsourcing companies from different Asian and European countries participated in the bidding for a software project. A participating company either started with the provision of a bid based on a reduced version of the specification and then continued with a bid based on the full specification (the Increase situation), or started with the full specification and then continued with the reduced one (the Decrease situation). We observed important differences in bids for the same project as a result of different bidding sequences. Our results constitute evidence that in situations similar to the one we studied, the client will typically select a provider with about a 40\% lower price in the Decrease situation than in the Increase situation. The difference in bids seems to be explained by whether the first bid was provided on the full or the reduced specification, not by the process of updating, i.e., increasing or decreasing, the bids. We warn against manipulation of the bidding processes to receive lower bids. This increases the risk of over-optimistic bids. Over-optimistic bids frequently have as a consequence "the winner's curse", which leads easily to "the client's curse".},
  journal = {International Journal of Project Management},
  volume = {24},
  number = {6},
  pages = {522-528 }
}

@Inproceedings{Grimstad.2006.2,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {A Framework for the Analysis of Software Cost Estimation Accuracy},
  year = {2006},
  abstract = {},
  booktitle = {ISESE 2006},
  editor = {Maldonado, Wohlin, Mendes	  },
  pages = {58--65},
  publisher = {ACM Press},
  isbn = {1-59593-218-6}
}

@Article{Jorgensen.2006.4,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {How Large Are Software Cost Overruns? Critical Comments on the Standish Group{\textquoteright}s CHAOS Reports},
  year = {2006},
  abstract = {The Standish Group reported in 1994 that the average cost overrun of software projects was as high as 189\%. This cost overrun number is used as input in recent governmental reports on software development and as benchmark for several recent projects{\textquoteright} estimation performances. It is therefore important that we can trust this number. More recent cost overrun results reported by the Standish Group and others, however, show much lower cost overrun. Does this mean that software companies have improved their estimation ability strongly the last 10 years? In this paper we argue that there are reasons to doubt the validity of the Standish Group{\textquoteright}s 1994 cost overrun results and that a continued use of these results may hinder progress.},
  journal = {Information and Software Technology},
  volume = {48},
  number = {4},
  pages = {297-301}
}

@Inbook{Arisholm.2006.1,
  author = {Arisholm, Erik and Sj{\o}berg, Dag},
  title = {Evaluating the Effect of a Delegated versus Centralized Control Style on the Maintainability of Object-Oriented Software},
  year = {2006},
  abstract = {},
  booktitle = {Software Process Improvement: Results and Experience from the Field},
  editor = {Conradi, Dyb{\r a}, Sj{\o}berg \& Ulsund},
  publisher = {Springer},
  chapter = {20},
  pages = {379-406}
}

@Inbook{Sjoberg.2003.1,
  author = {Sj{\o}berg, Dag I. K and Anda, Bente Cecilie Dahlum and Arisholm, Erik and Dyb{\r a}, Tore and J{\o}rgensen, Magne and Karahasanovic, Amela and Vok{\a\'a}c, Marek},
  title = {Challenges and Recommendations When Increasing the Realism of Controlled Software Engineering Experiments},
  year = {2003},
  abstract = {An important goal of most empirical software engineering experiments is the transfer of the research results to industrial applications. To convince industry about the validity and applicability of the results of controlled software engineering experiments, the tasks, subjects and the environments should be as realistic as practically possible. Such experiments are, however, more demanding and expensive than experiments involving students, small tasks and pen-and-paper environments. This chapter describes challenges of increasing the realism of controlled experiments and lessons learned from the experiments that have been conducted at Simula Research Laboratory. },
  booktitle = {Empirical Methods and Studies in Software Engineering: Experiences from ESERNET},
  editor = {Reidar Conradi and Alf Inge Wang},
  publisher = {Springer},
  address = {Berlin / Heidelberg},
  series = {Lecture Notes in Computer Science, Volume 2765 },
  chapter = {Part II: Method Chapters},
  pages = {24--38},
  isbn = {978-3-540-40672-3}
}

@Mastersthesis{Almqvist.2006.1,
  author = {Almqvist, Johan Per Fredrik},
  title = {Replication of Controlled Experiments in Empirical Software Engineering - A Survey},
  year = {2006},
  abstract = {},
  school = {Department of Computer Science, Faculty of Science, Lund University},
  note = {Supervisors: Amela Karahasanovic, Simula RL and Goran Fries, Lund University}
}

@Article{Arisholm.2006.2,
  author = {Arisholm, Erik and Gallis, Hans Enger and Dyb{\r a}, Tore and Sj{\o}berg, Dag I. K},
  title = {Evaluating Pair Programming with Respect to System Complexity and Programmer Expertise},
  year = {2007},
  abstract = {A total of 295 junior, intermediate and senior professional Java consultants (99 individuals and 98 pairs) from 29 international consultancy companies in Norway, Sweden and the UK were hired for one day to participate in a controlled experiment on pair programming. The subjects used professional Java tools to perform several change tasks on two alternative Java systems with different degrees of complexity.
The results of this experiment do not support the hypotheses that pair programming in general reduces the time required to solve the tasks correctly or increases the proportion of correct solutions. On the other hand, there is a significant 84 percent increase in effort to perform the tasks correctly. However, on the more complex system, the pair programmers had a 48 percent increase in the proportion of correct solutions, but no significant differences in the time taken to solve the tasks correctly. For the simpler system, there was a 20 percent decrease in time taken but no significant differences in correctness. However, the moderating effect of system complexity depends on the expertise of the subjects. The observed benefits of pair programming in terms of correctness on the complex system apply mainly to juniors, whereas the reductions in duration to perform the tasks correctly on the simple system apply mainly to intermediates and seniors. 
},
  journal = {IEEE Transactions on Software Engineering},
  volume = {33},
  number = {2},
  pages = {65-86}
}

@Article{Arisholm.2006.3,
  author = {Arisholm, Erik and Briand, Lionel Claude L and Hove, Siw Elisabeth and Labiche, Yvan},
  title = {The Impact of UML Documentation on Software Maintenance: An Experimental Evaluation},
  year = {2006},
  abstract = {The Unified Modeling Language (UML) is becoming the de-facto standard for software analysis and design modeling. However, there is still a significant resistance to model-driven development in many software organizations as it is perceived to be expensive and not necessarily cost-effective. It is therefore important to investigate the benefits obtained through modeling. As a first step in this direction, this paper reports on controlled experiments, spanning across two locations, which investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and past a certain learning curve, the availability of UML documentation may result in significant improvements of the functional correctness of changes as well as their design quality. On the other hand, there does not seem to be any resulting time saving. For simpler tasks, the time needed to update the UML documentation may be substantial compared with the potential benefits, thus motivating the need for UML tools with better support for software maintenance.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {32},
  number = {6},
  pages = {365-381}
}

@Inproceedings{Jorgensen.2006.7,
  author = {J{\o}rgensen, Magne},
  title = {Software Cost Estimation: When to Use Expert Judgment and When to Use Models},
  year = {2006},
  abstract = {},
  booktitle = {International Symposium on Forecasting},
  editor = {Antonio Garc{\a\'\i}a-Ferrer},
  pages = {53},
  publisher = {International Institute of Forecasters},
  isbn = {999-999-999}
}

@Inproceedings{Jorgensen.2006.8,
  author = {J{\o}rgensen, Magne},
  title = {A Preliminary Model of Judgment-based Project Software Effort Predictions},
  year = {2006},
  abstract = {An improvement of the software industry{\textquoteright}s software development effort estimation processes may benefit from a better understanding of the mental, partly unconscious, processes involved in estimating effort. This paper proposes and tests a theory potentially explaining essential parts of typical judgment-based effort estimation processes ({\textquotedblleft}expert estimation{\textquotedblright}). The theory is based on findings from the human judgment research literature and proposes that judgment-based effort estimation is based on: i) an early categorization of the project to be estimated, ii) a resistance towards a change of the chosen category, and, iii) a {\textquotedblleft}regression{\textquotedblright} of the effort estimate towards a reference value of the chosen category, where the amount of regression depends on the level of uncertainty of the project work. Implications of the theory are tested with results from three software effort estimation experiments. All examined studies confirmed the theory. There is, however, a strong need for more work, independent evidence and clearer description of scope and concepts part of the theory. Finally, we outline a study planned for further testing of essential parts of the theory.},
  booktitle = {IRNOP VIII, Project Research Conference},
  editor = {Lixiong Ou, Rodney Turner},
  pages = {661-668},
  publisher = {Publishing House of Electronci Industry},
  address = {Beijing},
  isbn = {7-121-03252-X}
}

@Inproceedings{Arisholm.2006.4,
  author = {Arisholm, Erik and Briand, Lionel Claude L},
  title = {Predicting Fault-prone Components in a Java Legacy System},
  year = {2006},
  abstract = {This paper reports on the construction and validation of fault-proneness prediction models in the context of an object-oriented, evolving, legacy system. The goal is to help QA engineers focus their limited verification resources on parts of the system likely to contain faults. A number of measures including code quality, class structure, changes in class structure, and the history of class-level changes and faults are included as candidate predictors of class fault-proneness. A cross-validated classification analysis shows that the obtained model has less than 20\% of false positives and false negatives, respectively. However, as shown in this paper, statistics regarding the classification accuracy tend to inflate the potential usefulness of the fault-proneness prediction models. We thus propose a simple and pragmatic methodology for assessing the cost-effectiveness of the predictions to focus verification effort. On the basis of the cost-effectiveness analysis we show that change and fault data from previous releases is paramount to developing a practically useful prediction model. When our model is applied to predict faults in a new release, the estimated potential savings in verification effort is about 29\%. In contrast, the estimated savings in verification effort drops to 0\% when history data is not included.},
  booktitle = {5th ACM-IEEE International Symposium on Empirical Software Engineering (ISESE), Rio de Janeiro, Brazil, September 21-22},
  editor = {Maldonado, Mendes \& Wohlin},
  pages = {8-17},
  publisher = {ACM},
  isbn = {1-59593-218-6}
}

@Mastersthesis{Tommerberg.2006.1,
  author = {T{\o}mmerberg, G{\o}ril},
  title = {Comprehension-Related Activities during Maintenance of Object-Oriented Systems: An In-DEpth Study},
  year = {2006},
  abstract = {},
  school = {University of Oslo, Department of Informatics},
  note = {Supervisor Amela Karahasanovic}
}

@Mastersthesis{Kvern.2006.1,
  author = {Kv{\ae}rn, Kaja},
  title = {Effects of Expertise and Strategies on Program Comprehension in Maintenance of Object-Oriented Systems: A Controlled Experiment with Professional Developers},
  year = {2006},
  abstract = {},
  school = {University of Oslo, Department of Informatics},
  note = {Supervisor Amela Karahasanovic}
}

@Mastersthesis{Vikskjold.2006.1,
  author = {Vikskjold, Espen Albert},
  title = {Measuring Time - A Systematic Survey of Controlled Experiments in Software Engineering },
  year = {2006},
  abstract = {},
  school = {Department of Informatics, University of Oslo},
  note = {Supervisor: Amela Karahasanovic,
Published: December 2005, Presentation: January 2006,
was not included in the 2005 Annual Report}
}

@Article{Dyba.2006.1,
  author = {Dyb{\r a}, Tore and Kampenes, Vigdis By and Sj{\o}berg, Dag I. K},
  title = {A systematic review of statistical power in software engineering experiments},
  year = {2006},
  abstract = {Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993-2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study{\textquoteright}s findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.},
  journal = {Journal of Information \& Software Technology},
  volume = {48},
  number = {8},
  pages = {745-755}
}

@Mastersthesis{Holt.2006.1,
  author = {Holt, Nina Elisabeth},
  title = {A Systematic Review of Case Studies in Software Engineering},
  year = {2006},
  abstract = {},
  school = {University of Oslo}
}

@Article{Hannay.2006.1,
  author = {Hannay, Jo E and Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore},
  title = {A Systematic Review of Theory Use in Software Engineering Experiments},
  year = {2007},
  abstract = {Empirically-based theories are generally perceived as foundational to
science. However, in many disciplines, the nature, role and even the
necessity of theories remain matters for debate, particularly in young or
practical disciplines such as software engineering.  This article
reports a systematic review of the explicit use of theory in a comprehensive
set of 103 articles reporting controlled experiments, from of a total
of 5,453 articles published in major software engineering journals and
conferences in the decade 1993-2002.  Of the 103 articles, 24 use a
total of 39 theories in various ways to explain the cause-effect
relationship(s) under investigation. The majority of these use theory in the
experimental design to justify research questions and hypotheses;
some use theory to provide \emph{post-hoc} explanations of their results;
while a few test or modify theory. A third of the theories are
proposed by authors of the reviewed articles. The interdisciplinary nature
of the theories used is greater than that of research in software
engineering in general. We found that theory use and awareness of
theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering.  Several
articles comment explicitly on the lack of relevant theory. We
call for an increased awareness of the potential benefits of involving
theory, when feasible. To support software engineering researchers who
wish to use theory, we give an overview that shows which of the reviewed
articles on which topics use which theories for what purposes, as well
as details of the theories' characteristics.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {33},
  number = {2},
  pages = {87--107}
}

@Inbook{Jorgensen.2006.9,
  author = {J{\o}rgensen, Magne and Sj{\o}berg, Dag},
  title = {Expert Estimation of Software Development Work},
  year = {2006},
  abstract = {},
  booktitle = {Software Evolution and Feedback: Theory and Practice},
  editor = {Nazim H.Madhavji, Juan Fernandez-Ramli, Dewayne Perry},
  publisher = {Wiley, ISBN-0-470-87180-6},
  chapter = {25},
  pages = {489-503}
}

@Misc{Jorgensen.2006.11,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Wie gross sind die kosten{\"u}berschreitungen tats{\"a}chlich},
  year = {2006},
  abstract = {},
  howpublished = {Article in OBJEKTspektrum (in German)}
}

@Misc{Jorgensen.2006.12,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {How Large Are Software Cost Overruns? A Review of the 1994 CHAOS Report},
  year = {2006},
  abstract = {},
  howpublished = {Article in Software Practitioner (Vol 16, no 4\&5, p 13-14)}
}

@Misc{Jorgensen.2006.13,
  author = {J{\o}rgensen, Magne},
  title = {How to achieve lower software bids and why not to do this},
  year = {2006},
  abstract = {},
  howpublished = {Article in Improve (Newsletter 2-2006, p 1-2)}
}

@Misc{Jorgensen.2006.14,
  author = {J{\o}rgensen, Magne},
  title = {Herre i eget hus?},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, nr 24, s. 40.}
}

@Article{Basili.2006.1,
  author = {Basili, Victor and Zelkowitz, Marvin and Sj{\o}berg, Dag I. K and Johnson, Philip and Cowling, Tony},
  title = {Protocols in the use of Empirical Software Engineering Artifacts},
  year = {2007},
  abstract = {If empirical software engineering is to grow as a valid scientific endeavor, the
ability to acquire, use, share, and compare data collected from a variety of sources must be
encouraged. This is necessary to validate the formal models being developed within
computer science. However, within the empirical software engineering community this has
not been easily accomplished. This paper analyses experiences from a number of projects,
and defines the issues, which include the following: (1) How should data, testbeds, and
artifacts be shared? (2) What limits should be placed on who can use them and how? How
does one limit potential misuse? (3) What is the appropriate way to credit the organization
and individual that spent the effort collecting the data, developing the testbed, and building
the artifact? (4) Once shared, who owns the evolved asset? As a solution to these issues, the
paper proposes a framework for an empirical software engineering artifact agreement. Such
an agreement is intended to address the needs for both creator and user of such artifacts and
should foster a market in making available and using such artifacts. If this framework for sharing software engineering artifacts is commonly accepted, it should encourage artifact
owners to make the artifacts accessible to others (gaining credit is more likely and misuse is
less likely). It may be easier for other researchers to request artifacts since there will be a
well-defined protocol for how to deal with relevant matters.},
  journal = {Journal of Empirical Software Engineering},
  volume = {12},
  number = {1},
  pages = {107{\textendash}119}
}

@Article{Krogstie.2006.1,
  author = {Krogstie, John and Jahr, Arthur and Sj{\o}berg, Dag I. K},
  title = {A Longitudinal Study of General and Functional  Maintenance in Norway},
  year = {2006},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {48},
  number = {11},
  pages = {993-1005}
}

@Article{Andrews.2006.1,
  author = {Andrews, James H and Briand, Lionel Claude L and Labiche, Yvan and Namin, Akbar Sami},
  title = {Using Mutation Analysis for Assessing and Comparing Testing Coverage Criteria},
  year = {2006},
  abstract = {The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (Block, Decision, C-Use, and P-Use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators is yielding trustworthy results: Generated mutants can be used to predict the detection effectiveness of real faults. Applying such a mutation analysis, we then investigate the relative cost and effectiveness of the above-mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants, which helps decrease the impact of random variation in our analysis and allows us to use a different analysis approach. Our results are then compared with published studies, plausible reasons for the differences are provided, and the research leads us to suggest a way to tune the mutation analysis process to possible differences in fault detection probabilities in a specific environment.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {32},
  number = {8},
  pages = {608-624}
}

@Inproceedings{Garousi.2006.2,
  author = {Garousi, Vahid and Briand, Lionel Claude L and Labiche, Yvan},
  title = {Analysis and Visualization of Behavioral Dependencies among Distributed Objects based on UML Models},
  year = {2006},
  abstract = {},
  booktitle = {ACM/IEEE MoDELS},
  editor = {Nierstrasz, Whittle, Harel, Reggio},
  pages = {365-379},
  publisher = {Springer},
  isbn = {3-540-45772-0}
}

@Inproceedings{Elaasar.2006.1,
  author = {Elaasar, Maged and Briand, Lionel Claude L and Labiche, Yvan},
  title = {A Metamodeling Approach to Pattern Specification and Detection},
  year = {2006},
  abstract = {},
  booktitle = { Proc. of ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MoDELS)},
  editor = {Nierstrasz, Whittle, Harel, Reggio},
  pages = {484-498},
  publisher = {Springer},
  isbn = {3-540-45772-0}
}

@Misc{Molokken-ostvold.2006.1,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Overskridelser og skandaler i offentlige IT-prosjekter - Hvorfor vi heller ikke kommer til {\r a} l{\ae}re av problemene til Golf, MinSide og Flexus.},
  year = {2006},
  abstract = {},
  howpublished = {Presentation for Abelia's 5th anniversary seminar},
  note = {Se also published newsarticle in dagensit.no (http://www.dagensit.no/bedrifts-it/article803978.ece)}
}

@Inproceedings{Holt.2006.2,
  author = {Holt, Nina Elisabeth and Anda, Bente Cecilie Dahlum and Asskildt, Knut and Briand, Lionel Claude L and Endresen, Jan and Fr{\o}ystein, Sverre},
  title = {Experiences with Precise State Modeling in an Industrial Safety Critical System},
  year = {2006},
  abstract = {The development of safety critical systems is a complex and challenging task. There are many claims regarding how precise modeling, either with the UML or other modeling notations, can yield benefits in terms of more rigorous specifications and designs. This paper reports on experiences from applying statechart-driven UML modeling in the development of a safety-critical system at ABB. The primary lessons learned from the study are related to the impact of precise modeling on the ease of transitioning to design.},
  booktitle = {Critical Systems Development Using Modeling Lanuguages, CSDUML'06},
  editor = {Siv Hilde Houmb, Geri Georg, Robert France, Dorina C. Petriu, and Jan J{\"u}rjens},
  chapter = {6},
  pages = {68-77},
  publisher = {Springer},
  edition = {9th edition},
  isbn = {0809-1021}
}

@Article{Li.2006.1,
  author = {Li, Jingyue and Bj{\o}rnson, Finn Olav and Conradi, Reidar and Kampenes, Vigdis By},
  title = {An Empirical Study of Variations in COTS-based Software Development Processes in Norwegian IT Industry},
  year = {2006},
  abstract = {},
  journal = {International Journal on Empirical Software Engineering},
  volume = {11},
  number = {3},
  pages = {433-461}
}

@Misc{Haugen.2006.1,
  author = {Haugen, Nils Christian and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Planning Poker {\textendash} Playing for better estimates},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at JavaZone}
}

@Article{Karahasanovic.2006.4,
  author = {Karahasanovic, Amela and Levine, Annette Kristin and Thomas, Richard},
  title = {Comprehension strategies and difficulties in maintaining object-oriented systems: an explorative study},
  year = {2007},
  abstract = {Program comprehension is a major time-consuming activity in software maintenance. Understanding the underlying mechanisms of program comprehension is therefore necessary for improving software maintenance. It has been argued that acquiring knowledge of how a program works before modifying it (the systematic strategy) is unrealistic in larger programs. The goal of the experiment presented in this paper is to explore this claim. The experiment examines strategies for program comprehension and cognitive difficulties of developers who maintain an unfamiliar object-oriented system. The subjects were 38 students in their third or fourth year of study in computer science. They used a professional Java tool to perform several maintenance tasks on a medium-size Java application system in a six-hour long experiment. The results showed that the subjects who applied the systematic strategy were more likely to produce correct solutions. Two major groups of difficulties were related to the comprehension of the application structure, namely to the understanding of GUI implementation and OO comprehension and programming. Acquisition of strategic knowledge might improve program comprehension in software maintenance.},
  journal = {The Journal of Systems and Software},
  volume = {80},
  number = {9},
  pages = {1541-1559}
}

@Article{Karahasanovic.2006.5,
  author = {Karahasanovic, Amela and Hinkel, Unni Nyhamar and Sj{\o}berg, Dag I. K and Thomas, Richard},
  title = {Comparing of Feedback Collection and Think-Aloud Methods in Program Comprehension Studies},
  year = {2007},
  abstract = {This paper reports an explorative experimental comparison of (i) an experience-sampling method called feedback collection and (ii) the think-aloud methods with respect to their usefulness in studies on program comprehension. Think-aloud methods are widely used in studies of cognitive processes, including program comprehension. Alternatively, as in the feedback-collection method (FCM), cognitive processes can be traced by collecting written feedback from the subjects at regular intervals. We compare FCM with concurrent think-aloud (CTA) and retrospective think-aloud (RTA) regarding type and usefulness of the collected information, costs related to analysis of the collected information, and effects of the data collection methods on the subjects{\textquoteright} performance. FCM allowed us to identify a greater number of comprehension problems that prevented progress or caused significant delay (FCM: 30 problems; CTA: 5; RTA: 15). It was less precise in identifying strategies for comprehension than CTA (92\% correctness for FCM; 100\% for CTA). FCM was less expensive in analysis (transcription and coding) than the other two methods (FCM: 0,7 hours of analysis per protocol; CTA: 31 hours; RTA: 7,9 hours). The results indicate that all three methods of data collection were intrusive and affected the performance of the subjects with respect to time and correctness (small to medium effect size). This research confirms that FCM can be used beneficially in studies that trace the cognitive processes involved in, and identify problems related to, the comprehension of software applications. Based on our experience, we recommend that FCM be used in studies that have a large number of subjects and as a complement to other methods for tracing cognitive processes, such as user log files. We recommend a design with two groups (verbalization and silent control) and a pretest task to be used in studies with FCM or CTA that focus on performances. },
  journal = {Journal of Behaviour \& Information Technology}
}

@Article{Arisholm.2006.5,
  author = {Arisholm, Erik},
  title = {Empirical Assessment of the Impact of Structural Properties on the Changeability of Object-Oriented Software},
  year = {2006},
  abstract = {Evolutionary software development processes mainly address concerns regarding client requirements and risk management. A side-effect of allowing frequent, and often unanticipated, modifications to the software is that they may result in poor structure, because the designers may not have anticipated the final set of requirements that would be implemented. By monitoring structural properties of the software one may be able to identify and correct design problems before they become unmanageable. 
Ways in which degradations in structural properties of the software can be measured are described and empirically validated, based on data collected from an industrial Java development project. The measures are validated by using them as candidate variables in a prediction model of the actual effort required to make modifications to the evolving software system. 
The results suggest that some measures that combine existing structural attribute measures with a weighting factor based on the relative proportion of change in each class are reasonably accurate predictors of change effort. This constitutes initial, empirical evidence that the proposed measures are valid quality indicators. Consequently, they may help designers to identify and correct design problems during the evolutionary development of object-oriented software.},
  journal = {Information and Software Technology},
  volume = {48},
  number = {11},
  pages = {1046-1055}
}

@Inproceedings{Karahasanovic.2007.1,
  author = {Karahasanovic, Amela and Thomas, Richard},
  title = {Difficulties experienced by students in maintaining object-oriented systems: an empirical study},
  year = {2007},
  abstract = {It is widely accepted that software maintenance absorbs a
significant amount of the effort expended in software
development. Proper training of both university students
and professional developers is required in order to
improve software maintenance. Understanding cognitive
difficulties the students have while maintaining objectoriented
systems is a prerequisite for improving their
university education and preparing them for jobs in
industry. The goal of the experiment reported in this paper
is to explore the difficulties of students who maintain an
unfamiliar object-oriented system. The subjects were 34
students in their third year of study in computer science.
They used a professional Java tool to perform several
maintenance tasks on a medium-size Java application
system in a seven-hour long experiment. The major
difficulties were related to understanding program logic,
algorithms, finding change impacts, and inheritance of the
functionality. Based on these results we suggest teaching
the basics of impact analysis and introducing examples of
modifying larger object-oriented programs in courses on
object-oriented programming.},
  booktitle = {Australasian Computing Education Conference ACE 2007},
  editor = {Mann, S. and Simon},
  pages = {81--87},
  publisher = {Australian Computer Society Inc., Vol 66},
  isbn = {1-920-68247-3}
}

@Inbook{Anda.2006.2,
  author = {Anda, Bente Cecilie Dahlum and Hansen, Kai and Gullesen, Ingolf and Thorsen, Hanne Kristin},
  title = {Experiences from Introducing UML-based Development in a Large Safety-Critical Project},
  year = {2006},
  abstract = {},
  booktitle = {Software Process Improvement. Results and Experiences from the Field},
  editor = {Reidar Conradi, Tore Dyb{\r a}, Dag I.K. Sj{\o}berg, Tor Ulsund},
  publisher = {Springer},
  chapter = {18},
  pages = {329-359},
  isbn = {3-540-32178-0}
}

@Article{Kampenes.2006.1,
  author = {Kampenes, Vigdis By and Dyb{\r a}, Tore and Hannay, Jo Erskine and Sj{\o}berg, Dag I. K},
  title = {A Systematic Review of Effect Size in Software Engineering Experiments},
  year = {2007},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {49},
  number = {11-12},
  pages = {1073-1086}
}

@Misc{Molokken-ostvold.2006.3,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Combining expert estimates},
  year = {2006},
  abstract = {},
  howpublished = {Presentation during MeLLow Workshop, Brunel University, London.}
}

@Book{Shull.2007.1,
  editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K},
  title = {Advanced Topics in Empirical Software Engineering},
  year = {2008},
  abstract = {Empirical studies have become an integral element of software engineering
research and practice. This unique text/reference includes chapters from
some of the top international empirical software engineering researchers and
focuses on the practical knowledge necessary for conducting, reporting and
using empirical methods in software engineering.},
  publisher = {Springer-Verlag London},
  isbn = {13:978-1-84800-043-8}
}

@Inbook{Sjoberg.2007.1,
  author = {Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore and Anda, Bente Cecilie Dahlum and Hannay, Jo Erskine},
  title = {Building Theories in Software Engineering},
  year = {2008},
  abstract = {Empirical studies have become an integral element of software engineering
research and practice. This unique text/reference includes chapters from
some of the top international empirical software engineering researchers and
focuses on the practical knowledge necessary for conducting, reporting and
using empirical methods in software engineering.},
  booktitle = {Advanced Topics in Empirical Software Engineering},
  editor = {Forrest Shull, Janice Singer, Dag I.K. Sj{\o}berg},
  publisher = {Springer-Verlag London},
  isbn = {13:978-1-84800-043-8}
}

@Misc{Sjoberg.2002.2,
  author = {Sj{\o}berg, Dag},
  title = {Obituary - Kristen Nygaard},
  year = {2002},
  abstract = {}
}

@Article{Jorgensen.2007.1,
  author = {J{\o}rgensen, Magne and Shepperd, Martin},
  title = {A Systematic Review of Software Development Cost Estimation Studies},
  year = {2007},
  abstract = {This paper aims to provide a basis for the improvement of software estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. Based on the review, we provide recommendations for future software cost estimation research: 1) Increase the breadth of the search for relevant studies, 2) Search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) Conduct more research on basic software cost estimation topics, 4) Conduct more studies of software cost estimation in real-life settings, 5) Conduct more studies on estimation methods commonly used by the software industry, and, 6) Conduct fewer studies that evaluate methods based on arbitrarily chosen data sets.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {33},
  number = {1},
  pages = {33-53}
}

@Article{Grimstad.2007.1,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {Inconsistency in Expert Judgment-based Estimates of Software Development Effort },
  year = {2007},
  abstract = {Effort estimation of software development work is partly based on non-mechanical and unconscious processes, i.e., human judgment. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions may lead to different effort estimates.  In this paper, we report from an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the tasks were estimated twice. We found a high degree of inconsistency in a subject{\textquoteright}s effort estimates of the same task. The mean difference of the effort estimates of the same task by the same estimator was as much as 71\%! Surprisingly, the software professionals{\textquoteright} estimation accuracy on previously completed development tasks did not correlate with their degree of estimation inconsistency in the experiment. We discuss reasons for and consequences of our findings. },
  journal = {Journal of Systems and Software},
  volume = {80},
  number = {11},
  pages = {1770--1777}
}

@Article{Gruschke.2007.1,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {Assessing Uncertainty of Software Development Effort Estimates: Learning from Outcome Feedback},
  year = {2008},
  abstract = {To enable properly-sized software project budgets and plans it is important to be able to assess the uncertainty of the estimates of the most likely effort required to complete the projects. Previous studies show that software professionals tend to be too optimistic about the uncertainty of their effort estimates, i.e., they tend to be overconfident. This paper reports the results from a study on the role of outcome feedback in the uncertainty assessment learning process. Software developers were given repeated and immediate outcome feedback about the discrepancy between the estimated most likely effort and the actual effort. We found that one condition for the improvement of uncertainty assessments of effort estimates may be the use of explicitly-formulated strategies for uncertainty assessment. By contrast, intuition-based uncertainty assessment strategies may lead to no or little improvement. This has the implication that we cannot expect much improvement in uncertainty assessment unless we train and instruct software developers to use the previous estimation accuracy experience as input to an explicitly defined uncertainty assessment process.},
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {17},
  number = {4},
  pages = {20-35}
}

@Article{Jorgensen.2007.2,
  author = {J{\o}rgensen, Magne},
  title = {Estimation of Software Development Work Effort:Evidence on Expert Judgment and Formal Models},
  year = {2007},
  abstract = {The main goal of the review presented in this paper is to examine when to use expert judgment, when to use formal models, and, when to combine these two approaches when estimating software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgment-based effort estimates was better than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgment-based effort estimates seem to be that: i) the estimation models are not calibrated to the organization using the model, and, ii) the experts possess important context information not included in the formal estimation models. The use of models, on the other hand, may be particularly useful in estimation situations believed to lead to stronger than usual degree of over-optimism, particularly very large projects. Five of the reviewed studies evaluated estimates based on a combination of expert judgment and model. In all five studies the average accuracy of combination-based effort estimates had similar or more accurate estimates than the average accuracy of the expert estimates and of the best model.},
  journal = {International Journal of Forecasting},
  volume = {23},
  number = {3},
  pages = {449-462}
}

@Article{Jorgensen.2007.3,
  author = {J{\o}rgensen, Magne and Faugli, Bj{\o}rn and Gruschke, Tanja Milijana},
  title = {Characteristics of Software Engineers with Optimistic Predictions},
  year = {2007},
  abstract = {This paper examines to which degree level of optimism in software engineers{\textquoteright} predictions is related to optimism on previous predictions, scores on tests of general level of optimism (explanatory style, life orientation and self-assessed optimism), development skill, confidence in accuracy of own predictions, and, ability to recall effort used on previous tasks. Results from four experiments suggest that more optimistic software engineers are characterized by more optimistic previous predictions, higher confidence in accuracy of own predictions, lower development skills, poorer ability to recall effort on previous tasks, and, higher optimism scores. A substantial part of the variation of level of optimism seems, however, to be random.},
  journal = {Journal of Systems and Software},
  volume = {80},
  number = {9},
  pages = {1472-1482}
}

@Inproceedings{Grimstad.2007.2,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {The Impact of Irrelevant Information on Estimates of Software Development Effort},
  year = {2007},
  abstract = {Software professionals typically estimate software development effort based on a requirement specification. Parts of this specification frequently contain information that is irrelevant to the estimation of the actual effort involved in the development of software. We hypothesize that effort-irrelevant information sometimes has a strong impact on effort estimates. To test this hypothesis, we conducted two controlled experiments with software professionals. In each of the experiments, the software professionals received specifications describing the same requirements. However, we gave one group of the software professionals a version of the requirement specification where we had included additional, effort-irrelevant, information. In both experiments we observed that the estimates of most likely effort increased when the estimates were based on requirement specifications that contained the information irrelevant to development effort. The results suggest that when estimation-irrelevant information is included as input to expert judgment-based estimation processes, the estimators find it difficult to distinguish between the estimation-relevant and the estimation-irrelevant information. A possible consequence of our findings is that estimation-irrelevant information should be removed from the requirement specification prior to the use of it as input to estimation work.},
  booktitle = {The Australian Software Engineering Conference (Paper received "Best Paper Award")},
  editor = {Doug Grant},
  pages = {359-368},
  publisher = {IEEE Computer Society},
  isbn = {999},
  note = {Awarded a prize for the conference's best paper.}
}

@Article{Anda.2006.3,
  author = {Anda, Bente Cecilie Dahlum and Hansen, Kai and Gullesen, Ingolf and Thorsen, Hanne Kristin},
  title = {Experiences from Using a UML-based Development Method in a Large Safety-Critical Project},
  year = {2006},
  abstract = {},
  journal = {Empirical Software Engineering},
  volume = {11},
  number = {4},
  pages = {555-581}
}

@Misc{Molokken-ostvold.2006.2,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and Haugen, Nils Christian},
  title = {Know when to hold 'em, know when to fold 'em - Combining estimates with Planning Poker},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at Simula's annual estimation seminar}
}

@Misc{Molokken-ostvold.2006.5,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Kombinasjon av estimater - en enkel og praktisk metode for {\r a} {\o}ke presisjonen},
  year = {2006},
  abstract = {},
  howpublished = {Presentation for IKT-Norge/REFLEKS-project}
}

@Misc{Jorgensen.2006.5,
  author = {J{\o}rgensen, Magne},
  title = {Min venstre hjernehalvdel h{\o}rer ikke p{\r a} meg},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, No 42, p 40}
}

@Misc{Jorgensen.2006.6,
  author = {J{\o}rgensen, Magne},
  title = {L{\ae}ring av erfaring},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, No 30, p 36}
}

@Misc{Jorgensen.2006.10,
  author = {J{\o}rgensen, Magne},
  title = {En jungel av forskningsresultater},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, No 36, p 40}
}

@Misc{Jorgensen.2006.15,
  author = {J{\o}rgensen, Magne},
  title = {Software cost estimation research at Simula Research Laboratory},
  year = {2006},
  abstract = {},
  howpublished = {Seminar presentation at New Bulgarian University, Sofia}
}

@Misc{Jorgensen.2006.16,
  author = {J{\o}rgensen, Magne},
  title = {Hvor rasjonelle er v{\r a}re beslutninger ved valg av metoder og verkt{\o}y?},
  year = {2006},
  abstract = {},
  howpublished = {Keynote at SPIKE conference, Oslo, April 4}
}

@Misc{Jorgensen.2006.17,
  author = {J{\o}rgensen, Magne},
  title = {Overoptimisme i IT-prosjekter: Hvorfor l{\ae}rer vi aldri?},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at OMG-seminar, Oslo, April 19}
}

@Misc{Jorgensen.2006.19,
  author = {J{\o}rgensen, Magne},
  title = {Overoptimisme i IT-prosjekter?},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at Veivesenets 11th summer meeting, Svolv{\ae}r, Juni 1}
}

@Misc{Jorgensen.2006.20,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {N{\r a}r skal vi bruke hodet og n{\r a}r skal vi bruke estimeringsmodeller?},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at JavaZone, September 13}
}

@Misc{Jorgensen.2006.21,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Hvordan bli bedre til {\r a} estimere? (Inkludert NM i estimering)},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at JavaZone, September 13}
}

@Misc{Jorgensen.2006.22,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {How to avoid impact from irrelevant and misleading information on your cost estimates},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at Simula's Annual Estimation Seminar, November 21}
}

@Misc{Jorgensen.2006.23,
  author = {J{\o}rgensen, Magne},
  title = {Hvorfor kostnadsestimater er overoptimistiske. Kundens ansvar er sterkt undervurdert!},
  year = {2006},
  abstract = {},
  howpublished = {Presenation at IKT-Norge seminar, Desember 5}
}

@Inproceedings{Gruschke.2006.5,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {To know or not to know: when does feedback lead to better assessment of uncertainty of own beliefs?},
  year = {2006},
  abstract = {People are frequently overconfident about the accuracy of their own beliefs. The goal of this experiment is to examine whether, and if so under what conditions, very large amounts of feedback lead to better assessments of the uncertainty of one{\textquoteright}s own beliefs. Fifteen participants answered the same 960 general knowledge questions over two days. Questions were selected from the board game {\textquotedblleft}Who wants to be a millionaire?{\texttrademark}{\textquotedblright}. Each question had four answer alternatives. When an answer alternative had been chosen, the participants were asked to assess the probability that it was the correct answer. They did this by choosing from a list of predefined confidence intervals. The questions belonged to one out of six difficulty categories, as decided by the board game developers. Participants answered piles of 80 questions of the same difficulty at a time. Feedback about the correct answer was received immediately, and feedback about correspondence between {\textquotedblleft}hit rate{\textquotedblright} and confidence level was received immediately after a pile of questions had been answered. In addition, a summary of the first day{\textquoteright}s performance was provided at the start of Day 2. We found that thirteen out of the fifteen participants improved in the correspondence between hit rate and confidence level on the second day, which suggests that the feedback had an effect. The strongest improvement was achieved on questions with high {\textquotedblleft}global{\textquotedblright} difficulty, i.e., high difficulty as assessed by the board game developers, and low {\textquotedblleft}internal{\textquotedblright} difficulty, i.e., low difficulty as perceived by the participants.},
  booktitle = {IAREP/SABE congress (Behavioural Economics and Economic Psychology)},
  editor = {Christine Roland-L{\a\'e}vy},
  pages = {O-226 },
  publisher = {Elsevier},
  isbn = {999-999-999}
}

@Inproceedings{Molokken-ostvold.2007.1,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and Haugen, Nils Christian},
  title = {Combining Estimates with Planning Poker {\textendash} An Empirical Study},
  year = {2007},
  abstract = {Combination of expert opinion is frequently used to produce estimates in software projects. However, if, when and how to combine expert estimates, is poorly understood. In order to study the effects of a combination technique called planning poker, the technique was introduced in a software project for half of the tasks. The tasks estimated with planning poker provided: 1) group consensus estimates that were less optimistic than the mechanical combination of individual estimates for the same tasks, and 2) group consensus estimates that were more accurate than the mechanical combination of individual estimates for the same tasks. The set of control tasks in the same project, estimated by individual experts, achieved similar estimation accuracy as the planning poker tasks. However, for both planning poker and the control group, the median estimation bias indicated that both groups had unbiased estimates, as the typical estimated task was perfectly on target.},
  booktitle = {18th Australian Conference on Software Engineering},
  editor = {Doug Grant, Swinburne University of Technology, Australia },
  pages = {349-358},
  publisher = {IEEE},
  isbn = {0-7695-2778-7}
}

@Inproceedings{Simula.SE.3,
  author = {Wang, Alf Inge and Arisholm, Erik and Jaccheri, Letizia},
  title = {Educational Approach to an Experiment in a Software Archiecture Course},
  year = {2007},
  abstract = {This paper reports experiences from an experiment in a software architecture course where the
focus was both on giving students valuable education as well as getting important empirical results.
The paper describes how the experiment was integrated in the course, and presents an evaluation
of the experiment from an educational point of view. Further, the paper reflects on the costs and
the benefits of carrying out an experiment in the context of a software architecture course for the
involved stakeholders namely the researchers, the students, and the instructors. The main results
show that we managed to integrate the experiment in the course, that the students learned several
aspects of software architecture through the experiment, and that the main motivation for putting
effort into the experiment was to prove own skills. We also describe some guidelines for planning
and executing experiments as a part of a software engineering course.},
  booktitle = {20th Conference on Software Engineering Education and Training (CSEE\&T 2007)},
  editor = {?},
  pages = {291-300},
  publisher = {IEEE Computer Society},
  isbn = {0-7695-2893-7}
}

@Article{Simula.SE.9,
  author = {Hannay, Jo Erskine and J{\o}rgensen, Magne},
  title = {The Role of Artificial Design Elements in Software Engineering Experiments},
  year = {2008},
  abstract = {Increased realism in software engineering experiments is often promoted as an important means to increase generalizability and industrial relevance. In this context, artificiality, e.g., the use of constructed tasks in place of realistic tasks, is seen as a threat. In this article, we examine the opposite view, that deliberately introduced artificial design elements may increase knowledge gain and enhance both generalizability and relevance. In the first part of the article, we identify and evaluate arguments and examples in favor of, and against, deliberately introducing artificiality into software engineering experiments. In the second part of the article, we summarize a content analysis of articles reporting software engineering experiments published over the ten-year period 1993-2002. The analysis reveals a striving for realism and external validity, but little awareness of for what and when, various degrees of artificiality and realism are appropriate. We conclude that an increased awareness and deliberation in these respects is essential. However, arguments in favor of artificial design elements should not be used to justify studies that are badly designed or that have research questions of low relevance.},
  journal = {Transactions on Software Engineering},
  volume = {34},
  number = {2},
  pages = {242--259}
}

@Inbook{Simula.SE.10,
  author = {Sj{\o}berg, Dag I. K},
  title = {Documenting Theories},
  year = {2007},
  abstract = {},
  booktitle = {Empirical Software Engineering Issues: Critical Assessment and Future Directions},
  editor = {Basili, V.R., Rombach, D., Schneider, K., Kitchenham, B., Pfahl, D. and Selby, R.},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  series = {LNCS 4336},
  pages = {111-114},
  isbn = {0302-9743}
}

@Inbook{Simula.SE.11,
  author = {Sj{\o}berg, Dag I. K},
  title = {Knowledge Acquisition in Software Engineering Requires Sharing of Data and Artifacts},
  year = {2007},
  abstract = {},
  booktitle = {Empirical Software Engineering Issues: Critical Assessment and Future Directions},
  editor = {Basili, V.R., Rombach, D., Schneider, K., Kitchenham, B., Pfahl, D. and Selby, R.},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  series = {LNCS 4336},
  pages = {77-82},
  isbn = {0302-9743}
}

@Inproceedings{Simula.SE.13,
  author = {Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore and J{\o}rgensen, Magne},
  title = {The Future of Empirical Methods in Software Engineering Research},
  year = {2007},
  abstract = {We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research. },
  booktitle = {Future of Software Engineering (FOSE '07)},
  editor = {Briand L. and Wolf A.},
  pages = {358-378},
  publisher = {IEEE-CS Press},
  isbn = {432663526}
}

@Inproceedings{Simula.SE.14,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and Furulund, Kristian Marius},
  title = {The Relationship between Customer Collaboration and Software Project Overruns},
  year = {2007},
  abstract = {Most agile projects rely heavily on good collaboration with the customer in order to achieve project goals and avoid overruns. However, the role of the customer in software projects is not fully understood. Often, successful projects are attributed to developer competence, while unsuccessful projects are attributed to customer incompetence. A study was conducted on eighteen of the latest projects of a software contractor. Quantitative project data was collected, and project managers interviewed, on several issues related to estimates, key project properties, and project outcome. It was found that in projects where collaboration was facilitated by daily communication between the contractor and the customer, they experienced a lesser magnitude of effort overruns. In addition, employing a contract that facilitates risk-sharing may also have a positive impact.},
  booktitle = {Agile 2007},
  editor = {Melnik},
  pages = {72-83},
  publisher = {IEEE},
  isbn = {0-7695-2872-4}
}

@Phdthesis{Simula.SE.16,
  author = {Grimstad, Stein},
  title = {Software Effort Estimation Error},
  year = {2006},
  abstract = {The goal of the thesis is to contribute to reducing the estimation error in software development projects by coming to a better understanding of the shortcomings of, and how to improve, expert judgment-based processes of effort estimation. The following topics are addressed:

{\textbullet} When, and how much, do judgmental biases and inconsistencies in effort estimates impact the estimation error?
{\textbullet} What is the software clients' role in reducing effort estimation error in software development projects?
{\textbullet} What is the state of practice of software effort estimation error measurement and analysis?},
  school = {University of Oslo}
}

@Techreport{Simula.ND.24,
  author = {Kvalbein, Amund and Lysne, Olav},
  title = {Robust Load Balancing using Multi-Topology Routing},
  year = {2007},
  abstract = {Current methods for traffic engineering with traditional link state
  routing protocols like OSPF and IS-IS are based on optimizing link
  weights to fit a given estimate of the traffic demands. These
  methods are not good at handling natural changes in the traffic over
  time. In this paper, we introduce new methods for IGP load balancing
  based on the upcoming standard for Multi-Topology routing. The main
  advantage of our approach is that it is far more robust to changes
  in the traffic demands. Our initial evaluations indicate that our
  method significantly reduces the chances of losing packets due to
  congestion.},
  institution = {Simula},
  number = {2007-03}
}

@Inproceedings{Simula.SC.20,
  author = {Al-Khayat, Omar and Bruaset, Are Magnus and Langtangen, Hans Petter},
  title = {Lattice Boltzmann method and Turbidity Flow Modeling},
  year = {2007},
  abstract = {We discuss the Lattice Boltzmann method (LBM), which provides a new way of thinking
on complex, multiphase fluid flow problems. We present the basics of the LBM as well as its
origins in the Cellular Automata. The particle-based nature of the LBM makes it well suited to
study fluid-boundary interfaces. We then commence to sketch advanced applications like particle
suspension problems and we show how the LBM can be applied on turbidity currents, a challenging
fluid flow problem. We show that the LBM has a good deal of versatility in modeling physical
systems.},
  booktitle = {MekIT'07 : KONFERANSE I BEREGNINGSORIENTERT MEKANIKK (COMPUTATIONAL MECHANICS)},
  editor = {B. Skallerud and H. I. Andersson},
  pages = {213--228},
  publisher = {Tapir Academic Press},
  isbn = {978-82-519-2235-7}
}

@Article{Simula.SC.21,
  author = {Karper, Trygve and Mardal, Kent-Andre and Winther, Ragnar},
  title = {Unified finite element discretizations of coupled Darcy-Stokes flow},
  year = {2007},
  abstract = {},
  journal = {Numerical Methods for Partial Differential Equations}
}

@Inproceedings{Simula.SC.11,
  author = {Petersen, Steen Agerlin and Hjelle, {\O}yvind and Jensen, Susanne Lund},
  title = {Earth Modelling using Distance Fields Derived by Fast Marching},
  year = {2007},
  abstract = {A fast methodology to distribute properties of any geological complexity in 2D and 3D space is presented. The method uses combinations of distance-fields (and other attributes) derived by Fast Marching Level Set techniques from simpler geometries in 2D and 3D to populate space with relevant physical properties.
The paper presents how the methodology works for different geometries and the result of applying combined distance fields to describe realistic and complicated geological situations.},
  booktitle = {EAGE 69th Conference \& Exhibition, London},
  note = {Extended abstract}
}

@Article{Simula.SC.16,
  author = {Aln{\ae}s, Martin Sandve and Isaksen, J{\o}rgen Gjernes and Mardal, Kent-Andre and Romner, Bertil and Morgan, Michael and Ingebrigtsen, Tor},
  title = {Computation of hemodynamics in the circle of Willis},
  year = {2007},
  abstract = {},
  journal = {Stroke},
  volume = {38},
  number = {9},
  pages = {2500--2505},
  pmid = {17673714}
}

@Misc{Simula.SC.17,
  author = {Cai, Xing},
  title = {Building hybrid parallel PDE software by domain decomposition and object-oriented programming},
  year = {2007},
  abstract = {},
  howpublished = {Talk at the ICCM 2007 Conference, April 4-6, Hiroshima, Japan}
}

@Article{Simula.SE.112,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {How to Avoid Impact from Irrelevant and Misleading Information When Estimating Software Development Effort},
  year = {2008},
  abstract = {Software development effort estimates are reported to be highly inaccurate and systematically over-optimistic. We provide empirical evidence that suggests that this problem is caused, to some extent, by the influence of irrelevant and misleading information, e.g., information about the client{\textquoteright}s budget, present in the estimation material. The only really effective means of eliminating this influence is to avoid exposure to this type of information. Other means, such as more use of formal effort estimation models, improved analysis of requirement specifications, and better selection of estimators, had a positive effect but did not eliminate the influence. We propose process elements that are designed to avoid irrelevant and misleading information and illustrate how this process may lead to more realism in effort estimation.},
  journal = {IEEE Software},
  number = {May/June},
  pages = {78-83}
}

@Inproceedings{Simula.SE.113,
  author = {J{\o}rgensen, Magne},
  title = {Individual Differences in How Much People are Affected by Irrelevant and Misleading Information},
  year = {2007},
  abstract = {People differ in how much they update their beliefs based on new information. According to a recently proposed theory, individual differences in belief updating are, to some extent, determined by neurological differences, i.e., differences in the organization of the brain. The same neurological differences that affect belief updating may also affect handedness. In particular, more mixed-handed people may have a lower threshold for updating beliefs than strongly right-handed people. On the basis of the proposed theory, we hypothesize that mixed-handed software engineers will be more affected by irrelevant and misleading information when providing expert judgments. This hypothesis is tested in five experiments conducted in software engineering contexts. All five experiments supported the hypothesis and suggest that a low threshold for updating beliefs, as measured by degree of mixed-handedness correlates with inaccurate judgment in situations that contain irrelevant or misleading information. On the basis of the results, we argue that software engineering decisions, problem solving and estimation processes should take into account differences in individuals{\textquoteright} threshold for updating belief and not be based on the assumption that one {\textquotedblleft}process fits all{\textquotedblright}.},
  booktitle = {Second European Conference on Cognitive Science},
  editor = {Stella Vosniadou, Daniel Kayser, Athanassios Protopapaps},
  pages = {347-352},
  publisher = {Hellenic Cognitive Science Society},
  isbn = {978-1-84169-696-6}
}

@Inproceedings{Simula.ND.21,
  author = {Alia, Mourad and Hallsteinsen, Svein and Paspallis, Nearchos and Eliassen, Frank},
  title = {Managing Distributed Adaptation of Mobile Applications},
  year = {2007},
  abstract = {Mobile computing is characterised by variations in user needs and in 
the computing and communication resources. We have developed a middleware 
centric approach for the development of software capable of dynamically adapting to such variations. The middleware leverages models of needs and resources and the adaptation capabilities of the software and performs context monitoring, adaptation planning and dynamic reconfiguration at runtime. In this paper we focus on the modelling of resources of a distributed mobile computing infrastructure and how the resource model is used in adaptation planning. We present a distributed resource management framework and mechanisms necessary to maintain an up to date resource model at runtime. The challenge is to balance the level of abstraction so as to hide some of the heterogeneity of the actual infrastructure while retaining sufficient detail to serve the needs of distributed and centralized adaptation planning. The proposed framework is evaluated by a walkthrough of an example. },
  booktitle = {7th IFIP International Conference on Distributed Applications and Interoperable Systems (DAIS)},
  editor = {Jadwiga Indulska, Kerry Raymond},
  chapter = {8},
  pages = {104-118},
  publisher = {Springer Verlag },
  series = {LNCS 4531},
  isbn = {978-3-540-72881-8}
}

@Article{Simula.SC.12,
  author = {Lines, Glenn Terje and Linge, Svein and MacLachlan, Mary and Tveito, Aslak},
  title = {Incorporating temporal measurements into a mathematical model of atrial el
ectrical activity},
  year = {2007},
  abstract = {When the function of the atria is hampered by irregular electrical activation, radiofrequency ablation therapy can be used to restore normal activation and rhythm. Electroanatomical mapping  systems provide the clinician with a picture of the activation sequence, from which he/she can determine where to ablate.  The purpose of this paper is to present an algorithm for incorporating data from these mapping systems into a numerical simulator of the atria.
The method is applied to 3D test problems with synthetic mapping data. Convergence to the correct solution is observed provided that a sufficient number of mapping sites are used. For our simulations, we use the monodomain model coupled with FitzHugh-Nagumo cell dynamics.},
  journal = {journal for publication}
}

@Misc{Simula.SC.13,
  author = {Cai, Xing and Langtangen, Hans Petter},
  title = {On a future software platform for demanding multi-scale and multi-physics problems},
  year = {2007},
  abstract = {},
  howpublished = {Talk at SIAM CSE07 Conference, Costa Mesa, CA, Feb. 19-23}
}

@Misc{Simula.SC.5,
  author = {Mardal, Kent-Andre},
  title = { SyFi - A package for symbolic finite element computations},
  year = {2006},
  abstract = {},
  howpublished = {Presented at the Fenics06 workshop in Delft, Netherlands, 2006.},
  note = {
}
}

@Inproceedings{Simula.SE.17,
  author = {Arisholm, Erik and Briand, Lionel Claude L and Fuglerud, Magnus J},
  title = {Data Mining Techniques for Building Fault-proneness Models in Telecom Java Software},
  year = {2007},
  abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high probability of fault. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and inspections and would like to be able to devote extra resources to faulty system parts. The main research focus of this paper is two-fold: (1) use and compare many data mining and machine learning techniques to build fault-proneness models based mostly on source code measures and change/fault history data, and (2) demonstrate that the usual classification evaluation criteria based on confusion matrices may not be fully appropriate to compare and evaluate models.},
  booktitle = {18th International Symposium on Software Reliability Engineering (ISSRE) },
  editor = {?},
  pages = {215-224},
  publisher = {IEEE Computer Society},
  isbn = {0-7695-3024-9}
}

@Article{Simula.SE.116,
  author = {Wang, Alf Inge and Arisholm, Erik},
  title = {The Effect of Task Order on the Maintainability of Object-Oriented Software},
  year = {2008},
  abstract = {This paper presents results from a quasi-experiment that investigates how the sequence in which maintenance tasks are performed affects the time required to perform them and the functional correctness of the changes made. Specifically, the study compares how time required and correctness are affected by (1) starting with the easiest change task and progressively performing the more difficult tasks (Easy-First), versus (2) starting with the most difficult change task and progressively performing the easier tasks (Hard-First). In both cases, the experimental tasks were performed on two alternative types of design of a Java system to assess whether the choice of the design strategy moderates the effects of task order on effort and correctness.  The results show that the time spent on making the changes is not affected significantly by the task order of the maintenance tasks, regardless of the type of design. However, the correctness of the maintainability tasks is significantly higher when the task order of the change tasks is Easy-First compared to Hard-First, again regardless of design. A possible explanation for the results is that a steeper learning curve (Hard-First) causes the programmer to create software that is less maintainable overall.},
  journal = {Information and Software Technology}
}

@Misc{Simula.ND.19,
  author = {Lysne, Olav and Reinemo, Sven-Arne and Skeie, Tor and Solheim, {\r A}shild Gr{\o}nstad and S{\o}dring, Thomas},
  title = {The realization of virtual compute resources in a Utility Computing Data Center (UCDC) in the many core era},
  year = {2006},
  abstract = {},
  howpublished = {2006 Workshop on On- and Off-Chip Interconnection Networks for Multicore Systems}
}

@Inproceedings{Simula.SE.117,
  author = {Furulund, Kristian Marius and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Using Project Experience to Increase Software Estimation Accuracy - An Empirical Study},
  year = {2007},
  abstract = {The use of experience data is frequently suggested as a mean to increase software effort estimation accuracy. However, there has been done limited empirical research on the subject. To investigate the effect of using experience data, we conducted a study on eighteen of the latest projects of a software contractor. Quantitative project data was collected, and project managers interviewed, on several issues related to estimates, key project properties, and project outcome. It was found that in projects where experience data was utilized in the estimation process, they experienced a lesser magnitude of effort overruns. The use of a checklist also appeared to increase estimation accuracy. However, the utilization of an estimation model in the estimation process appeared not to have any impact on the estimation accuracy.},
  booktitle = {The Seventh International Conference on Quality Software - QSIC 2007},
  editor = {C.V. Ramamoorthy},
  publisher = {IEEE Computer Society Press},
  isbn = {ISBN}
}

@Techreport{Simula.SE.31,
  author = {No names specified},
  title = {A State-based Approach to Integration Testing for Object-Oriented Programs},
  year = {2006},
  abstract = {},
  institution = {Carleton University},
  number = {SCE-05-08}
}

@Article{Simula.SE.313,
  author = {Arisholm, Erik and Briand, Lionel Claude and Johannessen, Eivind B},
  title = {A Systematic and Comprehensive Investigation of Methods to Build and Evaluate Fault Prediction Models},
  year = {2008},
  abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets entailing different data collection costs, such as source code structural measures and historic change/fault (process) measures, and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).  The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas across releases {\textendash} both in terms of ROC area and cost-effectiveness. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. The regular confusion matrix criteria, although popular, are not clearly related to what we consider to be a crucial aspect, namely the cost-effectiveness of using fault-proneness prediction models to focus verification effort where it is the most needed.},
  journal = {Journal of Systems and Software}
}

@Phdthesis{Simula.SC.166,
  author = {Clark, Stuart R},
  title = {The time-dependent evolution of subduction zones : interactions of the lithosphere and mantle / Stuart Raymond Clark},
  year = {2008},
  abstract = {This thesis addresses key issues in the time-evolution of subduction zones by carefully linking models with observations.  Understanding the link between the lithosphere and the mantle has provided complex challenges because of the interdisciplinary nature of the subject. The process of subduction is one of the main ways that the mantle and lithosphere interact as the lithosphere is recycled into the mantle, slab-enriched mantle volcanics are emplaced on the surface and back-arc rifting on the overriding plate occurs. The down-going slab also pulls on the lithosphere as it sinks causing inundation of continents and the sediment loading of basins.   This thesis creates a new methodology for understanding these processes, by combining plate-motion, age and subduction history with mantle convection models. This allows dynamic topography to be quantified both spatially and in time. The research shows that dynamic topography is within the constraints placed by external factors, such as the inundation of continents and anomalous subsidence reported in back-arc basins. The research also has implications for the density anomalies in the lower mantle that have often been used to predict dynamic topography {\textendash} that a simple conversion between seismic velocity and mantle density does not work for the lower mantle.   I have used the seismic tomography in the upper mantle to validate particular plate kinematic reconstructions for the Kamchatka and Tonga regions. This provides a new and important constraint on kinematic reconstructions which have often been more diagrammatic than rigorously defined by plate boundaries, in particular subduction trenches, which are self-consistent with the reconstruction of surface features and spreading ridges.   Subduction models have also been used to explain the episodicity of compression, quiescence and extension on the overriding plate found in many of the world{\textquoteright}s back-arc basins. This episodicity is reproduced in three-dimensional subduction models by my innovative use of an overriding plate. The net force on the place, expressed as a surface velocity, is found to be the fundamental requirement in reproducing episodicity on the subducting plate. Periods of episodicity have been predicted according to the dynamic evolution of the subduction system.},
  school = {School of Geosciences},
  address = {University of Sydney},
  type = {PhD (Geophysics)},
  isbn = {0}
}

@Misc{Simula.SC.130,
  author = {L{\o}vgren, Alf Emil and R{\o}nquist, Einar and Maday, Yvon},
  title = {Reduced basis modeling of complex flow systems},
  year = {2007},
  abstract = {},
  howpublished = {Invited talk at the Modelling and Scientific Computing group at EPFL, Switzerland, November, 2007. Also presented at the workshop on Modelling and Computation of Biomedical Processes at CBC, Simula, June 7-14}
}

@Article{Simula.SC.146,
  author = {Hanslien, Monica and Sundnes, Joakim and Holden, Nina},
  title = {A note on discontinues rate functions for the gate variables in   mathematical models of cardiac cells},
  year = {2008},
  abstract = {The gating mechanism of ionic channels in cardiac cells is often modeled by ordinary differential equations (ODEs) with voltage dependent rates of change. Some of these rate functions contain discontinuities or singularities, which are not physiologically founded but rather introduced to fit experimental data. Such non-smooth right hand sides of ODEs are associated with potential problems when the equations are solved numerically, in the form of reduced order of accuracy and inconsistent convergence. In this paper we propose to replace the discontinuous rates with smooth versions, by fitting functions of the form introduced by Noble (1962) to the original data found by Ebihara and Johnson (1980). We find that eliminating the discontinuities in the rate functions enables the numerical method to  obtain the expected order of accuracy, and has a negligible effect on the kinetics of the membrane model.},
  journal = {journal, 2008}
}

@Misc{Simula.SC.170,
  author = {Hake, Johan Elon and Lines, Glenn Terje},
  title = {Modelling the mesoscopic length scale of EC coupling: the diadic cleft},
  year = {2008},
  abstract = {Ca2+ signalling in the dyadic cleft plays an important role in the excitation contraction (EC) coupling in ventricular myocytes. The cleft is a fuzzy space between the T-tubule and the Sarcoplasmic reticulum (SR). An action potential triggers the opening of L-type Ca2+ channels that let Ca2+ ions into the cleft. Most of the ions diffuses into the cytosole; but some bind to Ryanodine receptors (RyR), which are situated in clusters at the SR membrane, triggering further Ca 2+ release from SR. The signal is conveyed by a small number of diffusing Ca2+ ions, and is therefor fundamentally stochastic. The small number of Ca2+ ions involved in the process would suggest that diffusion in the dyadic cleft have to be modelled using Random Walk methods, making a deterministic and continuous model of diffusion useless. However the signal is not conveyed by Ca2+ ions in the cleft but rather by Ca2+ ions binding to single RyRs. To study the mesoscopic scale for this process we compared the event of single Ca2+ ions binding to RyRs in the cleft using two different models of diffusion: a stochastic and discrete Random Walk (RW) model, and a deterministic continuous model. We investigate whether the latter model, together with a stochastic receptor model, can reproduce binding events registered in fully stochastic RW simulations. By evaluating the continuous model goodness-of-fit, for a large range of parameters, we present evidence that it can. The large fluctuations in binding rate observed at the level of single time steps are integrated and smoothed at the larger time scale of single binding events, explaining the continuous model goodness-of-fit. Three parameters, which are combined into a dimensionless parameter: the diffusion constant of Ca2+, the binding rate of Ca2+ to the RyRs, and the diffusion length of the RW model, defines an upper limit for the agreement between the two models. As a result these parameters set the mesoscopic scale for the EC coupling.},
  howpublished = {Invited talk at 'European Conference on Mathematical and Theoretical Biology'}
}

@Techreport{Simula.SE.317,
  author = {Benestad, Hans Christian and Anda, Bente and Arisholm, Erik},
  title = {An investigation of change effort in two evolving software systems},
  year = {2008},
  abstract = {},
  institution = {Simula Research Laboratory}
}

@Inproceedings{Simula.ND.201,
  author = {Apeland, Ole K and Cicic, Tarik},
  title = {IP Fast Reroute using Relaxed MRC},
  year = {2008},
  abstract = {We demonstrate IP fast reroute using a recently proposed method called Relaxed Multiple Routing Configurations. While IP fast reroute has been researched intensively in recent years, no practical implementations are known to exist prior to the system we demonstrate. The goal of the demonstrator is to show the effect of network failures on real-time applications with and without IP fast reroute mechanisms deployed. For best illustration we use video transmission showing that IP fast reroute can elliminate frame freezing upon cable cut.},
  booktitle = {SIGCOMM '08: Proceedings of the ACM SIGCOMM 2008 conference on Data communication},
  editor = {ACM},
  chapter = {Demo Session},
  pages = {531},
  publisher = {ACM},
  isbn = {978-1-60558-177-4}
}

@Mastersthesis{Simula.SE.287,
  author = {S{\o}rlie, Linda},
  title = {Automatisk og manuell sk{\r a}ring av Javaoppgaver i programmeringstester: En prototypeimplementasjon av persistens for bruk i ferdighetstester},
  year = {2007},
  abstract = {},
  school = {University of Oslo},
  type = {Master's Theis}
}

@Article{Simula.SE.305,
  author = {Hannay, Jo and Arisholm, Erik and Engvik, Harald and Sj{\o}berg, Dag},
  title = {Effects of Personality on Pair Programming},
  year = {2008},
  abstract = {Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting programming and team performance in software engineering. However, research suggests that other human-related factors, such as motivation, general mental ability, expertise and task complexity also affect performance in general. This paper reports on a study of the impact of Big-Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals from three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part, and the results show that (1) our data does not confirm a meta-analysis-based model of the impact of certain personality traits on performance, and (2) personality traits in general have modest predictive value on pair programming performance compared with expertise, task complexity, and even country. Rather than focusing on direct effects of personality on pair programming performance, we conclude that (a) effort should be spent on elaborating on personality{\textquoteright}s (and other factors{\textquoteright}) indirect effects on performancemediated by formalized scales of collaboration, and (b) more effort should be spent on investigating other performance-related predictors such as programming skill, learning, motivation, expertise, and task complexity.},
  journal = {Transaction on Software Engineering}
}

@Article{Simula.SC.136,
  author = {Ruud, Tomas Syrstad and Nielsen, Bj{\o}rn Fredrik and Gr{\o}ttum, Per and Lysaker, Ola Marius},
  title = {Body Surface ST shifts Generated by Ischemic Heart Disease; a Simulation Study},
  year = {2008},
  abstract = {This paper considers the issue of where to place electrodes for  electrocardiography from a simulationist viewpoint. We use a stationary version of the bidomain equations to calculate the ST-shift on the body surface from 26 different cases of ischemia. The resulting data is analyzed in terms of where measurements should be recorded for satisfactory coverage.},
  journal = {journal for publication}
}

@Inproceedings{Simula.ND.197,
  author = {Xiang, Jie and Zhang, Yan and Skeie, Tor},
  title = {Joint Admission and Power Control for Cognitive Radio Cellular Networks},
  year = {2008},
  abstract = {In cognitive radio cellular networks (CogCell), the Secondary Users (SUs) can be admitted to the Base Station (BS) provided that the interference caused by SUs to the Primary Users (PUs) is no higher than the pre-defined threshold. In addition, different SUs may require different Quality of Service (QoS) and hence make different payment based on the provided QoS level. In this paper, we investigate the maximally achievable revenue obtained from SUs subjected to the interference constraints on PUs and QoS requirements of SUs. To solve the identified issue, we introduce the revenue efficiency factor and propose an efficient Joint Admission and Power Control scheme using a Minimal Revenue Efficiency Removal algorithm (JAPC-MRER). With comparison to the other two schemes JAPC-MSRA and JAPC-Random, our strategy is able to achieve much higher revenue with guaranteed interference requirements and QoS demands. In addition, the proposed scheme is evaluated with the effects of the key parameters, i.e. the number of PUs, the number of SUs and the interference threshold.},
  booktitle = {the 11th IEEE International Conference on Communication Systems 2008 (ICCS 2008), invited paper}
}

@Mastersthesis{Simula.SE.229,
  author = {Brunsvig, Espen},
  title = {How much empirical evidence is there to support claims made by software engineering tool providers about the benefits of their tools, and how valid is this evidence?},
  year = {2008},
  abstract = {},
  school = {University of Oslo},
  type = {Master Thesis}
}

@Inproceedings{Simula.ND.159,
  author = {Petlund, Andreas and Evensen, Kristian and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {TCP mechanisms for improving the user experience  for time-dependent thin-stream applications},
  year = {2008},
  abstract = {A wide range of Internet-based services that use reliable transport protocols display what we call thin-stream properties. This means that the application sends data with such a low rate that the retransmission mechanisms of the transport protocol are not fully effective. In time-dependent scenarios where the user experience depends on the data delivery latency, packet loss can be devastating for the service quality. In order to reduce application-layer latency when packets are lost, we have implemented modifications to the TCP retransmission mechanisms in the Linux kernel. The changes are only active when thin-stream properties are detected, thus not affecting TCP behaviour when the stream is not thin. In this paper, we show the latency improvements from these thin-stream modifications. We have tested several thin-stream applications like remote terminals (SSH) and audio conferencing (Skype), and we evaluate the user experience with and without the TCP modifications. Our experimental results show that our modifications allow TCP to recover earlier from packet loss. Furthermore, user surveys indicate that the majority of users easily detect improvements in the perceived quality of the tested applications.},
  booktitle = {The 33rd Annual IEEE Conference on Local Computer Networks (LCN)}
}

@Inproceedings{Simula.ND.189,
  author = {Gj{\o}rven, Eli and Eliassen, Frank and Rouvoy, Romain},
  title = {Experiences from Developing a Component Technology Agnostic Adaptation Framework},
  year = {2008},
  abstract = {Systems are increasingly expected to adapt themselves to changing requirements and environmental situations with minimum user interactions. A challenge for self-adaptation is the increasing heterogeneity of applications and services, integrating multiple systems implemented in different platform and language technologies. In order to cope with this heterogeneity, self-adaptive systems need to support the integration of various technologies, allowing the target adaptive system to be built from subsystems realized with different implementation technologies. In this paper, we argue that state-of-the adaptation frameworks do not lend themselves to ease technology integration and exploitation of advanced features and opportunities offered by different implementation technologies. We present the QuA adaptation framework and its support for technology integration and exploitation. Unlike other adaptation frameworks the adaptation framework of QuA is able to exploit a wide range of adaptation mechanisms and technologies, without modification to the adaptation framework itself. As a demonstration of this property of QuA, we describe the integration of an advanced component model technology, the Fractal component model, with the QuA framework. Our experience from this exercise shows that the QuA adaptation framework indeed allows integration of advanced implementation technologies with moderate effort.},
  booktitle = {11th International Conference on Component Based Software Engineering (CBSE{\textquoteright}08)}
}

@Article{Simula.SC.134,
  author = {Linge, Svein and Sundnes, Joakim and Hanslien, Monica and Lines, Glenn Terje and Tveito, Aslak},
  title = {Numerical solution of the bidomain equations - a review},
  year = {2008},
  abstract = {Knowledge of cardiac electrophysiology is eﬃciently formulated in terms of mathematical models. Most of these models are, however, very complex and thus denies direct mathematical reasoning founded on classical and analytical considerations. This is particularly the case for the celebrated bidomain model, developed almost 40 years ago for concurrent analysis of extra- and intracellular electrical activity. Numerical simulations represent an indispensible tool to study electrophysiology based on this model. However,  both steep gradients in the solutions and complicated geometries lead to extremely challenging computational problems.   The greatest achievement in scientiﬁc computing over the past 50 year was to enable solution of linear systems of algebraic equations arising from discretizations of partial diﬀerential equations in an optimal manner; i.e. such that the CPU-eﬀorts increases linearly in the number of computational nodes. Over the past decade such optimal methods have been introduced in simulation of electrophysiology. This development, together with the development of aﬀordable parallel computers, has enabled the solution of the bidomain model combined with accurate cellular models deﬁned on realistic geometries. However, in spite of recent progress, the full potential of modern computational methods has yet to be exploited for solution of the bidomain model.   It is the purpose of this paper to review the development of numerical methods for solving the bidomain model. The ﬁeld is huge, and we have to restrict our focus to the development after year 2000.},
  journal = {journal}
}

@Misc{Simula.SC.118,
  author = {Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik and Gr{\o}ttum, Per and Abildgaard, Andreas and Fjeld, Jan and Haugaa, K},
  title = {Computer simulations for identifying ischemic heart disease; a validation study},
  year = {2008},
  abstract = {},
  howpublished = {Presented at the BBG-MedViz seminar at the Department of Mathematics, University of Bergen }
}

@Inproceedings{Simula.SE.285,
  author = {Zhang, He and Kitchenham, Barbara and Pfahl, Dietmar},
  title = {Software Process Simulation over the Past Decade: Trends Discovery from A Systematic Review},
  year = {2008},
  abstract = {Software Process Simulation (SPS) research has increased since 1998 when the frst ProSim Workshop was held. Thispaper aims to reveal how SPS has evolved during the past 10 years based on the preliminary results from the systematic literature review of SPS publications from 1998 to 2007. Trends over the period showed that interest in continuous modelling was decreasing and interest in micro-processes was increasing. Hybrid models were based primarily on system dynamics and discrete event simulation and were all implemented by vertical integration.},
  booktitle = {ACM-IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM 2008)},
  editor = {S. Elbaum, and J. Muench},
  pages = {345-348},
  publisher = {ACM},
  organization = {ACM SIGSOFT \& IEEE CS},
  isbn = {978-1-59593-971-5}
}

@Inproceedings{Simula.ND.205,
  author = {Apeland, Ole K and Cicic, Tarik},
  title = {Architecture and Performance of a Practical IP Fast  Reroute Implementation},
  year = {2008},
  abstract = {IP Fast Reroute (IP FRR) denominates a set of methods and technologies for proactive, local recovery in IP networks. It has received much attention recently, in the research community and the IETF. Several proposals for IP FRR have been published by independent research groups, and main properties and trade-offs are understood. However, only simulation-based and analytical evaluations are known to be performed so far. In this paper we present the architecture and evaluate performance of the first known practical implementation of IP FRR. The arcitecture features a modular approach consisting of an IP FRR framework and exchangeable backup path calculation modules. The performance evaluation is based on an implementation of Multiple Routing Configurations. The implementation provide throughput that closely matches the achieved performance during normal operation.},
  booktitle = {IEEE Global Communications Conference (GLOBECOM 2008)}
}

@Inproceedings{Simula.ND.209,
  author = {Cicic, Tarik and Apeland, Ole K},
  title = {Muliple routing configurations demonstrator: IP fast reroute in practice},
  year = {2007},
  abstract = {We demonstrate IP fast reroute using a recently proposed method called Multiple Routing Conﬁgurations. The demonstrator comprises a small network of Linux routers, where the effect of network failures can be observed on real-time applications with and without IP fast reroute in effect.},
  booktitle = {Middleware '07: Proceedings of the 8th ACM/IFIP/USENIX international conference on Middleware},
  editor = {ACM},
  chapter = {Demo Session},
  pages = {1--2},
  publisher = {ACM},
  isbn = {978-1-59593-935-7}
}

@Misc{Simula.SC.101,
  author = {Haga, Joachim Berdal and Bruaset, Are Magnus and Cai, Xing and Langtangen, Hans Petter and Osnes, Harald and Skogseid, Jakob},
  title = {Parallelisation and Numerical Performance of a 3D Model for Coupled Deformation, Fluid Flow, and Heat Transport in Porous Geological Formations},
  year = {2007},
  abstract = {In this paper, we present some parallel performance results for a 3D simulator of coupled deformation, fluid flow and heat transfer in sedimentary basins. The model parameters are derived from an industry simulator, with realistic material properties and complex irregular grids of up to 1.5 million nodes with 7.3 million degrees of freedom. We have performed parallelisation on the linear algebra level using the ML algebraic multigrid preconditioner with iterative methods in the Diffpack finite element framework. Implementation and speedup results are presented.},
  howpublished = {Talk at the Fourth National Conference on Computational Mechanics (MekIT'07), Trondheim, Norway},
  note = {Presented by J. B. Haga}
}

@Phdthesis{Simula.ND.58,
  author = {Musunoori, Sharath Babu},
  title = {Quality Aware Application Service Placement in a Stochastic Grid Environment},
  year = {2007},
  abstract = {},
  school = {Faculty of Mathematics and Natural Sciences, University of Oslo},
  address = {Oslo, Norway}
}

@Misc{Simula.SC.98,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Gr{\o}ttum, Per and Tveito, Aslak and Abildgaard, Andreas and Fjeld, Jan Gunnar and Haugaa, Kristina Hermann},
  title = {On the use of computer simulations for identifying ischemic heart disease; theoretical and practical aspects},
  year = {2008},
  abstract = {},
  howpublished = {Presented at the workshop "Mathematics in Medicine/Biology", Centre of Mathematics for Applications, Department of Mathematics, University of Oslo}
}

@Article{Simula.SE.205,
  author = {Raffo, David and Pfahl, Dietmar and Wang, Qing},
  title = {Guest Editors' Introduction - ICSP 2007 Special Issue},
  year = {2008},
  abstract = {},
  journal = {Software Process: Improvement and Practice},
  volume = {13},
  number = {1},
  pages = {1-3}
}

@Phdthesis{Simula.SE.178,
  author = {Kampenes, Vigdis By},
  title = {Quality of design, analysis and reporting of software engineering experiments: A systematic review},
  year = {2007},
  abstract = {Background: Like any research discipline, software engineering research must be of a certain quality to be valuable. High quality research in software engineering ensures that knowledge is accumulated and helpful advice is given to the industry. One way of assessing research quality is to conduct systematic reviews of the published research literature. 

Objective: The purpose of this work was to assess the quality of published experiments in software engineering with respect to the validity of inference and the quality of reporting. More specifically, the aim was to investigate the level of statistical power, the analysis of effect size, the handling of selection bias in quasi-experiments, and the completeness and consistency of the reporting of information regarding subjects, experimental settings, design, analysis, and validity. Furthermore, the work aimed at providing suggestions for improvements, using the potential deficiencies detected as a basis. 

Method: The quality was assessed by conducting a systematic review of the 113 experiments published in nine major software engineering journals and three conference proceedings in the decade 1993-2002. 

Results: The review revealed that software engineering experiments were generally designed with unacceptably low power and that inadequate attention was paid to issues of statistical power. Effect sizes were sparsely reported and not interpreted with respect to their practical importance for the particular context. There seemed to be little awareness of the importance of controlling for selection bias in quasi-experiments. Moreover, the review revealed a need for more complete and standardized reporting of information, which is crucial for understanding software engineering experiments and judging their results.

Implications: The consequence of low power is that the actual effects of software engineering technologies will not be detected to an acceptable extent. The lack of reporting of effect sizes and the improper interpretation of effect sizes result in ignorance of the practical importance, and thereby the relevance to industry, of experimental results. The lack of control for selection bias in quasi-experiments may make these experiments less credible than randomized experiments. This is an unsatisfactory situation, because quasi-experiments serve an important role in investigating cause-effect relationships in software engineering, for example, in industrial settings. Finally, the incomplete and unstandardized reporting makes it difficult for the reader to understand an experiment and judge its results.

Conclusions: Insufficient quality was revealed in the reviewed experiments. This has implications for inferences drawn from the experiments and might in turn lead to the accumulation of erroneous information and the offering of misleading advice to the industry. Ways to improve this situation are suggested.  

},
  school = {University of Oslo}
}

@Misc{Simula.SE.182,
  author = {J{\o}rgensen, Magne},
  title = {Hvordan kundens anbudsprosess f{\r a}r deg til {\r a} estimere overoptimistisk og hva du kan gj{\o}re med det},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at JavaZone 2007}
}

@Misc{Simula.SE.183,
  author = {J{\o}rgensen, Magne},
  title = {Estimering av IT-prosjekter: Ekspertestimat eller modell?},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at Simula's annual estimation seminar}
}

@Misc{Simula.SE.184,
  author = {J{\o}rgensen, Magne},
  title = {Estimering av IT-prosjekter},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at Lindorff-seminar}
}

@Misc{Simula.SE.186,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Software Development Effort Estimation},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at Making Waves-seminar}
}

@Misc{Simula.SE.187,
  author = {J{\o}rgensen, Magne},
  title = {Det kommer an p{\r a} hvordan man sp{\o}r},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.188,
  author = {J{\o}rgensen, Magne},
  title = {Overtro p{\r a} f{\o}rsteinntrykket},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.189,
  author = {J{\o}rgensen, Magne},
  title = {Hva slags informasjon stoler vi mest p{\r a}?},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.191,
  author = {J{\o}rgensen, Magne},
  title = {Hypnose, rasjonalisering og litt m{\r a}leteori},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.192,
  author = {J{\o}rgensen, Magne},
  title = {Kunder, leverand{\o}rer, yrkesetikk og informasjonsasymmetri},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Article{Simula.SC.42,
  author = {Rahman, S and Langtangen, Hans Petter and Barnes, C. H. W},
  title = {A finite element method for modelling electromechanical wave propagation in anisotropic piezoelectric media},
  year = {2007},
  abstract = {},
  journal = {Communications in Computational Physics},
  volume = {2},
  number = {2},
  pages = {271-292}
}

@Article{Simula.SC.46,
  author = {Kirby, Robert C and Logg, Anders},
  title = {Benchmarking Domain-specific Compiler Optimizations for Variational Forms},
  year = {2007},
  abstract = {},
  journal = {ACM Transactions on Mathematical Software}
}

@Article{Simula.SC.55,
  author = {Oelgaard, Kristian and Logg, Anders and Wells, Garth N},
  title = {Automated Code Generation for Discontinuous Galerkin Methods},
  year = {2007},
  abstract = {},
  journal = {SIAM J. Sci. Comput.}
}

@Inproceedings{Simula.SC.56,
  author = {Logg, Anders},
  title = {Automated Solution of Differential Equations},
  year = {2007},
  abstract = {},
  booktitle = {ICIAM 2007: 6th International Congress on Industrial and Applied Mathematics}
}

@Misc{Simula.SC.71,
  author = {Cai, Xing},
  title = {Making parallel PDE software by object-oriented programming},
  year = {2007},
  abstract = {},
  howpublished = {Guest lecture given at Hohai University, China, May 17}
}

@Article{Simula.ND.83,
  author = {Evensen, Kristian R and Petlund, Andreas and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Redundant Bundling in TCP to Reduce Perceived  Latency for Time-Dependent Thin Streams},
  year = {2008},
  abstract = {TCP and UDP are the dominant transport protocols today, with TCP
being preferred because of the lack of fairness mechanisms in
UDP. Some time-dependent applications with small bandwidth 
requirements, however, occationally suffer from unnecessarily 
high latency due to TCP retransmission mechanisms that are 
optimized for high-throughput streams. Examples of such 
thin-stream  applications are Internet telephony and multiplayer 
games. For such interactive applications, the high delays can be 
devastating to the experience of the service.
To address the latency issues, we explored application-transparent, 
sender-side modifications. We investigated whether it is possible 
to bundle unacknowledged data to preempt the experience of packet 
loss and improve the perceived latency in time-dependent systems. 
We implemented and tested this idea in Linux. Our results show
that we can reduce the application latency by trading it against
bandwidth.},
  journal = {IEEE Communications Letters},
  volume = {12},
  number = {4},
  pages = {334 -- 336}
}

@Inproceedings{Simula.ND.86,
  author = {Hansen, Audun Fosselie and Egeland, Geir and Engelstad, Paal},
  title = {Could Proactive Link-State Routed Wireless Networks Benefit from Local Fast Reroute?},
  year = {2008},
  abstract = {The communication performance in wireless net- works is often heavily influenced by failures caused by node mobility and radio disturbance. Proactive link- state routing protocols like OLSR and OSPFv3 with wireless extensions (WOSPF) are currently relying on slow re-convergence to restore successful packet for- warding. This may not be appropriate for particular applications, and it may cause instability during tran- sient failures. Support for fast and local reroute should therefore be added as a feature to proactive link-state based routing protocols. In this paper, we discuss the challenges and requirements that are associated with such solutions. In addition, we evaluate and discuss to what extent recent developments for fixed IP routing can serve as solutions. To our best knowledge, no pub- lished work presents a similar study on fast recovery in proactive link-state routed wireless networks.},
  booktitle = {6th annual Communication Networks and Services Research Conference, 2008. CNSR 2008},
  editor = {Patrick Kellenberger - IEEE},
  pages = {453-462},
  publisher = {IEEE},
  isbn = {978-0-7695-3135-9}
}

@Misc{Simula.SC.85,
  author = {Al-Khayat, Omar and Bruaset, Are Magnus and Langtangen, Hans Petter},
  title = {The Lattice Boltzmann method applied on Turbidity Flow Modeling},
  year = {2007},
  abstract = {},
  howpublished = {Talk at MekIT'07}
}

@Misc{Simula.SC.89,
  author = {Cai, Xing},
  title = {Simulation of tsunami propagation},
  year = {2008},
  abstract = {},
  howpublished = {Talk at the 2nd eScience Meeting, Jan. 21-22, Geilo, Norway}
}

@Article{Simula.SC.90,
  author = {Clark, Stuart Raymond and M{\"u}ller, R Dietmar},
  title = {Convection Models in the Kamchatka Region using Imposed Plate Motion and Thermal Histories},
  year = {2008},
  abstract = {Mantle convection modelling with realistic plate histories has been limited by the availability of published gridded kinematic models. While alternative kinematic models may exist for a given region, they are not easily converted into gridded velocity models, because of the difficulty in compiling closed time-dependent plate polygons consistent with a particular plate rotation history. This problem arises because even though the location of mid-ocean ridges through time is usually well constrained, the location of subduction trenches is not always well known for the geological past.   In addition, while a rigid plate may be defined as a continuous body, any two points of which have no motion relative to each other, the boundaries of plates continuously evolve. If regional plate boundaries can easily be calculated and defined over a series of time steps in a self-consistent manner, then compiling time-dependent velocity and oceanic palaeo age-grids is greatly simplified from a given plate kinematic model as input for mantle convection modelling. We present a solution to this problem for regional plate models, implemented through user-friendly open source software.  We use this solution to prepare the plate motion and thermal histories for the Kamchatka region. The implications of the plate model, combined with the mantle rheology and thermal boundary conditions, is realised by slabs in the mantle. The position of these slabs can then be compared with seismic tomography images. Our convection model broadly agrees with the tomography, with slabs located under the Aleutian Basin and the Kamchatka peninsula.},
  journal = {Journal of Geodynamics},
  volume = {46},
  number = {1-2},
  pages = {1-9}
}

@Misc{Simula.SC.94,
  author = {Mardal, Kent-Andre and Aln{\ae}s, Martin Sandve},
  title = {Finite Elements with Symbolic Computations and Code Generation},
  year = {2007},
  abstract = {},
  howpublished = {Talk at Software Issues in Computational Science and Engineering (SCSE 2007), Uppsala, 2007-08-11}
}

@Inbook{Simula.SC.26,
  author = {L{\o}vgren, Alf Emil and R{\o}nquist, Einar M and Maday, Yvon},
  title = {The reduced basis element method for fluid flows},
  year = {2007},
  abstract = {The reduced basis element approximation is a discretization method for solving partial differential equations that has inherited features from the domain decomposition method and the reduced basis approximation paradigm in a similar way as the spectral element method has inherited features from domain decomposition methods and spectral approximations. We present here  a review of the method directed to the application of fluid flow simulations in hierarchical geometries. We present the rational and the basics of the method together with details on the implementation. We illustrate also the rapid convergence with numerical results.},
  booktitle = {Analysis and Simulation of Fluid Dynamics},
  editor = {Calgaro, C., Coulombel, J.-F., Goudon, T.},
  publisher = {Birkh{\"a}user},
  address = {Basel},
  series = {Advances in Mathematical Fluid Mechanics},
  pages = {129--154},
  isbn = {978-3-7643-7741-0}
}

@Article{Simula.SC.29,
  author = {Deuflhard, Peter and Erdmann, Bodo and Roitzsch, Rainer and Lines, Glenn Terje},
  title = {Adaptive Finite Element Simulation of Ventricular Fibrillation Dynamics},
  year = {2007},
  abstract = {},
  journal = {Computing and Visualization in Science}
}

@Inproceedings{Simula.ND.38,
  author = {Zhang, Y and Chen, Y},
  title = {Queueing Analysis of OFDM Subcarrier Allocation in Wireless Multimedia Networks},
  year = {2007},
  abstract = {},
  booktitle = {IEEE International Symposium on Wireless Communication Systems 2007 (IEEE ISWCS{\textquoteright}07)},
  editor = {IEEE},
  pages = {81-85},
  publisher = {IEEE},
  isbn = { 978-1-4244-0979-2}
}

@Inproceedings{Simula.ND.61,
  author = {Sem-Jacobsen, Frank Olaf and Lysne, Olav},
  title = {Fault Tolerance with Shortest Paths in Regular and Irregular Networks},
  year = {2008},
  abstract = {Fault tolerance has become an important part of current supercomputers.  Local dynamic fault tolerance is the most expedient way of tolerating faults by preconfiguring the network with multiple paths from every node/switch to every destination.  In this paper we present a local shortest path dynamic fault-tolerance mechanism inspired by a solution developed for the Internet that can be applied to any shortest path routing algorithm such as Dimension Ordered Routing, Fat Tree Routing, Layered Shortest Path, etc., and provide a solution for achieving deadlock freedom in the presence of faults.   Simulation results show that 1) for fat trees this yields the to this day highest throughput and lowest requirements on virtual layers for dynamic one-fault tolerance, 2) we require in general few layers to achieve deadlock freedom, and 3) for irregular topologies it gives at most a 10 times performance increase compared to FRoots.},
  booktitle = {22nd IEEE International Parallel \& Distributed Processing Symposium}
}

@Article{Simula.ND.64,
  author = {Sem-Jacobsen, Frank Olaf and Skeie, Tor and Lysne, Olav and Duato, Jose},
  title = {Dynamic Fault Tolerance with Adaptive Routing in Fat Trees},
  year = {2007},
  abstract = {With the curren tlarg-scale interconenction networks necessary to support tens of thousands of processors, it is no longer acceptable to require that all of the
components of the interconnection network are fault-free at all
times. Thus, the ability to handle multiple faulty and unstable links
without interrupting the operation of the computer is more
important than ever.  This is best acheived by having the local
network nodes around the fault to transparently reroute the
traffic affected by the fault, hiding both permanent and short-lived faults.

In this paper, we present such a local dynamic
fault-tolerant routing algorithm for fat trees, and show that
it remains connected and deadlock free for up to and including
radix/2-1 arbitrary simultaneous faults.  The method requires only a simple misrouting
functionality in the downward phase and a small modification
of the selection function in the upward phase. No control messages
are required.

To the best of our knowledge, this is the first reported
method for fat trees that provides local dynamic fault
tolerance of several simultaneous network faults,  without adding
additional hardware or additional
virtual channels. Our simulation results display a very graceful
degradation as faults appear.},
  journal = {IEEE Transactions on Computers}
}

@Inproceedings{Simula.ND.65,
  author = {Oudenstad, Johannes and Eliassen, Frank and Gj{\o}rven, Eli and Rouvoy, Romain},
  title = {Peer-to-peer brokering of planning meta-data},
  year = {2007},
  abstract = {In self-adaptive systems, metadata about resources in the system
(e.g., services, nodes) must be dynamically published, updated,
and removed. Current middleware approaches use statically configured,
centralised repositories for storing and retrieving of such
metadata. However, in the context of peer-to-peer (P2P) environments,
we can not assume the existence of server nodes that are
always available for hosting such centralised services.
Thus, in our planning-based adaptation middleware, we introduce
a P2P broker, which is a metadata advertisement service based
on P2P technology. We use a structured P2P protocol that distributes
the service metadata over a set of nodes based on service
type and property information. Initial experiments show that the
metadata distributes well over the nodes in the network, thus enabling
scalability and robustness to node failures.},
  booktitle = {Adaptable and Reflective Middleware (ARM) 2007},
  editor = {F.M. Costa},
  publisher = {ACM Press},
  isbn = {978-1-59593-931-9/07/11}
}

@Inproceedings{Simula.ND.72,
  author = {Cicic, Tarik and Hansen, Audun Fosselie and Kvalbein, Amund and Hartmann, Matthias and Martin, Rudiger and Menth, Michael},
  title = {Relaxed Multiple Routing Configurations for IP Fast Reroute},
  year = {2008},
  abstract = {Multi-topology routing is an increasingly popular IP network management concept that allows transport of different traffic types over disjoint network paths. The concept is of particular interest for implementation of IP fast reroute (IP FRR). First, it can support guaranteed, instantaneous recovery from any link or node failure. Second, different failures result in routing over different network topologies, which augments the parameter space for load distribution optimizations. Multiple Routing Configurations (MRC) is the state-of-the-art IP FRR scheme based on multi-topology routing today. In this paper we present a new, enhanced IP FRR scheme which we call {\textquotedblleft}relaxed MRC{\textquotedblright} (rMRC). rMRC simplifies the topology construction and increases the routing flexibility in each topology. According to our experimental evaluation, rMRC has several benefits compared to MRC. The number of backup topologies required to provide protection against the same set of failures is reduced, hence reducing state in routers. In addition, the backup paths are shorter, and the link utilization is significantly better.},
  booktitle = {IEEE/IFIP Network Operations and Management Symposium (NOMS'08)},
  editor = {IEEE},
  publisher = {IEE},
  isbn = {978-1-4244-2066-7}
}

@Misc{Simula.SE.185,
  author = {J{\o}rgensen, Magne},
  title = {Software Cost Estimation:When to Use Estimation Models and When to Use Expert Judgment},
  year = {2007},
  abstract = {},
  howpublished = {Keynote at 1st Conference on Software Productivity Analysis and Cost Estimation (SPACE'07)}
}

@Techreport{Simula.SC.49,
  author = {Logg, Anders},
  title = {Efficient Representation of Computational Meshes},
  year = {2007},
  abstract = {},
  institution = {Finite Element Center},
  type = {Preprint},
  number = {2007-03}
}

@Inproceedings{Simula.SC.54,
  author = {Logg, Anders},
  title = {Efficient Representation of Computational Meshes},
  year = {2007},
  abstract = {},
  booktitle = {MekIT'07},
  editor = {B. Skallerud and H. I. Andersson},
  publisher = {Tapir Akademisk Forlag},
  isbn = {9788259122357}
}

@Book{Simula.SC.63,
  author = {Langtangen, Hans Petter},
  title = {Python Scripting for Computational Science},
  year = {2008},
  abstract = {},
  publisher = {Springer},
  address = {Heidelberg},
  edition = {third},
  isbn = {978-3-540-73915-9}
}

@Misc{Simula.SC.64,
  author = {Langtangen, Hans Petter},
  title = {Computational Modeling of Huge Tsunamis from Asteroid Impacts},
  year = {2007},
  abstract = {},
  howpublished = {Invited keynote lecture at the International conference on Computational Science 2007 (ICCS'07), Beijing, China}
}

@Misc{Simula.SC.65,
  author = {Enger, H and Feder, J and Malthe-S{\o}renssen, A and Langtangen, Hans Petter},
  title = {Optimal Coupling in a Multiscale Model},
  year = {2007},
  abstract = {},
  howpublished = {Poster at the Kongsberg Seminar on geology, Kongsberg, Norway}
}

@Misc{Simula.SC.66,
  author = {Holm{\r a}s, H and Clamond, D and Langtangen, Hans Petter},
  title = {A Pseudospectral Fourier Method Applied to an Incompressible Two-Fluid Model},
  year = {2007},
  abstract = {},
  howpublished = {Talk at the International Conference on Multiphase Flow, ICMF 2007, Leipzig, Germany}
}

@Article{Simula.SE.195,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag and Mockus, Audris},
  title = {Variability and Reproducibility in Software Engineering: A Study of four Companies that Developed the same System},
  year = {2008},
  abstract = {The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. However, reproducibility in software engineering (SE) has not been thoroughly investigated, although generalizing the results of SE studies depend on SE phenomena being reproducible. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirements specification. In a call for tender to 81 consultancy companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, {\textquotedblleft}low{\textquotedblright}, {\textquotedblleft}low{\textquotedblright}, and {\textquotedblleft}medium{\textquotedblright} reproducibility. The contractor{\textquoteright}s internal costs, actual lead time, and schedule overrun of the projects had, respectively, {\textquotedblleft}medium{\textquotedblright}, {\textquotedblleft}high{\textquotedblright}, and {\textquotedblleft}low{\textquotedblright} reproducibility. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, {\textquotedblleft}low{\textquotedblright}, {\textquotedblleft}high{\textquotedblright}, and {\textquotedblleft}low{\textquotedblright} reproducibility. We proposed a coarse-grained model that uses software process inputs to predict key project outcomes. The comparisons of the model{\textquoteright}s predictions with the actual outcomes indicate some reproducibility. This initial work may contribute to developing more accurate models, but making SE more reproducible remains a great challenge for SE research, education, and industry.},
  journal = {IEEE Transactions on Software Engineering}
}

@Article{Simula.SC.73,
  author = {Ruud, Tomas Syrstad and Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Sundnes, Joakim},
  title = {A Computationally Efficient Method for Determining the Size and Location of Myocardial Ischemia},
  year = {2008},
  abstract = {},
  journal = {IEEE Transactions on Biomedical Engineering}
}

@Article{Simula.SE.196,
  author = {Benestad, Hans Christian and Anda, Bente Cecilie Dahlum and Arisholm, Erik},
  title = {Understanding software maintenance and evolution by analyzing individual changes: A literature review},
  year = {2008},
  abstract = {},
  journal = {Journal of Software Maintenance and Evolution: Research and Practice}
}

@Inproceedings{Simula.ND.88,
  author = {Robles-G{\a\'o}mez, Antonio and Berm{\a\'u}dez, Aurelio and Casado, Rafael and Solheim, {\r A}shild Gr{\o}nstad},
  title = {Deadlock-Free Dynamic Network Reconfiguration Based on Close Up*/Down* Graphs},
  year = {2008},
  abstract = {},
  booktitle = {Euro-Par 2008},
  editor = {E. Luque, T. Margalef, and D. Ben{\a\'\i}tez},
  pages = {940{\textendash}949},
  publisher = {Springer Berlin / Heidelberg},
  series = {LNCS 5168},
  isbn = {978-3-540-85450-0}
}

@Article{Simula.SC.76,
  author = {Hanslien, Monica and Artebrant, Robert and Sundnes, Joakim and Tveito, Aslak},
  title = {An unconditionally stable second order method for the Luo-Rudy 1 model used in simulations of defibrillation},
  year = {2008},
  abstract = {Simulations of cardiac defibrillation are associated with considerable
numerical challenges. The cell models have traditionally been discretized  
by first order explicit schemes, which are associated with severe stability
issues. The sharp transition layers in the
solution call for stable and efficient solvers. We
propose a second order accurate numerical method for the 
Luo-Rudy phase 1 model of electrical activity in a cardiac cell, which
provides sequential update of each governing ODE. An \textit{a priori}
estimate for the scheme is given, showing that the bounds of the
variables typically observed 
during electric shocks constitute an invariant region for the
system, regardless of the time step chosen. Thus the
choice of time step is left as a matter of accuracy. Conclusively, we 
demonstrate the theoretical result by some numerical examples,
illustrating second order convergence for the Luo-Rudy 1 model.},
  journal = {journal}
}

@Misc{Simula.SC.84,
  author = {Al-Khayat, Omar},
  title = {Introduction to particle based Lattice Boltzmann Methods},
  year = {2007},
  abstract = {Introduction at a Minisymposium session at ICIAM07},
  howpublished = {Minisymposium Talk}
}

@Misc{Simula.SC.86,
  author = {Dahl, Sigrid Kaarstad},
  title = {A first-approach towards patient-specific 2D FSI-simulation of mitral valve dynamics during diastolic filling},
  year = {2007},
  abstract = {},
  howpublished = {Talk at MekIt-07, Fourth national conference on Computational Mechanics}
}

@Misc{Simula.SC.88,
  author = {Cai, Xing},
  title = {High-performance computing on distributed-memory architecture},
  year = {2008},
  abstract = {},
  howpublished = {Lecture at the 2008 Winter School on Parallel Computing, Jan. 20-25, Geilo, Norway}
}

@Article{Simula.SC.91,
  author = {Clark, Stuart R and Stegman, Dave and M{\"u}ller, R Dietmar},
  title = {Episodicity in back-arc tectonic regimes},
  year = {2008},
  abstract = {The evolution of back-arc basins is tied to the development of the dynamics of the subduction system they are a part of. We present a study of back-arc basins and model their development by implementing 3-D time-dependant computer models of subduction including an overriding plate. We define three types of episodicity - pseudo-, quasi- and hyper-episodicity, and find evidence of these in nature. Quasi-episodicity, in which the back-arc shifts between phases of rifting/spreading and quiescence, is the dominant form of episodic back-arc development in the present. We find this type of episodicity in models for which the system is dynamically consistent - that we have allowed the subducting plate's velocity to be determined by the sinking slabs' buoyancy. Hyper-episodicity occurs in models that are driven by an imposed velocity boundary condition. In this case the full range of behaviour is witnessed - trench advance, retreat and stability. In nature, this occurs rarely, but may occur in places that undergo sudden shifts in plate tectonics and that are therefore unable to self-balance within a noticeable period. Pseudo-episodicity is a category for back-arc rifting or spreading that seems to be periodic, but is really related to ridge-jumps and the brittle nature of the overriding plate. Constant rollback and pseudo-episodic back-arc rifting are witnessed in places where the subducting plate's motion is inhibited by a retarding force, at the trailing edge in our models. This may be, for example, a continent or another subduction zone. Records of back-arc basin rifting/spreading and quiescence from oceanic paleo-age grids have been observed and further validate the conceptual division of epsidocity into these three categories.},
  journal = {Physics of the Earth and Planetary Interiors},
  volume = {171},
  number = {Special Issue on Recent Advanced in Computational Geodynamics: Theory, Numerics and Applications},
  pages = {265-279}
}

@Book{Simula.SC.34,
  editor = {Cai, Xing and Yeh, Jim},
  title = {Quantitative Information Fusion for Hydrological Sciences},
  year = {2008},
  abstract = {In a rapidly evolving world of knowledge and technology, do you ever wonder how hydrology is catching up? This book takes the angle of computational hydrology and
envisions one of the future directions, namely, quantitative integration of high-quality hydrologic field data with geologic, hydrologic, chemical, atmospheric, and biological information to characterize and predict natural systems in hydrological sciences.
Intelligent computation and information fusion are the key words. The aim is to provide both established scientists and graduate students with a summary of recent developments in this topic. The chapters of this edited volume cover some of the most important ingredients for quantitative hydrological information fusion, including data fusion techniques, interactive computational environments, and supporting mathematical and numerical methods. Real-life applications of hydrological information fusion are also addressed.},
  publisher = {Springer Verlag},
  address = {Berlin Heidelberg},
  volume = {79 in Studies in Computational Intelligence},
  isbn = {978-3-540-75383-4}
}

@Inproceedings{Simula.ND.93,
  author = {Zhang, Y},
  title = {Call Admission Control in OFDM Wireless Multimedia Networks},
  year = {2008},
  abstract = {},
  booktitle = {the IEEE International Conference on Communications (ICC), ICC 2008}
}

@Article{Simula.ND.96,
  author = {Zhou, M and Wang, Q and Luo, B and Guo, Y and Ong, C and Zhang, Y and Zhang, Y and Soh, Y and Miura, R},
  title = {Performance Improvement and Wavelength Reuse in Millimeter-Wave Radio-over-Fiber Links Incorporating All-fiber Optical Interleaver},
  year = {2008},
  abstract = {},
  journal = {Elsevier Optics Communications}
}

@Inproceedings{Simula.ND.111,
  author = {Stensland, H{\r a}kon Kvale and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Evaluation of Multi-Core Scheduling Mechanisms for Heterogeneous Processing Architectures},
  year = {2008},
  abstract = {General-purpose CPUs with multiple cores are established
products, and new heterogeneous technology like the Cell
broadband engine and general-purpose GPUs bring an even
higher degree of true multi-processing into the market. However, 
means for utilizing the processing power is immature.
Current tools typically assume that exclusive use of these
resources is sufficient, but this assumption will soon be invalid 
because the interest in using their processing power
for general-purpose tasks. Among the applications that can
benefit from such technology is transcoding support for distributed 
media applications, where remote participants join
and leave dynamically. Transcoding consists of several clearly
separated processing operations that consume a lot of resources, 
such that individual processing units are unable to
handle all operations of a session of arbitrary size. The
individual operations can then be distributed over several
processing units, and data must be moved between them
according to the dependencies between operations. Many
multi-processor scheduling approaches exist, but to the best
of our knowledge, a challenge is still to find mechanisms that
can schedule dynamic workloads of communicating operations 
while taking both the processing and communication
requirements into account. For such applications, we believe
that feasible scheduling can be performed in two levels, i.e., 
divided into the task of placing a job onto a processing unit
and the task of multitasking time-slices within a single processing 
unit. We have implemented some simple high-level
scheduling mechanisms and simulated a video conferencing
scenario running on topologies inspired by existing systems
from Intel, AMD, IBM and nVidia. Our results show the
importance of using an efficient high-level scheduler.},
  booktitle = {Network and Operating System Support for Digital Audio and Video (NOSSDAV 2008)},
  publisher = {ACM}
}

@Misc{Simula.SC.109,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Gr{\o}ttum, Per and Tveito, Aslak and Abildgaard, Andreas and Fjeld, Jan Gunnar and Haugaa, Kristina Hermann},
  title = {Theoretical and practical aspects of the inverse problem of electrocardiography},
  year = {2008},
  abstract = {},
  howpublished = {Presented at Institut f{\"u}r Mathematik und Wissenschaftliches Rechnen, Karl Franzes Universit{\"a}t in Graz, Austria. Also presented at Institut f{\"u}r Numerische und Angewandte Mathematik, Westf{\"a}lische Wilhelms-Universit{\"a}t M{\"u}nster, Germany}
}

@Inproceedings{Simula.ND.73,
  author = {Vik, Knut-Helge and Halvorsen, P{\r a}l and Griwodz, Carsten},
  title = {Multicast tree diameter for dynamic distributed interactive applications},
  year = {2008},
  abstract = {Considerable attention has been given to latency reduction in distributed interactive applications. Such applications may have stringent latency requirements and dynamic user groups. In this paper, our focus has been on using application layer multicast with a centralized approach to the group management. The groups are organized in overlay networks that are created using graph algorithms. We investigate many spanning tree problems with particular focus on reducing the diameter of a tree, i.e., the maximum pairwise latency between users. In addition, we focus on reducing the time it takes to execute membership changes. In that context, we use core selection heuristics to find well located group nodes, and edge pruning algorithms to reduce the number of edges in an otherwise fully meshed overlay. Our edge pruning algorithms strongly connect well located group nodes to the remaining group members, to create new and pruned group graphs. Such that, when a tree algorithm is given a pruned group graph, it is manipulated into creating trees with lower diameter. In the investigation presented here, we have implemented and experimentally analyzed spanning tree heuristics, core selection heuristics and edge pruning algorithms. We find that faster heuristics that do not explicitly optimize the diameter are able to compete with slower heuristics that optimize the diameter.},
  booktitle = {IEEE International Conference on Computer Communications - IEEE INFOCOM},
  publisher = {IEEE},
  edition = {27th}
}

@Misc{Simula.SE.180,
  author = {J{\o}rgensen, Magne},
  title = {Help! My Brain is Out of Control! Impact from Irrelevant and Misleading Information in Software Effort Estimation},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at BEKK-seminar}
}

@Misc{Simula.SE.181,
  author = {J{\o}rgensen, Magne},
  title = {Rasjonelle beslutninger ved valg av metoder og verkt{\o}y},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at JavaZone 2007}
}

@Misc{Simula.SE.190,
  author = {J{\o}rgensen, Magne},
  title = {Er selvbeherskelsen din sliten og utrent?},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.193,
  author = {J{\o}rgensen, Magne},
  title = {Sannsynligvis vil noe usannsynlig skje},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SC.37,
  author = {Hake, Johan Elon and Lines, Glenn Terje},
  title = {Stochastic binding of Ca2+ ions in the dyadic cleft continuous vs Random walk description of diffusion},
  year = {2007},
  abstract = {},
  howpublished = {Poster presentation at 5th Annual CHFR Symposium}
}

@Article{Simula.SC.43,
  author = {Rahman, S and Stace, T. M and Langtangen, Hans Petter and Kataoka, M and Barnes, C. H. W},
  title = {Pulse-Induced Acoustoelectric Vibrations in Surface-Gated GaAs-based Quantum Devices},
  year = {2007},
  abstract = {},
  journal = {Physical Review B},
  volume = {75},
  number = {205303}
}

@Inproceedings{Simula.SC.44,
  author = {Langtangen, Hans Petter},
  title = {A Case Study in High-Performance Mixed-Language Programming},
  year = {2007},
  abstract = {},
  booktitle = {Applied Parallel Computing - State of the Art in Scientific Computing (PARA'06)},
  editor = {B. K{\r a}gstr{\o}m, E. Elmroth, J. Dongarra, J.  Wasniewski},
  pages = {36-49},
  publisher = {Springer},
  isbn = {978-3540290674}
}

@Techreport{Simula.SC.50,
  author = {Kirby, Robert C and Logg, Anders},
  title = {Benchmarking Domain-Specific Compiler Optimizations for Variational Forms},
  year = {2007},
  abstract = {},
  institution = {Finite Element Center},
  type = {Preprint},
  number = {2007-04}
}

@Misc{Simula.SE.194,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {How should we (not) design empirical studies of software development? },
  year = {2007},
  abstract = {},
  howpublished = {Invited presentation at the University of New South Wales, Sydney}
}

@Misc{Simula.SC.59,
  author = {Sundnes, Joakim},
  title = {Computational techniques for heart muscle mechanics},
  year = {2007},
  abstract = {The heart occupies the most central role of the cardiovascular system, with the 
crucial role of supplying a continuous flow of blood through the vast network of vessels
composing the circulatory system. Mathematical models of the heart in 
health and disease is an increasingly important tool for improving our understanding 
of this vital organ, and thereby help to reduce health problems and costs related
to cardiovascular disorders.

In this talk we give an overview of computational challenges and techniques related to
modelling the mechanical function of the heart muscle. Models applied for
this purpose vary in complexity from the simplest pressure-volume relations based on 
a given time varying elastance, to systems of differential equations that give a detailed 
description of cardiac electro-mechanical interaction. 

The primary focus of the talk will be on finite element modeling of the heart muscle, 
which includes modeling the passive mechanical properties of the tissue as well as 
the active muscle contraction and its coupling to electrophysiology. The heart muscle is
normally modeled as a hyper-elastic material, with strongly non-linear and anisotropic
material behavior. The resulting mathematical model consists of a large-strain 
elasticity equation, which is coupled to DAE systems that describe electro-chemical 
reactions on cell level, and also to a system of PDEs that describe 
the conduction of the electrical signal in the tissue. We discuss some 
approaches for solving this system, which offer a varying degree of coupling and 
feedback between electrophysiology and mechanics. We also discuss different 
approaches for coupling the heart muscle models to the rest of the 
circulatory system. The latter topic will also be covered in more detail in the other 
talks of the minisymposium.

},
  howpublished = {Minisymposium talk at ICIAM 07, Zurich}
}

@Misc{Simula.SC.60,
  author = {Sundnes, Joakim},
  title = {Software components for biomedical flows},
  year = {2007},
  abstract = {},
  howpublished = {National seminar on medical technology, NFA (Norsk forening for automatisering)}
}

@Misc{Simula.SC.61,
  author = {Sundnes, Joakim and Aln{\ae}s, Martin Sandve and Mardal, Kent-Andre},
  title = {A finite element model of cardiac electrophysiology and mechanics},
  year = {2007},
  abstract = {Background
Computer models of heart function has the potential to become a valuable tool both
for medical research and clinical practice. Simulations based on 
accurate biophysical models increase our understanding of heart physiology and
pathology, and may also predict the outcome of therapeutic interventions and 
drugs. This has been an active research area for decades, but the models, and
in particular their application in clinical practice, are still in an early development stage.
Unresolved challenges in the field include the extreme complexity of the biological
processes involved, and the multiscale nature of the problem, covering processes 
from molecular level to the complete organ system.

Models and methods
Passive heart tissue is commonly modeled as a hyperelastic material, which undergoes
large deformations during normal heart function. We apply an exponential stress-strain
relation, resulting in a strongly non-linear elasticity equation describing the 
deformations of the muscle. In order to model the actively contracting muscle, 
the elasticity equation is coupled to systems of ordinary differential equations (ODEs) 
which describes the electrical activation and active force development in 
the muscle cells. These systems
are in turn coupled to a system of partial differential equations (PDEs) 
known as the bidomain model, which describes propagation of the electrical 
signal through the heart muscle. 

We apply operator splitting to divide the complete model into two separate PDE
systems, describing electrophysiology and mechanics on tissue level, and 
one system of ODEs that describes electro-mechanical coupling on cell level. 
These individual systems are then discretized in time with implicit schemes of first or 
second order, and in space with the finite element method.

Simple no-flux boundary conditions are applied for the electrophysiology problem, 
but the mechanics problem is subject to fairly complex dynamic boundary conditions 
resulting from the interaction with the blood flow. To avoid solving a fluid-structure 
interaction problem involving the full Navier-Stokes equation for blood flow, we
employ a lumped parameter model to describe the circulation. This model is a system 
of ODEs that describes pressures, volumes and flows through a simplified, closed 
loop vessel system. When this model is coupled to the finite element model for the 
heart muscle the result is an index one differential-algebraic (DAE) system, where the 
algebraic part contains the finite element based electro-mechanics model. 
We propose to solve the DAE system with a  Runge-Kutta method of
Radau type, where the time step is automatically adjusted to the different phases 
of the heart cycle. 

Results
Combining the Radau solver with operator splitting and finite element discretization 
results in a fairly robust method, which is flexible with respect to changing or replacing
individual parts of the model. Initial test results confirm the robustness and 
convergence of the algorithm, and verifies that the mechanics part of the problem can 
be solved with good accuracy. The elecrophysiology part, which contains very 
steep gradients, has still not been solved with the desired accuracy.},
  howpublished = {VII International conference on computational plasticity}
}

@Article{Simula.SC.62,
  author = {Glimsdal, S and Pedersen, G. K and Langtangen, Hans Petter and Shuvalov, V and Dypvik, H},
  title = {Tsunami Generation and Propagation from the Mj{\o}lnir Asteroid Impact},
  year = {2007},
  abstract = {},
  journal = {Meteoritics \& Planetary Science},
  volume = {42},
  number = {9},
  pages = {1473-1493}
}

@Article{Simula.SC.67,
  author = {Holm{\r a}s, H and Clamond, D and Langtangen, Hans Petter},
  title = {A Pseudospectral Fourier
Method for a 1D Incompressible Two-Fluid Model},
  year = {2007},
  abstract = {},
  journal = {International Journal for Numerical Methods in Fluids}
}

@Article{Simula.SC.68,
  author = {Holm{\r a}s, H and Sira, T and Nordsveen, M and Langtangen, Hans Petter},
  title = {Analysis of a 1D Incompressible Two-Fluid Model Including Artificial
Diffusion},
  year = {2007},
  abstract = {},
  journal = {IMA Journal of Applied Mathematics}
}

@Inproceedings{Simula.ND.85,
  author = {Li, Cindy Y and Wong, Prudence W.H and Xin, Qin and Yung, Fencol C.C},
  title = {Approximating Border Length for DNA Microarray Synthesis},
  year = {2008},
  abstract = {},
  booktitle = {The 5th Annual Conference on Theory and Applications of Models of Computation (TAMC'08)},
  publisher = {Springer LNCS}
}

@Article{Simula.ND.87,
  author = {Robles-G{\a\'o}mez, Antonio and Berm{\a\'u}dez, Aurelio and Casado, Rafael and Solheim, {\r A}shild Gr{\o}nstad and S{\o}dring, Thomas and Skeie, Tor},
  title = {A New Distributed Management Mechanism for ASI Based Networks},
  year = {2008},
  abstract = {},
  journal = {Computer Communications}
}

@Inproceedings{Simula.SC.77,
  author = {Hellevik, Leif Rune and Dahl, Sigrid Kaarstad and Skallerud, Bj{\o}rn},
  title = {A first-approach towards patient-specific 2D FSI-simulation of mitral valve dynamics during diastolic filling},
  year = {2007},
  abstract = {The mitral valve controls the flow of blood into the ventricle during diastole and prevents
backflow into the atrium in systole. The two mitral valve leaflets have significantly different
geometries. The dynamics of the leaflets and their influence on the transmitral and intraventricular
flow is a controversial topic. In this paper we report results from work in progress on fluid
structure interaction simulations of the mitral leaflets during left ventricular filling. The aim of the
present study was to assess the impact of left ventricular initial pressures and vortices on the mitral
leaflet dynamics. An explicitly coupled fluid structure interaction scheme was utilized for the
numerical simulations. A 2D transient representation of the left ventricular wall movement was
imposed as boundary conditions for the simulations. The left ventricular wall was rendered by
ultrasound recordings, and speckle tracking was used for transient tracking of the wall movement
during diastole. Thus, the structural calculations are simplified and involve only the mitral valves.
The ultrasound measurements were used both for validation/comparison with the simulated mitral
valve dynamics as well as for imposition of the left ventricular wall movement. Our simulations
show that the initial pressure configuration in the left heart is significant for the mitral valve dynamics.
The simulations also indicate that the mitral valve flutter has important bearings on the
vortex formation in the vicinity of the mitral valves.},
  booktitle = {MekIT-07, Fourth national conference on Computational Mechanics},
  editor = {B. Skallerud and H. I Andersson},
  pages = {175--184},
  publisher = {Tapir Academic Press},
  address = {Trondheim},
  isbn = {978-82-519-2235-7}
}

@Misc{Simula.SC.80,
  author = {Logg, Anders},
  title = {Automated Solution of Differential Equations},
  year = {2007},
  abstract = {},
  howpublished = {Talk at 6th International Congress on Industrial and Applied Mathematics (ICIAM 2007), Z{\"u}rich, 2007-07-18}
}

@Misc{Simula.SC.87,
  author = {Cai, Xing},
  title = {Parallel Computing; Why \& How?},
  year = {2008},
  abstract = {},
  howpublished = {Lecture at the 2008 Winter School on Parallel Computing, Jan. 20-25, Geilo, Norway}
}

@Article{Simula.SC.96,
  author = {Aln{\ae}s, Martin Sandve and Logg, Anders and Mardal, Kent-Andre and Skavhaug, Ola and Langtangen, Hans Petter},
  title = {Unified Framework for Finite Element Assembly},
  year = {2009},
  abstract = {},
  journal = {International Journal of Computational Science and Engineering}
}

@Inproceedings{Simula.ND.92,
  author = {Zhang, Y},
  title = {Authentication Overhead in Wireless Networks},
  year = {2008},
  abstract = {},
  booktitle = {the IEEE International Conference on Communications (ICC), ICC 2008}
}

@Inproceedings{Simula.ND.94,
  author = {Zhang, Y},
  title = {Dynamic Spectrum Access in Cognitive Radio Wireless Networks},
  year = {2008},
  abstract = {},
  booktitle = {the IEEE International Conference on Communications (ICC), ICC 2008}
}

@Inproceedings{Simula.ND.95,
  author = {Zhang, Y and Soong, B and Zhang, L and Leng, S},
  title = {Utility-based Weighted Prioritization in Heterogeneous Wireless Networks},
  year = {2008},
  abstract = {},
  booktitle = {the IEEE International Conference on Circuits \& Systems for Communications (ICCSC), ICCSC 2008}
}

@Article{Simula.ND.63,
  author = {Sem-Jacobsen, Frank Olaf and Skeie, Tor and Lysne, Olav},
  title = {Dynamic Fault Tolerance in Multistage Interconnection Networks},
  year = {2007},
  abstract = {Deterministically routed fat trees are a very common
communication architecture in large-scale parallel computers. We present a routing method for deterministically routed, both distributed and source routed, fat
trees that is able to handle several concurrent
faults and that transparently returns to the original routing
strategy once the faulty components have recovered.  The method is
completely local and dynamic. It only requires a
small extra functionality in the switches to handle misrouting
around a fault. The method guarantees connectedness and
deadlock and livelock freedom for one fault, without
additional virtual channels.  By adding only one virtual
channel, the method guarantees a fully functional network with
up to radix/2-1 arbitrary simultaneous faults. Our simulation experiments show a
graceful degradation of performance as faults are added.
Furthermore, they demonstrate that for most fault
combinations, our method will be able to handle significantly
more faults than it can give a 100\\% guarantee for.

To the best of our knowledge, this is the first reported
method for local dynamic fault tolerance for deterministically
routed Fat-Trees that does not require additional hardware.
Furthermore, it is the first reported algorithm for local
rerouting around faults that work in a source-routed
environment.},
  journal = {IEEE Transactions on Computers}
}

@Article{Simula.ND.102,
  author = {Vik, Knut-Helge and Halvorsen, P{\r a}l and Griwodz, Carsten},
  title = {Evaluating Steiner tree heuristics and diameter variations for Application Layer Multicast},
  year = {2008},
  abstract = {Latency reduction in distributed interactive applications has been studied intensively. Such applications may have stringent latency requirements and dynamic user groups. We focus on application-layer multicast with a centralized approach to the group management. The groups are organized in overlay networks that are created using graph algorithms that create a tree structure for the group. A tree has no cycles and uses a small routing table, as opposed to a connected overlay mesh. We investigate a group of spanning tree problems that are referred to as Steiner tree problems, and we have a particular focus on reducing the diameter of a tree, which is the maximum pairwise latency in a tree. In addition, we focus on reducing the time it takes to execute membership changes. In that context, we use core-selection heuristics to find well-placed client nodes, and edge-pruning algorithms to reduce the number of edges in an otherwise fully meshed overlay. Our edge-pruning algorithms strongly connect well-placed client nodes to the remaining group members, to create new and pruned group graphs. Consequently, when a tree algorithm is applied to a pruned group graph, it is manipulated into creating trees with a smaller diameter.We devised new Steiner-tree heuristics that reduced the diameter, and also proposed new edge-pruning algorithms to make the tree construction faster. These heuristics and algorithms were implemented and analyzed experimentally along with several spanning-tree and core-selection heuristics found in the literature. We found
that a full-mesh of shortest paths makes it difficult for Steiner-tree heuristics to find better trees than spanning tree algorithms. The network seen from the application layer is in fact a full mesh of shortest paths. In addition, we found that faster Steiner-tree heuristics that do not explicitly optimize the diameter are able to compete with slower heuristics that do optimize it.},
  journal = {Special issue of the international journal Computer Networks on Complex Computer and Communication Networks, Elsevier}
}

@Article{Simula.SE.179,
  author = {Garousi, Vahid and Briand, Lionel Claude and Labiche, Yvan},
  authorURLs = {  and /people/briand and http://www.sce.carleton.ca/faculty/labiche},
  title = {Traffic-aware stress testing of distributed real-time systems based on UML models using genetic algorithms},
  year = {2008},
  abstract = {This paper presents a model-driven, stress test methodology aimed at increasing chances of discovering faults related to network traffic in distributed real-time systems (DRTS). The technique uses the UML 2.0 model of the distributed system under test, augmented with timing information, and is based on an analysis of the control flow in sequence diagrams. It yields stress test requirements that are made of specific control flow paths along with time values indicating when to trigger them. The technique considers different types of arrival patterns (e.g., periodic) for real-time events (common to DRTSs), and generates test requirements which comply with such timing constraints. Though different variants of our stress testing technique already exist (that stress different aspects of a distributed system), they share a large amount of common concepts and we therefore focus here on one variant that is designed to stress test the system at a time instant when data traffic on a network is maximal. Our technique uses genetic algorithms to find test requirements which lead to maximum possible traffic-aware stress in a system under test. Using a real-world DRTS specification, we design and implement a prototype DRTS and describe, for that particular system, how the stress test cases are derived and executed using our methodology. The stress test results indicate that the technique is significantly more effective at detecting network traffic-related faults when compared to test cases based on an operational profile.},
  journal = {Journal of Systems and Software},
  volume = {81},
  number = {2},
  pages = {161-185}
}

@Misc{Simula.SC.38,
  author = {Hake, Johan Elon and Lines, Glenn Terje},
  title = {Stochastic binding of Ca2+ ions in the dyadic cleft continuous vs Random walk description of diffusion},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at ICIAM 07},
  note = {Contributed talk}
}

@Article{Simula.SC.40,
  author = {Hake, Johan Elon and Lines, Glenn Terje},
  title = {Stochastic binding of Ca2+ in the dyadic cleft: continuous vs. random-walk description of diﬀusion. },
  year = {2008},
  abstract = {Ca2+ signaling in the dyadic cleft in ventricular myocytes is fundamentally discrete and stochastic. We study the stochastic binding of single Ca2+ ions to receptors in the cleft using two different models of diffusion: a stochastic and discrete Random Walk (RW) model, and a deterministic continuous model. We investigate whether the latter model, together with a stochastic receptor model, can reproduce binding events registered in fully stochastic RW simulations. By evaluating the continuous model goodness-of-fit for a large range of parameters, we present evidence that it can. Further, we show that the large fluctuations in binding rate observed at the level of single time-steps are integrated and smoothed at the larger timescale of binding events, which explains the continuous model goodness-of-fit. With these results we demonstrate that the stochasticity and discreteness of the Ca2+ signaling in the dyadic cleft, determined by single binding events, can be described using a deterministic model of Ca2+ diffusion together with a stochastic model of the binding events, for a specific range of physiological relevant parameters. Time-consuming RW simulations can thus be avoided. We also present a new analytical model of bimolecular binding probabilities, which we use in the RW simulations and the statistical analysis.},
  journal = {Biophysical Journal},
  volume = {94},
  number = {11},
  pages = {4184-4201}
}

@Article{Simula.ND.74,
  author = {Zhang, Y and Yang, Laurence T and Ma, J and Zheng, J},
  title = {Quantitative Analysis of Location Management and QoS in Wireless Networks},
  year = {2007},
  abstract = {},
  journal = {Elsevier Journal of Network and Computer Applications}
}

@Misc{Simula.SC.58,
  author = {Sundnes, Joakim},
  title = {Using mathematical models to test physiological hypotheses},
  year = {2007},
  abstract = {Mathematical models have been used in physiological research for centuries, but 
they have so far played a less important role than in many other branches of science.
Important reasons for this include the extreme complexity and multi-scale nature
of biological systems, which makes it challenging both to derive accurate models and
to solve the resulting mathematical equations. 

With the increasing availabilty of powerful computer hardware and 
numerical software, accompanied by a revolutionary increase of knowledge in
molecular biology, this situation is currently about to change. The last ten years in 
particular, we have seen the advent of increasingly accurate mathematical models
of physiological systems, linking biophysical processes across a wide range of
spatial and temporal scales. An illustrative example is mathematical models of heart
electrophysiology and mechanics, where models have been derived that link
processes on cellular and sub-cellular level to the function of the complete organ. 
Although modeling of other components of the body lags behind that of the 
cardiovascular system, there is an ongoing development to model other organs and
organ systems. Profiled initiatives in this direction include the IUPS Physiome project 
(http://www.physiome.org.nz) and the Virtual Physiological Human
(http://www.europhysiome.org). Although slightly differing in focus, the vision of both
these initiatives is to construct advanced computational models of the complete 
human body, which may be used both for clinical work and as test beds for new
physiological hypotheses.

In this talk we present a few examples of mathematical models describing physiological
phenomena, and how they have been used to verify and quantify physiological and
clinical knowledge. The focus will be on mathematical models of the cardiovascular
system.

},
  howpublished = {Invited talk at the annual meeting of the Scandinavian Physiological Society}
}

@Misc{Simula.SC.70,
  author = {Cai, Xing},
  title = {Bridging the gap between computational scientists and HPC},
  year = {2007},
  abstract = {},
  howpublished = {Article published in Meta, Number 3}
}

@Article{Simula.SE.197,
  author = {F{\o}lstad, Asbj{\o}rn and Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag},
  title = {Usability Inspection: A Comparative Study of the Evaluator Performance of Work-Domain Experts vs. Usability Experts},
  year = {2008},
  abstract = {When applications that are tailored to work domains of which usability experts have little knowledge are evaluated, the usefulness of usability inspection methods is challenged. To counter this challenge, usability inspections with work-domain experts have been explored, but no empirical research has been reported on such experts{\textquoteright} performance as evaluators. The present study compares the performance of work-domain experts and usability experts, with respect to the validity and thoroughness of their evaluations. Fifteen work-domain experts and 12 usability experts participated. The applied inspection method was group-based expert walkthrough. The basis for comparison was user test results. The work-domain experts produced equally valid but less thorough evaluation results than the usability experts. On average, two work-domain experts produced results of similar quality to those of one usability expert, which implies that work-domain experts may substitute for usability experts if the number of evaluators is increased. These results should be taken into account when developing future usability inspection methods with work-domain experts as evaluators.},
  journal = {a journal}
}

@Misc{Simula.SC.78,
  author = {Logg, Anders},
  title = {Efficient Representation of Computational Meshes},
  year = {2007},
  abstract = {},
  howpublished = {Talk at MekIT'07, Trondheim, 2007-05-23}
}

@Misc{Simula.SC.79,
  author = {Logg, Anders and Skavhaug, Ola},
  title = {Software Components for Solving PDEs},
  year = {2007},
  abstract = {},
  howpublished = {Talk at Modelling and Computation of Biomedical Processess Workshop, Simula Oslo, 2007-06-12}
}

@Misc{Simula.SC.81,
  author = {Logg, Anders},
  title = {Finite Element Code Generation: Simplicity, Generality, Efficiency},
  year = {2007},
  abstract = {},
  howpublished = {Talk at Software Issues in Computational Science and Engineering (SCSE 2007), Uppsala, 2007-08-11}
}

@Misc{Simula.SC.82,
  author = {Logg, Anders},
  title = {Activities at Simula: FEniCS and the Center for Biomedical Computing},
  year = {2007},
  abstract = {},
  howpublished = {Computational and Applied Mathematics Seminar, Chalmers G{\"o}teborg, 2007-09-26}
}

@Misc{Simula.SC.93,
  author = {Bruaset, Are Magnus},
  title = {By Thinking Constantly About It - Scientific Computing in Simula},
  year = {2007},
  abstract = {Simula Research Laboratory conducts research in the fields of scientific computing, software engineering, and networks and distributed systems. In addition, the organization has a strong focus on education and innovation. Recently, the educational efforts have been strengthened through the foundation of the Simula School of Research and Innovation, a graduate school that collaborates closely with the University of Oslo and is co-owned by industrial partners.

<p>

The scientific computing research at Simula spans from development of generic software tools, via numerical methods, to a variety of applications in science and technology. The field of scientific computing is characterized by being interdisciplinary, involving mathematics, computer science, and the relevant application field(s).

<p>

In this talk I will briefly present the Simula organization and the principles on which our research activities are based. Thereafter, I will give an overview of the ongoing projects in scientific computing. In particular, I will discuss our research on computational middleware, simulation of electromechanical behavior of the heart, use of inverse modeling for determination of myocardial infarction, biomedical flow simulations, and different applications in geoscience. These applications range from simulation of the deformation in sedimentary basins to 4D modeling of the lithosphere. The research in computational geosciences is financed by one of the leading technological innovators in the oil and gas business, StatoilHydro. },
  howpublished = {Invited talk at College of Engineering, Pennsylvania State University}
}

@Misc{Simula.SC.95,
  author = {Mardal, Kent-Andre},
  title = {Computation of Hemodynamics in the Circle of Willis},
  year = {2007},
  abstract = {},
  howpublished = {Talk at Workshop: Modelling and Computation of Biomedical Processes - June 7-14, 2007}
}

@Misc{Simula.SC.108,
  author = {Hanslien, Monica and Sundnes, Joakim and Tveito, Aslak},
  title = {Simulations of reentrant arrhythmias in the heart, and defibrillation as a treatment},
  year = {2006},
  abstract = {},
  howpublished = {Presented at NADA, KTH, Sweeden}
}

@Inproceedings{Simula.ND.112,
  author = {Petlund, Andreas and Evensen, Kristian R and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {TCP Enhancements For Interactive Thin-Stream Applications},
  year = {2008},
  abstract = {TCP is frequently used for interactive multimedia applications like online games and voice-over-IP (VoIP) due to firewall issues and fairness considerations. However, traffic analysis shows that these streams usually have small packets and a low packet rate, and that in case of loss, severe latency penalties occur for all existing TCP variations in Linux. In this demonstration, we show how small, efficient TCP enhancements greatly improve the perceived quality of such low latency, interactive applications.},
  booktitle = {NOSSDAV 2008},
  isbn = {978-1-60588-157-6/05/2008}
}

@Article{Simula.ND.56,
  author = {Hu, H and Zhang, Y and Chen, H},
  title = {An Effective QoS Differentiation Scheme for Wireless Mesh Networks},
  year = {2008},
  abstract = {},
  journal = {IEEE Network},
  volume = {22},
  number = {1},
  pages = {66-73}
}

@Article{Simula.ND.101,
  author = {Robles-G{\a\'o}mez, Antonio and Berm{\a\'u}dez, Aurelio and Casado, Rafael and Solheim, {\r A}shild Gr{\o}nstad},
  title = {A Dynamic Distributed Mechanism for Reconfiguring High-Performance Networks},
  year = {2008},
  abstract = {},
  journal = {Parallel Computing}
}

@Inproceedings{Simula.ND.33,
  author = {Vik, Knut-Helge and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Dynamic Group Membership Management for Distributed Interactive Applications},
  year = {2007},
  abstract = {Distributed interactive applications have become increasingly popular, making it important to address their communication needs, where, one of the needs is group communication. In this paper, we consider the applications in which it is at any given time possible to divide its users into groups. The group membership changes over time and the group division is unrelated to the physical proximity. As a way of enabling the group communication in distributed interactive applications we choose application layer multicast. We use simulation to evaluate several dynamic algorithms for managing overlay multicast trees. They are compared with respect to four metrics that can be relevant for a distributed interactive application. These are total tree cost, diameter, reconfiguration time and stability. We demonstrate algorithms that perform well for these metrics although they do not consider all users during reconfiguration.},
  booktitle = {32nd IEEE Conference on Local Computer Networks (LCN)},
  editor = {Ken Christensen and Matthias Frank},
  pages = {141--148},
  publisher = {IEEE},
  isbn = {978-0-7695-3000-0}
}

@Article{Simula.SE.124,
  author = {Kampenes, Vigdis By and Dyb{\r a}, Tore and Hannay, Jo Erskine and Sj{\o}berg, Dag I. K},
  title = {A systematic review of quasi-experiments in software engineering},
  year = {2008},
  abstract = {},
  journal = {Information \& Software Technology}
}

@Inproceedings{Simula.ND.34,
  author = {Cicic, Tarik and Hansen, Audun Fosselie and Apeland, Ole Kristoffer},
  title = {Redundant Trees for Fast IP Recovery},
  year = {2007},
  abstract = {Slow convergence of IP routing in case of network component failures   represents a problem for emerging time-critical network   applications. It has motivated work on IP fast reroute mechanisms   that use local, proactive recovery in order to   immediately provide alternative routes for the affected data   packets.  We present a new mechanism for IP fast recovery called IP   Redundant Trees (IPRT). IPRT builds two redundant trees per network   destination and pre-calculates backup next hops based on them.   Combined with a smart forwarding scheme, IPRT provides proactive,   local and loop-free recovery in connectionless IP networks. IPRT is   simple and flexible, and requires a low increase of the routing   state amount.  We evaluate several key performance metrics in IPRT   and other actual IP fast recovery proposals and argue that IPRT   indeed has very attractive properties.},
  booktitle = {Broadnets 2007},
  editor = {IEEE},
  publisher = {IEEE},
  isbn = {0000000}
}

@Inproceedings{Simula.ND.29,
  author = {Kvalbein, Amund and Lysne, Olav},
  title = {How can Multi-Topology routing be used for intradomain Traffic Engineering?},
  year = {2007},
  abstract = {The concept of Multi-Topology routing allows each router in a
  network to maintain several valid routes to each destination. This
  increases the possibilities to spread traffic towards a destination
  over multiple paths with connectionless routing protocols like OSPF
  or IS-IS. In this paper, we report early ideas on how this can be
  utilized as a Traffic Engineering tool. We look at both offline and
  online approaches, and argue that a Multi-Topology based solution
  has advantages over previous solutions in both paradigms.},
  booktitle = {ACM SIGCOMM Workshop on Internet Network Management 2007},
  editor = {unknown},
  publisher = {ACM sigcomm},
  isbn = {unknown}
}

@Phdthesis{Simula.ND.50,
  author = {Hansen, Audun Fosselie},
  title = {Fast Reroute in IP Networks},
  year = {2007},
  abstract = {More and more applications with stringent requirements to delivery are being offered through the Internet. Typical examples are real time applications like voice over IP, IP TV, and business critical applications. If a component in the network fails, it is important to be able to route the traffic on other resources very rapidly. For IP networks, this is currently not the standard. The IP re-convergence process that is specified in the most common link state routing protocols does not operate in a time-scale that is compliant with the new type of applications. The IP protocols are continuously being deployed in different kinds of networks. Wireless networks and special purpose networks like sensor networks serve as present examples. These networks may require even stronger requirements to resilient routing due to higher frequency of failures than what has been common in traditional IP networks. The main theme for this thesis has been to develop and evaluate schemes that offer fast reroute around failed components in packet networks. First, we propose and evaluate a scheme named Resilient Routing Layer (RRL). RRL builds backup paths based on spanning sub-topologies of the network graph. RRL is not tailored for any specific routing or forwarding paradigms. We also propose a set of schemes that are based on link weight manipulation in multiple logical IP topologies. These schemes are tailored for link state based routing in connectionless IP networks. They serve as a supplement to IP full reconvergence in order to provide recovery in a suitable time-scale. Several schemes are developed and optimized according to different properties. We also propose a scheme named IPRT that relies on destination based backup next hops. IPRT bounds the additional state requirements compared to the schemes based on multiple topologies. The schemes are evaluated according to a set of functional properties and critical performance metrics, both in fixed and wireless networks. In addition, the thesis gives a thorough comparison with the most related schemes. 3},
  school = {University of Oslo},
  type = {PhD thesis},
  isbn = {ISSN 1501-7710}
}

@Article{Simula.SC.27,
  author = {Linge, S. O and Lines, Glenn Terje and Sundnes, Joakim and Tveito, Aslak},
  title = {On the frequency of automaticity during ischemia in simulations based on stochastic perturbations of the Luo-Rudy 1 model},
  year = {2008},
  abstract = {},
  journal = {Computers in biology and medicine}
}

@Book{Simula.ND.32,
  editor = {Griwodz, Carsten and Zimmermann, Roger},
  authorURLs = {/people/griff and http://www.comp.nus.edu.sg/\~rogerz/roger.html},
  title = {Multimedia Computing and Networking 2007},
  year = {2007},
  abstract = {It is our great pleasure to welcome you  to  the  Multimedia  Computing  and
Networking Conference (MMCN) 2007 at Electronic Imaging!

This year's MMCN is the 14th installment of the conference series. MMCN  has
traditionally served as an opportunity for researchers and practitioners  to
share their perspectives with others interested in the  various  aspects  of
multimedia systems and networks. Thanks to the outstanding  efforts  of  our
publicity co-chairs Andreas Mauthe, Wei-Tsang  Ooi,  and  Michael  Zink  and
with  the  additional  support  of  the  ACM  SIG  Multimedia,  the  program
committee had a good number of papers to choose from.

Each  original  submission  was  reviewed  by  at  least  three   reviewers,
conflicting review results were addressed in email discussions, and all  the
papers that received favorable reviews were intensely discussed  during  the
program committee's day-long meeting on  August  26,  2006,  that  committee
members attended either in person, or by IP or POTS  audio  conference.  The
average quality of submission was very high, so that we were able to  accept
18 full and seven short papers this year.

Putting together MMCN 2007 was a team effort. First of all,  we  would  like
to thank the  authors  for  their  submissions  and  for  choosing  MMCN  to
highlight their work. Our thanks also go to the  program  committee  members
and the external  reviewers  for  their  excellent  job  in  evaluating  the
submissions.  We  also  thank  the  following  colleagues  who   contributed
additional reviews: Sakire Arslan Ay, Jacob  Chakareski,  Beomjoo  Seo,  and
Leslie S. Liu. We would  further  like  to  express  our  gratitude  to  the
Computer Science department of the University of Massachusetts  that  hosted
the PC meeting, and Michael Zink in particular for being gracious  hosts  in
all aspects. We also want to thank SPIE, IS\&T, and ACM  SIG  Multimedia  for
making MMCN possible again this year.

The program addresses multimedia systems issues under the  topics  of  video
coding, systems and  measurements,  user  perception,  resource  management,
streaming scalability, multimedia systems,  and  multimedia  middleware.  In
addition to the presentation of the research papers, the program features  a
keynote speech by Ramesh Sarukkai of Yahoo! Inc.,  and  a  panel  that  will
discuss Getting in Touch with  the  Real  World  -  how  multimedia  systems
interact with physical objects. We hope  that  you  will  find  the  program
interesting and thought-provoking and that the conference will  provide  you
with a valuable opportunity to  share  ideas  with  other  researchers  from
institutions around the world!

                                                             Carsten Griwodz
                                                            Roger Zimmermann
},
  publisher = {SPIE},
  address = {Bellingham, WA, USA},
  volume = {6504},
  isbn = {9780819466174}
}

@Inproceedings{Simula.ND.31,
  author = {Nordbotten, Nils Agne and Skeie, Tor},
  title = {A Routing Methodology for Dynamic Fault Tolerance in Meshes and Tori},
  year = {2007},
  abstract = {This paper proposes a fully distributed fault-tolerant routing methodology for tori and meshes. A dynamic fault-model is supported, enabling the network to remain fully operational at all times. Contrary to most previous proposals that support a dynamic fault-model, the methodology is able to tolerate concave fault regions, thereby avoiding disabling healthy nodes in most practical scenarios. The methodology provides high network performance through the use of adaptive routing and provides graceful performance degradation in the presence of faults.},
  booktitle = {International Conference on High Performance Computing (HiPC)},
  editor = {Srinivas Aluru, Manish Parashar, Ramamurthy Badrinath, Viktor K. Prasanna},
  pages = {514--527},
  publisher = {Springer-Verlag},
  series = {LNCS 4873},
  isbn = {978-3-540-77219-4}
}

@Article{Simula.SC.24,
  author = {Nielsen, Bj{\o}rn Fredrik and Skavhaug, Ola and Tveito, Aslak},
  title = {Penalty methods for the numerical solution of American multi-asset option problems},
  year = {2008},
  abstract = {},
  journal = {Journal of Computational and Applied Mathematics},
  volume = {222},
  number = {1},
  pages = {3-16}
}

@Misc{Simula.SE.122,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {The impact of irrelevant information on inconsistency in expert judgment-based estimates of software development work},
  year = {2007},
  abstract = {Effort estimation is an important activity in software development and provides essential input to pricing, planning and budgeting processes. Unfortunately, many software effort estimates are inaccurate. The consequences of inaccurate estimates can be severe, e.g., budget-overruns, delayed time-to-market, and poor quality software. 

Most software effort estimation work is at least party based on expert judgment, i.e. non-mechanical and unconscious processes. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions sometimes lead to different effort estimates. We have conducted an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the sixty tasks were estimated twice. We found a surprisingly high degree of inconsistency in the software professionals{\textquoteright} effort. The mean difference of the effort estimates of the same task by the same estimator was as much 71\% (median 50\%) The correlation between the corresponding estimates was 0,7. Highly inconsistent effort estimates will, on average, be inaccurate and difficult to learn from. It is consequently important to focus estimation process improvement on consistency issues.

Evidence from other forecasting fields recommends that to reduce inconsistency, only the most important estimation information is used as input to the estimation work. We have empirically examined this advice{\textquoteright}s applicability to software effort estimation by analysing inter-estimator agreement in six software effort estimation experiments that report on the impact of including information of low or no relevance in the input to estimation work. The main findings are that inconsistency can increase when information of low or no relevance is present, and that this happens in spite of the software professionals knowing and accepting the lack of relevance. At present, the only safe way to remove the impact seems to be to remove the irrelevant information from the estimation material before it is given to the estimators. 
},
  howpublished = {International Symposium on Forecasting (ISF)}
}

@Article{Simula.SE.120,
  author = {Dzidek, James and Arisholm, Erik and Briand, Lionel Claude},
  title = {A Realistic Empirical Evaluation of the Costs and Benefits of UML in Software Maintenance},
  year = {2008},
  abstract = {The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54\% increase in the functional correctness of changes (p=0.03), an insignificant 7\% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56\%) - at the expense of an insignificant 14\% increase in development time caused by the overhead of updating the UML documentation (p=0.35).},
  journal = {IEEE Transaction on Software Engineering},
  volume = {34},
  number = {3},
  pages = {407-432}
}

@Misc{Simula.SE.119,
  author = {Karahasanovic, Amela},
  title = {Data collection and analysis for new usability },
  year = {2007},
  abstract = {},
  howpublished = {Invited talk at the Research methods workshop: Capturing user experience, Telenor R\&I, Oslo},
  note = {May 10th 2007, Oslo, FUSE, Telenor R\&I}
}

@Misc{Simula.SC.15,
  author = {Cai, Xing},
  title = {On building parallel algorithms and software for hydraulic tomography},
  year = {2007},
  abstract = {},
  howpublished = {Talk at SIAM GS2007 Conference, March 19-22, Santa Fe, New Mexico, USA}
}

@Misc{Simula.SC.14,
  author = {Cai, Xing},
  title = {Simulating tsunami propagation on parallel computers using a hybrid software framework},
  year = {2007},
  abstract = {},
  howpublished = {Guest lecture given at the University of Stuttgart, March 12}
}

@Article{Simula.ND.22,
  author = {Lysne, Olav and Montanana, Jose Miguel and Flich, Jose and Duato, Jose and Pinkston, Timothy Mark and Skeie, Tor},
  title = {An Efficient and Deadlock-free Network Reconfiguration Protocol},
  year = {2008},
  abstract = {Component failures and planned component replacements cause changes in the topology and routing paths  supplied by the interconnection network of a parallel processor system over time. Such changes may require  the network to be reconfigured such that the existing routing function is replaced by one which enables packets  to reach their intended destinations amid the changes. Efficient reconfiguration methods are desired that allow  the network to function uninterruptedly over the course of the reconfiguration process while remaining free  from deadlocking behavior. In this paper, we propose, evaluate, and prove deadlock freedom of a new network  reconfiguration protocol that overlaps various phases of {\textquotedblleft}static{\textquotedblright} reconfiguration processes traditionally used in  commercial and research systems to provide performance efficiency on par with that of recently proposed {\textquotedblleft}dynamic{\textquotedblright}  reconfiguration processes, but without their complexity. Simulation results show that the proposed Overlapping  Static Reconfiguration protocol can reduce reconfiguration time by up to 50\%, reduce packet latency by several  orders of magnitude, reduce packet dropping by an order of magnitude, and provide unhalted packet injection as  compared to traditional static reconfiguration while allowing network throughput similar to dynamic reconfiguration.},
  journal = {IEEE Transactions of Computers},
  volume = {57},
  number = {6},
  pages = {762-779}
}

@Article{Simula.ND.18,
  author = {Lysne, Olav and Reinemo, Sven-Arne and Skeie, Tor and Solheim, {\r A}shild Gr{\o}nstad and S{\o}dring, Thomas and Huse, Lars Paul and Johnsen, Bj{\o}rn Dag},
  title = {The Interconnection Network - Architectural Challenges for Utility Computing Data Centres},
  year = {2008},
  abstract = {The mode of operation employed by Computational Data Centres that offer Utility Computing differs significantly from that of traditional supercomputers and server clusters and as such present new architectural problems that should be studied and solved. In this paper we concentrate on issues facing the interconnection network. We argue that this is a part of the overall architecture where shortcomings in present day solutions are most severe and present a model for the mode of operation of a Utility Computing Data Centre where virtualisation is a main ingredient. Based on this model we identify several areas where the interconnection network faces new challenges and needs new solutions. In each of these areas we give a brief introduction to previous results before we identify the new challenges.},
  journal = {IEEE Computer},
  volume = {41},
  number = {9},
  pages = {62-69}
}

@Phdthesis{Simula.ND.26,
  author = {Kvalbein, Amund},
  title = {Fast network recovery},
  year = {2007},
  abstract = {The Internet is increasingly used to transport time-critical
traffic. Applications like video conferencing, television, telephony
and distributed games have strict requirements to the delay and
availability offered by the underlying network. At the same time,
connectivity failures caused by failures in network equipment is a
part of everyday operation in large communication systems.  The
traditional recovery mechanisms used in IP networks are not designed
with real-time applications in mind. The distributed nature of popular
intradomain routing protocols allows them to eventually recover from
any number of failures that leaves the network connected, but this is
a time consuming process that can lead to unacceptable performance
degradations for some applications.

In this work, we argue that there is a need for fast recovery
mechanisms that allow packet forwarding to continue over alternate
paths immediately after a failure, before the routing protocol has
converged on the altered topology. To give rapid response, such
mechanisms should be \emph{proactive} in the sense that an alternate
route is readily available when a failure is discovered, and
\emph{local}, so that the recovery action can be effected by the node
that discovers the failure. Further, care should be taken so that the
shifting of recovered traffic to an alternate route does not lead to
congestion and packet loss in other parts of the network.

We present and investigate mechanisms that can respond quickly to
failures or unexpected traffic shifts in the network. First, we
evaluate the recovery strategy used in a network protocol called
Resilient Packet Ring (RPR). The ring topology used in RPR allows the
implementation of very fast protection mechanisms. We look at the
performance of these mechanisms, and propose improvements that reduce
packet loss and shorten the experienced disruption time after a link
or node failure. Then, in the main part of this work, we focus on fast
recovery in general mesh networks. We present Resilient Routing
Layers (RRL) and Multiple Routing Configurations (MRC), which are
methods for near-instantaneous recovery from component failures in
packet networks. We discuss and evaluate our mechanisms with respect
to state requirements and distribution of the recovered traffic. For
MRC, we move on to present methods for reducing the chances of
congestion after a recovery operation. We show that if we have
knowledge about the traffic demands, we can use this information to
create MRC recovery paths that avoid the most heavily used parts of
the network. Finally, we show how the concepts used in RRL and MRC to
give recovery from component failures also can be used to avoid
congestion when there are sudden shifts in the traffic
distribution. Our method is more flexible than traditional traffic
engineering methods used in connectionless IP networks, since it does
not involve changing link weights to respond to a changed traffic
situation. 

Fast recovery mechanisms like those proposed in this work can help
improve the stability and availability of IP networks. This is
an important requirement for enabling new and existing real-time
applications over general-purpose Internet infrastructure.
},
  school = {University of Oslo}
}

@Inproceedings{Simula.ND.14,
  author = {Yu, R and Zhang, Y and Sun, Z and Mei, S},
  title = {Energy and QoS Aware Packet Forwarding in Wireless Sensor Networks},
  year = {2007},
  abstract = {},
  booktitle = { the IEEE International Conference on Communications (ICC), ICC 2007},
  editor = {IEEE},
  pages = {3277-3282},
  publisher = {IEEE},
  isbn = {1-4244-0353-7}
}

@Inproceedings{Simula.ND.12,
  author = {Zhang, Y and Yang, Laurence T and Ma, J and Zheng, J and Zhou, M and Xiao, S},
  title = {Quantitative Analysis of Location Management and QoS in Wireless Mobile Networks},
  year = {2007},
  abstract = {},
  booktitle = { the IEEE 21st International Conference on Advanced Information Networking and Applications (AINA-07)},
  editor = {IEEE},
  pages = { 573-579},
  publisher = {IEEE Computer Society Press},
  isbn = {0-7695-2846-5}
}

@Article{Simula.SE.24,
  author = {Briand, Lionel Claude and Labiche, Y and Cui, J},
  title = {Automated Support for Deriving Test Requirements from UML Statecharts},
  year = {2005},
  abstract = {},
  journal = {Journal of Software and Systems Modeling},
  volume = {4},
  number = {12},
  pages = {399-423}
}

@Inproceedings{Simula.SC.18,
  author = {Haga, Joachim Berdal and Bruaset, Are Magnus and Cai, Xing and Langtangen, Hans Petter and Osnes, Harald and Skogseid, Jakob},
  title = {Parallelisation and numerical performance of a 3D model for coupled deformation, fluid flow and heat transfer in sedimentary basins},
  year = {2007},
  abstract = {In this paper, we present some parallel performance results for a 3D simulator of coupled deformation, fluid flow and heat transfer in sedimentary basins. The model parameters are derived from an industry simulator, with realistic material properties and complex irregular grids of up to 1.5 million nodes with 7.3 million degrees of freedom. We have performed parallelisation on the linear algebra level using the ML algebraic multigrid preconditioner with iterative methods in the Diffpack finite element framework. Implementation and speedup results are presented.},
  booktitle = {MekIT'07},
  editor = {B. Skallerud and H. I. Andersson},
  pages = {151--162},
  publisher = {Tapir Academic Press},
  address = {Trondheim},
  isbn = {978-82-519-2235-7}
}

@Article{Simula.SE.22,
  author = {Briand, Lionel Claude and Labiche, Y and Di Penta, M and Yan-Bondoc, H D},
  title = {An Experimental Investigation of Formality in UML-Based Development},
  year = {2005},
  abstract = {},
  journal = {IEEE Transactions on Software Engineering},
  volume = {vol. 31},
  number = {nr. 10},
  pages = {p. 833-849}
}

@Inproceedings{Simula.ND.16,
  author = {Johnsen, Frank T and Hafs{\o}e, Trude and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Workload Characterization for News-on-Demand Streaming Services},
  year = {2007},
  abstract = {This paper focuses on design issues for multimedia distribution
architectures and the impact workload characteristics have on
architecture design. Our contribution is an analysis of server load
and user behavior in a News-on-Demand environment, with focus on
access patterns, popularity modeling, and the formation of traffic
peaks. Finally, we evaluate an existing synthetic workload generator,
MediSyn, and suggest some enhancements which will improve its
suitability for News-on-Demand workload modeling.
},
  booktitle = {International Performance Computing and Communications Conference (IPCCC)},
  editor = {?},
  pages = {314-323},
  publisher = {IEEE},
  isbn = {1-4244-1138-6}
}

@Article{Simula.ND.23,
  author = {Zheng, J and Zhang, Y and Wang, L and Chen, J},
  title = {Adaptive Location Update Area Design for Wireless Cellular Networks under 2D Markov Walk Model},
  year = {2007},
  abstract = {},
  journal = {Elsevier Computer Communications},
  volume = {30},
  number = {9},
  pages = {2060-2069}
}

@Misc{Simula.ND.27,
  author = {Hansen, Audun Fosselie and See Notes for complete author list},
  title = {﻿Proposal of recovery methods differentiation for networks with common control plane	 },
  year = {2007},
  abstract = {},
  howpublished = {Euro-Ngi Deliverable D.JRA.3.3.6, 6th Framework Programme, Project no: 507613},
  note = {﻿R{\"u}diger Martin, Michael Menth, Bjarne E. Helvik, Anders Mykkeltveit, Otto J. Wittner, Audun Fosselie Hansen, Andrzej Jajszczyk, Piotr Cho{\l}da and Zbigniew Hulicki 	 
}
}

@Misc{Simula.ND.28,
  author = {Hansen, Audun Fosselie and See notes for complete author list},
  title = {Conference and journal papers},
  year = {2007},
  abstract = {},
  howpublished = {Euro-Ngi Deliverable D.JRA.3.3.7, 6th Framework Programme, Project no: 507613},
  note = {﻿Ilkka Norros, Jorma Kilpi, Michael Menth, Bjarne Helvik, Anders Mykkeltveit, Audun Fosselie Hansen, Mateusz Dzida, Andrzej Jajszczy and Piotr Cho{\l}da 	 
}
}

@Inproceedings{Simula.ND.49,
  author = {Gjessing, Stein},
  title = {A novel method for re-routing in OBS networks},
  year = {2007},
  abstract = {An important challenge in Optical Burst Switching (OBS) is to reduce the burst loss caused by contention. In this paper we develop a new method that improves the burst loss probability by burst re-routing in case of contention. Our re-routing method guarantees that bursts (that are not discarded) are routed towards the egress on a loop-free path. We have made a detailed discrete event simulation model of OBS networks and evaluated the performance of our new method by comparing it to popular deflection methods. We simulate several realistic network topologies and find that, for realistic loads, re-routing indeed improves burst loss probability and has clear advantages when compared to the discussed deflection methods.},
  booktitle = {7th International Symposium on Communications and Information Technologies},
  editor = {Y Jay Guo},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  isbn = {1-4244-0977-2}
}

@Inproceedings{Simula.SE.164,
  author = {J{\o}rgensen, Magne},
  title = {A Critique of How We Measure and Interpret the Accuracy of Software Development Effort Estimation},
  year = {2007},
  abstract = {This paper criticizes current practice regarding the measurement and interpretation of the accuracy of software development effort estimation. The shortcomings we discuss are related to: 1) the meaning of {\textquoteleft}effort estimate{\textquoteright}, 2) the meaning of {\textquoteleft}estimation accuracy{\textquoteright}, 3) estimation of moving targets, and 4)  assessment of the estimation process, and not only the discrepancy between the estimated and the actual effort, to evaluate estimation skill. It is possible to correct several of the discussed shortcomings by better practice. However, there are also inherent problems related to both laboratory and field analyses of the accuracy of software development effort estimation. It is essential that both software researchers and professionals are aware of these problems and their implications for the analysis of the measurement of effort estimation accuracy.},
  booktitle = {1st International Workshop on Software Productivity Analysis and Cost Estimation},
  editor = {Jacky Keung},
  pages = {15-22},
  publisher = {Information Processing Society of Japan},
  isbn = {ISBN 978-4-915256-72-1 C3040}
}

@Inproceedings{Simula.ND.36,
  author = {Espeland, H{\r a}vard and Lunde, Carl Henrik and Stensland, H{\r a}kon Kvale and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Transparent Protocol Translation for Streaming},
  year = {2007},
  abstract = {The transport of streaming media data over TCP
is hindered by TCP's probing behavior that results in
the rapid reduction and slow recovery of the packet rates.
On the other side, UDP has been criticized for being unfair
against TCP connections, and it is
therefore often blocked out in the access networks.
In this paper, we try to benefit from a combined approach using a proxy
that transparently performs transport protocol translation.
We translate HTTP requests by the client transparently into RTSP requests,
and translate the corresponding RTP/UDP/AVP stream into the corresponding
HTTP response. This enables
the server to use UDP on the server side and TCP on the client side.
This is beneficial for the server side that scales to a higher load
when it doesn't have to deal with TCP.
On the client side, streaming over TCP has the advantage that
connections can be established from the client side, and data streams
are passed through firewalls.
Preliminary tests demonstrate that our protocol translation delivers a
smoother stream compared to HTTP-streaming where the
TCP bandwidth oscillates heavily.
},
  booktitle = {ACM International Multimedia Conference (ACM MM)},
  editor = {Rainer Lienhart and Anand R. Prasad},
  pages = {771 - 774},
  publisher = {ACM},
  isbn = {978-1-59593-702-5},
  note = {(short paper)
{\textcopyright} ACM, (2007). This is the author's version of the work. It is posted
here by permission of ACM for your personal use. Not for redistribution.
The definitive version was published in Proceedings of the 15th
international conference on Multimedia (2007), http://doi.acm.org/10.1145/1291233.1291407}
}

@Inproceedings{Simula.ND.35,
  author = {Harcsik, Szabolcs and Petlund, Andreas and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Latency Evaluation of Networking Mechanisms for  Game Traffic},
  year = {2007},
  abstract = {Large improvements in computer technology allow thousands of users to
concurrently interact in a virtual game. Due to this development, the
body of work analyzing game traffic has grown considerably in the
recent past. However, little work has been done to examine and
compare networking techniques with respect to meeting the stringent
latency requirements that are common for networked games. Most interactive
games need response times between 100 and 1000 ms depending on the
game genre.

In this paper, we evaluate different techniques for
delivering packets in a timely manner. In
particular, we compare existing user-space middleware running on top
of UDP and reliable, fair transport protocols like TCP and SCTP. In
addition, we evaluate some "low latency" extensions
to TCP, SCTP and one of the middleware platforms.
We present results concerning packet latency
and bandwidth requirements for the different approaches.
},
  booktitle = {Network \& System Support for Games (NetGames 2007)},
  editor = {Grenville Armitage},
  pages = {129--134},
  publisher = {ACM},
  isbn = { ISBN 978-0-9804460-0-5}
}

@Inproceedings{Simula.ND.25,
  author = {Solheim, {\r A}shild Gr{\o}nstad and Lysne, Olav and S{\o}dring, Thomas and Skeie, Tor and Libak, Jakob Aleksander},
  title = {Routing-Contained Virtualization based on Up*/Down* Forwarding},
  year = {2007},
  abstract = {Virtualization of computing resources is becoming increasingly important both in high-end servers and
in multi-core CPUs. In a virtualized system, the set of resources that constitute a virtual compute entity should be
spatially separated from each other. Dividing the cores on a chip, or the CPUs in a high end server into disjoint sets
for each task is a trivial problem. Ensuring that they use disjoint parts of the interconnection network is, however,
complex, and in existing methods the requirement of routing-containment of each virtual partition severely degrades
the utilization of the system. In this paper, we present an allocation strategy that is based on Up*/Down* routing.
Through simulations, we demonstrate increases (in some cases above 30\%) in system utilization relative to state-of-
the-art in a Dimension Order routed mesh - a topology that is assumed to be widely deployed in Networks on
Chip.},
  booktitle = {High Performance Computing - HiPC 2007},
  editor = {S. Aluru et al.},
  pages = {500--513},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  edition = {4873},
  series = {LNCS 4873},
  isbn = {978-3-540-77219-4}
}

@Inproceedings{ND.5.Horn.2003,
  author = {Horn, G and Lysne, O and Skeie, T},
  title = {A Criterion for Cost Optimal Construction of Irregular Networks},
  year = {2003},
  abstract = {},
  booktitle = {IPDPS'03: 17th International Parallel and},
  editor = {Michel Cosnard and Allan Gottlieb and Jack},
  publisher = {IEEE},
  address = {Nice, France},
  note = {Workshop on Communication Architecture for
                   Clusters (D. K. Panda, J. Duato and C.
                   Stunkel eds.)}
}

@Misc{ND.8.Gjessing.2001.d,
  author = {Gjessing, S},
  title = {A Fairness Algorithm for Dynamic Spatial Reuse Avoiding HOL Blocking},
  year = {2001},
  abstract = {},
  howpublished = {Presentation at the September 2001 IEEE 802.17
                   interim meeting}
}

@Inproceedings{ND.5.Gjessing.2002.b,
  author = {Gjessing, S and Maus, A},
  title = {A Fairness Algorithm for High-speed Networks based on a Resilient Packet Ring Architecture},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of 2002 IEEE International
                   Conference on Systems, Man and Cybernetics},
  pages = {279-284},
  address = {Hammamet, Tunesia}
}

@Techreport{ND.7.Lysne.2003,
  author = {Lysne, O and Duato, J and Pinkston, T},
  title = {A Methodology for Developing Deadlock-Free Dynamic Network Reconfiguration Proceses},
  year = {2003},
  abstract = {},
  institution = {Simula Research Laboratory},
  number = {10}
}

@Inproceedings{ND.5.Lysne.2003,
  author = {Lysne, O and Pinkston, T and Duato, J},
  title = {A Methodology for Developing Dynamic Network Reconfiguration Processes},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the 2003 International Conference
                   on Parallel Processing},
  pages = {77-86},
  publisher = {IEEE Computer Society}
}

@Inproceedings{ND.5.Fongen.2002,
  author = {Fongen, A and Eliassen, F and Ferguson, I and Stobart, S and Tait, J},
  title = {A Scalable and Fault-tolerant Architecture for Distributed Web Resource Discovery},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of Fourteenth IASTED International
                   Conference on Parallel and Distributed
                   Computing and Systems},
  address = {Cambridge, USA}
}

@Inproceedings{ND.5.Reinemo.2003.b,
  author = {Reinemo, S -A and Sem-Jacobsen, F O and Skeie, T and Lysne, O},
  title = {Admission Control for DiffServ based Quality of Service in Cut-through Networks},
  year = {2003},
  abstract = {Previous work on Quality of Service in Cut-through networks shows that resource reservation mechanisms are only effective below the saturation point. Admission control in these networks will therefore need to keep network utilization below the saturation point, while still utilising the network resources to the maximum extent possible. In this paper we propose and evaluate three admission control schemes. Two of these use a centralised bandwidth broker, while the third is a distributed measurement based approach. We combine these admission control schemes with a DiffServ based QoS scheme for virtual cut-through networks to achieve class based bandwidth and latency guarantees. Our simulations show that the measurement based approach favoured in the Internet communities performs poorly in cut-through networks. Furthermore it demonstrates that detailed knowledge on link utilization improves performance significantly.},
  booktitle = {Proceedings of the 10th International Conference on High Performance Computing (HiPC 2003)},
  editor = {Timothy Mark Pinkston and Viktor K. Prasanna},
  pages = {118-129},
  publisher = {Springer},
  address = {Heidelberg},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{ND.5.Cicic.2001.b,
  author = {Cicic, Tarik and Gjessing, Stein and Kure, {\O}ivind},
  title = {An Improved PIM-SM Tree Recovery Algorithm},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings IEEE Workshop in High                   Performance Switching and Routing},
  publisher = {IEEE},
  address = {Dallas, Texas}
}

@Inproceedings{ND.5.Reinemo.2003.a,
  author = {Reinemo, S -A and Skeie, T and Lysne, O},
  title = {Applying the DiffServ Model on Cut-through Networks},
  year = {2003},
  abstract = {Understanding the nature of traffic in high-speed communication systems is essential for achieving QoS in these networks. A first step towards this goal is understanding how basic QoS mechanisms work and affects the network predictability before we introduce more complex mechanisms such as admission control. In this paper we analyse the effect of a DiffServ inspired QoS concept applied to virtual cut-through networks. The main findings from our study are that (i) throughput differentiation can be achieved by weighting of virtual lanes (VL) and by classifying VLs as either low or high priority, (ii) the balance between VL weighting and VL load is not crucial when the network is operating below the saturation point, (iii) jitter, however, is large and good jitter characteristics seems unachievable with such a relative scheme.},
  booktitle = {Proceedings of the 2003 International Conference
                   on Parallel and Distributed Processing
                   Techniques and Applications (PDPTA2003)},
  editor = {H.R. Arabnia and Youngsong Mun},
  pages = {1089-1095},
  publisher = {CSREA Press},
  address = {Las Vegas, Nevada}
}

@Inproceedings{ND.5.Granmo.2002.b,
  author = {Granmo, O C},
  title = {Automatic Resource-aware Construction of Media Indexing Applications for Distributed Processing Environments},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the 2nd International Workshop on
                   Pattern Recognition in Information Systems
                   (PRIS)},
  pages = {124-139},
  address = {Alicante, Spain}
}

@Inproceedings{ND.5.Gjessing.2002.a,
  author = {Gjessing, S and Davik, F},
  title = {Avoiding Head-of-Line Blocking using an Enhanced Fairness Algorithm},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the International Conference on                   Telecommunication (ICT 2002), Paper C089, 6                   pages},
  publisher = { },
  address = {Beijing, China}
}

@Misc{ND.8.Gjessing.2001.a,
  author = {Gjessing, S},
  title = {C++ and Java: A Comparison and an Evaluation},
  year = {2001},
  abstract = {},
  howpublished = {Technologies for component based, distributed systems
                   workshop},
  note = {Talk (in Norwegian), Norwegian Computer Society}
}

@Inproceedings{ND.5.Rafaelsen.2002,
  author = {Rafaelsen, H O and Eliassen, F},
  title = {Design and Performance of a Media Gateway Trader},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of Distributed Objects and
                   Applications (DOA'02), IEEE},
  address = {Irvine, CA, USA}
}

@Inproceedings{ND.5.Fongen.2001,
  author = {Fongen, A and Eliassen, F and Ferguson, I and Stobart, S and Tait, J},
  title = {Distributed Resource Recovery Using a Context-Sensitive Infrastructure},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of Third International Conference
                   on Information Integration and Web-based
                   Applications \& Services (iiWAS2001)},
  address = {Vienna, Austria}
}

@Inproceedings{ND.5.Granmo.2001,
  author = {Granmo, O C and Eliassen, F and Lysne, O},
  title = {Dynamic Object-Oriented Bayesian Networks for Flexible Resource-Aware Content-based Indexing of Media Streams},
  year = {2001},
  abstract = {},
  booktitle = {12th Scandinavian Conference on Image Analysis
                   (SCIA2001)},
  address = {Bergen}
}

@Article{ND.4.Skeie.2002,
  author = {Skeie, T and Johannessen, S},
  title = {Ethernet in Substation Automation},
  year = {2002},
  abstract = {},
  journal = {IEEE Control Systems Magazine},
  volume = {22},
  number = {3},
  pages = {43-51}
}

@Misc{ND.8.Gjessing.2001.e,
  author = {Gjessing, S},
  title = {Evaluation of an Enhanced Fairness Algorithm that Avoids HOL Blocking},
  year = {2001},
  abstract = {},
  howpublished = {Presentation at the September 2001 IEEE 802.17
                   interim meeting}
}

@Inproceedings{ND.5.Skeie.2002.c,
  author = {Skeie, T and Theiss, I and Lysne, O},
  title = {Evaluation of Minimal Deterministic Routing in Irregular Networks},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the SSGRR International
                   Conference on Infrastructure for e-Business,
                   e-education, e-Science, and e-Medicine (SSGRR)}
}

@Techreport{ND.7.Eide.2003,
  author = {Eide, V SW and Eliassen, F and Lysne, O and Granmo, O C},
  title = {Extending Content-based Publish/Subscribe Systems with Multicast Support},
  year = {2003},
  abstract = {},
  institution = {Simula Research Laboratory},
  number = {2003-03}
}

@Misc{ND.8.Gjessing.2001.f,
  author = {Gjessing, S},
  title = {Flow Control algorithms revisited},
  year = {2001},
  abstract = {},
  howpublished = {Presentation at the November 2001 IEEE 802.17
                   plenary meeting}
}

@Inproceedings{ND.5.Theiss.2003,
  author = {Theiss, I and Lysne, O},
  title = {FROOTS - Fault handling in Up*/Down* routed networks with multiple roots},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the International Conference on
                   High Performance Computing HiPC},
  publisher = {Springer-Verlag}
}

@Inproceedings{ND.5.Skeie.2001,
  author = {Skeie, T and Johannessen, S and Holmeide, {\O}},
  title = {Highly Accurate Time Synchronization over Switched Ethernet},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of 8th IEEE conference on Emerging
                   Technologies and Factory Automation (ETFA)}
}

@Techreport{ND.7.Davik.2003,
  author = {Davik, F and Yilmaz, M and Gjessing, S and Uzun, N},
  title = {IEEE 802.17 Resilient Packet Ring Background and Overview},
  year = {2003},
  abstract = {The IEEE Working group P802.17 is standardizing a new ring topology network architecture, called the Resilient Packet Ring (RPR), to be used mainly in metropolitan and wide area networks. This paper presents a technology background, gives an overview, and explains many of the design choices the RPR working group faced during the development of the standard. Some major architectural features are illustrated and compared by showing performance evaluation results using the RPR simulator developed at Simula Research Laboratory using the OPNET Modeler simulation environment.},
  institution = {Simula Research Laboratory},
  number = {2003-11}
}

@Inproceedings{ND.5.Gjessing.2002,
  author = {Gjessing, S and Davik, F},
  title = {Improved Fairness and Class of Service Behaviour in a Resilient Packet Ring},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of 12th IEEE Workshop on Local and
                   Metropolitan Area Networks (LANMAN 2002)},
  address = {Stockholm, Sweden}
}

@Misc{ND.8.Engelsaastroe.2002,
  author = {Engels{\r a}str{\o}, M and Ingvaldsen, T and Gjessing, S},
  title = {IP Multicast: Id{\a\'e} som forenkler innholdsdistribusjon},
  year = {2002},
  abstract = {},
  howpublished = {Telenor ToU magasinet}
}

@Inproceedings{ND.5.Skeie.2002.a,
  author = {Skeie, T and Lysne, O and Theiss, I},
  title = {Layered Shortest Path (LASH) Routing in Irregular System Area Networks},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of Communication Architecture for
                   Clusters (CAC'02)},
  publisher = {IEEE Computer Society}
}

@Misc{ND.8.Gjessing.2002.a,
  author = {Gjessing, S},
  title = {Lessons Learned from Java Simulations},
  year = {2002},
  abstract = {},
  howpublished = {Presentation at the January 2002 IEEE 802.17 interim
                   meeting}
}

@Inproceedings{ND.5.Lysne.2001,
  author = {Lysne, O and Skeie, T},
  title = {Load balancing of irregular system area networks through multiple roots},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of the International Conference on
                   Communication in Computing, CIC 2001},
  pages = {165 -171},
  publisher = {CSREA Press}
}

@Inproceedings{ND.5.Theiss.2002,
  author = {Theiss, I and Skeie, T and Lysne, O},
  title = {Minimal Routing in Irregular Networks},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the International Conference on
                   Communications in Computing (CIC)},
  publisher = {CSREA Press}
}

@Misc{ND.2.Plagemann.2001,
  author = {Plagemann, T and Eliassen, F},
  title = {Multimedia Middleware},
  year = {2001},
  abstract = {},
  howpublished = {Misc},
  note = {ISBN 1-58113-396-0}
}

@Misc{ND.8.Plagemann.2001.a,
  author = {Plagemann, T and Eliassen, F},
  title = {Multimedia Middleware},
  year = {2001},
  abstract = {},
  howpublished = {Tutorial at ACM Multimedia 2001, Ottawa,
                   Canada}
}

@Misc{ND.8.Plagemann.2001.b,
  author = {Plagemann, T and Eliassen, F},
  title = {Multimedia Middleware},
  year = {2001},
  abstract = {},
  howpublished = {Tutorial at IFIP DAIS 2001, Cracow,
                   Poland}
}

@Misc{ND.8.Plagemann.2001.c,
  author = {Plagemann, T and Eliassen, F},
  title = {Multimedia Middleware},
  year = {2001},
  abstract = {},
  howpublished = {Tutorial at PROMS 2001, Twente, The
                   Netherlands}
}

@Article{ND.4.Krogdahl.2002,
  author = {Krogdahl, S and Lysne, O},
  title = {On verification of parallel message-passing processes},
  year = {2002},
  abstract = {},
  journal = {Formal Aspects of Computing},
  volume = {13},
  number = {6},
  pages = {471-492}
}

@Inproceedings{ND.5.Cicic.2001.a,
  author = {Cicic, Tarik and Gjessing, Stein and Kure, {\O}ivind},
  title = {Performace Evaluation of PIM-SM Recovery},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings IEEE International Conference on                   Networking, ICN{\textasciiacute}01},
  publisher = {Springer-Verlag},
  address = {Colmar, France}
}

@Inproceedings{ND.5.Gjessing.2002.c,
  author = {Gjessing, S and Davik, F},
  title = {Performance Evaluation of Back-Pressure Fairness in RPR},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the 7th European Conference on                   Networks \& Optical Communications},
  pages = {293-300},
  publisher = {a},
  address = {Darmstadt, Germany}
}

@Misc{ND.8.Gjessing.2001.b,
  author = {Gjessing, S},
  title = {Preliminary performance results from a simple Java model},
  year = {2001},
  abstract = {},
  howpublished = {Presentation at the May 2001 IEEE 802.17 interime
                   meeting}
}

@Article{ND.4.Eliassen.2002,
  author = {Eliassen, F and Plagemann, T and Hafskjold, B and Kristensen, T and Rafaelsen, H O and Macdonald, R H},
  title = {QoS management in the MULTE-ORB},
  year = {2002},
  abstract = {},
  journal = {IEEE Distributed System Online},
  volume = {(Middleware Archives)}
}

@Inproceedings{ND.5.Solberg.2003,
  author = {Solberg, A and Husa, K E and Aagedal, J {\O} and Abrahamsen, E},
  title = {QoS-aware MDA},
  year = {2003},
  abstract = {},
  booktitle = {Electronic publication from Model Driven
                   Architecture in the Specification, Implementation
                   and Validation of Object-oriented Embedded
                   Systems (SIVOES-MDA'2003) in conjunction with
                   UML'2003}
}

@Inproceedings{ND.5.Staehli.2003.b,
  author = {Staehli, R and Eliassen, F and Blair, G and Aagedal, J {\O}},
  title = {QuA: a QoS-Aware Component Architecture},
  year = {2003},
  abstract = {},
  booktitle = {Poster Session, Middleware 2003},
  pages = {330-330},
  publisher = {Pontificia Universidade Cat{\a\'o}lica do Rio de                   Janeiro},
  address = {Rio de Janerio}
}

@Techreport{ND.7.Staehli.2002,
  author = {Staehli, R and Eliassen, F},
  title = {QuA: A QoS-Aware Component Architecture},
  year = {2002},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2002-12}
}

@Inproceedings{ND.5.Staehli.2003.a,
  author = {Staehli, R and Eliassen, F and Aagedal, J {\O} and Blair, G},
  title = {Quality of Service Semantics for Component-Based Systems},
  year = {2003},
  abstract = {},
  booktitle = {Middleware 2003 Companion, Workshop                   Proceedings, 2nd International Workshop on                   Reflective and Adaptive Middleware Systems},
  pages = {153-157},
  publisher = {Pontificia Universidade Cat{\a\'o}lica do Rio de                   Janeiro},
  address = {Rio de Janerio}
}

@Inproceedings{ND.5.Granmo.2002.a,
  author = {Granmo, O C and Jensen, F V},
  title = {Real-time Hypothesis Driven Feature Extraction on Parallel Processing Architectures},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the 2002 International Conference
                   on Parallel and Distributed Processing
                   Techniques and Applications (PDPTA'02)},
  address = {Las Vegas, USA}
}

@Inproceedings{ND.5.Eide.2002.a,
  author = {Eide, V SW and Eliassen, F and Lysne, O and Granmo, O C},
  title = {Real-time Processing of Media Streams: A Case for Event-based Interaction},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of 1st International Workshop on
                   Distributed Event-Based Systems (DEBS'02),
                   IEEE},
  pages = {555-562},
  address = {Vienna, Austria},
  note = {ISBN 0-7695-1588-6}
}

@Misc{ND.8.Gjessing.2002.b,
  author = {Gjessing, S},
  title = {RPR's worst case scenario?},
  year = {2002},
  abstract = {},
  howpublished = {Presentation at the July 2002 IEEE 802.17 plenary
                   meeting}
}

@Inproceedings{ND.5.Eide.2002.b,
  author = {Eide, V SW and Eliassen, F and Granmo, O C and Lysne, O},
  title = {Scalable Independent Multi-level Distribution in Multimedia Content Analysis},
  year = {2002},
  abstract = {},
  booktitle = {Joint International Workshops on Interactive
                   Distributed Multimedia Systems and Protocols
                   for Multimedia Systems (IDMS/PROMS 2002)},
  pages = {37-48},
  publisher = {Springer-Verlag},
  address = {Coimbra, Portugal},
  series = {Lecture Notes in Computer Science},
  note = {ISBN 3-540-00169-7}
}

@Inproceedings{ND.5.Nordbotten.2003.a,
  author = {Nordbotten, N A and Skeie, T and Aakvaag, N D},
  title = {Service Discovery in Bluetooth Scatternets},
  year = {2003},
  abstract = {The article covers methods for service discovery in multi hop Bluetooth ad hoc networks, so called scatternets. A service discovery algorithm is presented that can be used to extend the Bluetooth Service Discovery Protocol to the scatternet without the use of broadcast messages. Extensive simulation results are presented showing that significant reductions in the number of messages sent are achieved compared to using a broadcast approach. Avoiding network flooding and reducing the number of messages is important as many Bluetooth devices have limited power sources and therefore benefit from keeping links idle in power saving modes.},
  booktitle = {Proc. Workshop on Mobile Ad Hoc Networking},
  pages = {69-74},
  publisher = { Institut EURECOM},
  address = {Sophia-Antipolis, France}
}

@Inproceedings{ND.5.Nordbotten.2003.b,
  author = {Nordbotten, N A and Skeie, T and Aakvaag, N D},
  title = {Service Discovery in Highly Dynamic Scatternets},
  year = {2003},
  abstract = {This paper covers methods for service discovery in multi-hop Bluetooth ad hoc networks, so called scatternets. An efficient service discovery protocol suitable for highly dynamic scatternets is proposed. Extensive simulation results are presented showing that the protocol significantly reduces network traffic. Reducing the network traffic is important as many Bluetooth devices have limited power sources and therefore benefit from keeping links idle in power saving modes. Finally it is explained how the proposed protocol can interact with reactive routing protocols and effectively assist route discovery.},
  booktitle = {Proceedings of the 3rd IEEE Workshop on Applications and Services in Wireless Networks},
  pages = {211-220},
  publisher = {University of Bern},
  address = {Bern, Switzerland}
}

@Misc{ND.8.Eliassen.2003.c,
  author = {Eliassen, F},
  title = {Simula Research Laboratory: Research and Opportunities},
  year = {2003},
  abstract = {},
  howpublished = {Invited talk at Lancaster University, UK}
}

@Inproceedings{ND.5.Eide.2001,
  author = {Eide, V SW and Eliassen, F and Lysne, O},
  title = {Supporting Distributed Processing of Time-based Media Streams},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of Distributed Objects and
                   Applications (DOA'01), IEEE},
  pages = {281-288},
  address = {Rome, Italy}
}

@Inproceedings{ND.5.Eide.2003,
  author = {Eide, V S. Wold and Eliassen, F and Granmo, O C and Lysne, O},
  title = {Supporting Timeliness and Accuracy in Real-time Content-based Video Analysis},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the 11th ACM International
                   Conference on Multimedia},
  pages = {21-32},
  publisher = {ACM},
  address = {Berkeley, California, USA}
}

@Inproceedings{ND.5.Granmo.2003,
  author = {Granmo, O C and Eliassen, F and Lysne, O and Eide, V SW},
  title = {Techniques for Parallel Execution of the Particle Filter},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of Scandinavian Conference on Image
                   Analysis (SCIA 2003)},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science}
}

@Misc{ND.8.Eliassen.2003.a,
  author = {Eliassen, F},
  title = {The QuA project: QoS-Driven Service Planning in an Open Component Architecture},
  year = {2003},
  abstract = {},
  howpublished = {Invited talk University of Twente, The
                   Netherlands}
}

@Misc{ND.8.Eliassen.2003.b,
  author = {Eliassen, F},
  title = {The QuA project: QoS-Driven Service Planning in an Open Component Architecture},
  year = {2003},
  abstract = {},
  howpublished = {Invited talk at Oregon Graduate Institute,
                   Oregon, USA}
}

@Inproceedings{ND.5.Skeie.2002.b,
  author = {Skeie, T and Johannessen, S and Holmeide, {\O}},
  title = {The Road to an End-to-End Deterministic Ethernet},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of 4th IEEE International Workshop
                   on Factory Communication Systems (WFCS)}
}

@Techreport{ND.7.Gjessing.2003,
  author = {Gjessing, S},
  title = {The Simula RPR Simulator implemented in Java},
  year = {2003},
  abstract = {},
  institution = {Simula Research Laboratory},
  number = {2003-12}
}

@Misc{ND.8.Gjessing.2003.c,
  author = {Gjessing, S and Lib{\ae}k, B and Teigen, P},
  title = {The Simula RPR simulator written i Java},
  year = {2003},
  abstract = {},
  howpublished = {Presentation at the July 2003 IEEE 802.17 plenary
                   meeting}
}

@Inproceedings{ND.5.Pinkston.2002,
  author = {Pinkston, T and Duato, J and Lysne, O and Pang, R},
  title = {Theoretical Support for Dynamic Network Reconfiguration},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the ICS Workshop on
                   Self-Healing, Adaptive and Self-Managed
                   Systems (SHAMAN)},
  publisher = {ACM Press}
}

@Inproceedings{ND.5.Reinemo.2001,
  author = {Reinemo, S -A and Dobinson, B and Haas, S and Lysne, O and Martin, B and Skeie, T},
  title = {Topologies and Routing in Gigabit Switching Fabrics},
  year = {2001},
  abstract = {Cluster networks will serve as the future access networks for multimedia streaming, massive multiplayer online gaming, e-commerce, network storage etc. And for those application areas provisioning of Quality of Service (QoS) is becoming and important issue. DiffServ as specified by the IETF is foreseen to be the most prominent concept for providing predictability in the future Internet. To enable seamless interoperation with the higher level IETF concepts the QoS architecture of the lower layers should comply with the DiffServ paradigm as well. Previous work on predictability in cut-through networks has only studied class based QoS. In this paper we set out to achieve flow level QoS using flow aware admission control in combination with a flow negligent DiffServ inspired QoS mechanism. Our results show that flow level bandwidth guarantees are achievable with the use of the Link-by-Link and the Probe based schemes. In addition we are able to achieve an order of magnitude improvement in jitter and latency in individual flows.},
  booktitle = {Proceedings of 2nd International Conference on
                   Communications in Computing (CIC2001)},
  editor = {Brian J. d'Auriol, Abdullah Abonamah  and Abdel-Elah Al-Ayyoub},
  pages = {142-149},
  publisher = {CSREA Press},
  address = {Las Vegas, Nevada}
}

@Techreport{ND.7.Cicic.2001,
  author = {Cicic, Tarik and Gjessing, Stein and Kure, {\O}ivind},
  title = {Topology Construction for Inter-Domain Network Protocol Simulations},
  year = {2001},
  abstract = {},
  institution = {Department of Informatics, University of Oslo},
  type = {Research Report},
  number = {296},
  note = {ISBN 82-7368-246-3}
}

@Inproceedings{ND.5.Rafaelsen.2001.b,
  author = {Rafaelsen, H O and Eliassen, F},
  title = {Towards support for ad-hoc multimedia bindings},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of Multimedia Middleware
                   (M3W'01), ACM},
  pages = {36-39},
  address = {Ottawa, Canada}
}

@Inproceedings{ND.5.Rafaelsen.2001.a,
  author = {Rafaelsen, H O and Eliassen, F},
  title = {Trading Media Gateways with CORBA Trader},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of Distributed Objects and
                   Applications (DOA'01), IEEE},
  pages = {115-124},
  address = {Rome, Italy}
}

@Article{ND.4.Cicic.2002,
  author = {Cicic, Tarik and Gjessing, Stein and Kure, {\O}ivind},
  title = {Tree Recovery in PIM Sparse Mode},
  year = {2002},
  abstract = {},
  journal = {Telecommunication Systems, Modeling, Analysis,                   Design and Management},
  volume = {19},
  number = {3-4},
  pages = {443-460}
}

@Inproceedings{ND.5.Manzke.2001,
  author = {Manzke, M and Kenny, S and Coghlan, B and Lysne, O},
  title = {Tuning and Verification of Simulation Models for High Speed Interconnect Fabrics},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of PDPTA},
  address = {Las Vegas (USA)}
}

@Phdthesis{ND.3.Theiss.2004,
  author = {Theiss, I},
  title = {Modularity, Routing and Fault Tolerance in Interconnection Networks},
  year = {2004},
  abstract = {},
  school = {University of Oslo},
  note = {ISSN 1501-7710, Nr. 351}
}

@Phdthesis{ND.3.Granmo.2004,
  author = {Granmo, O C},
  title = {Toward Controlling Accuracy and Timelines in Video Content Analysis},
  year = {2004},
  abstract = {},
  school = {Department of Informatics, University of Oslo}
}

@Article{ND.4.Gomez.2004,
  author = {G{\a\'o}mez, M E and Duato, J and Flich, J and Lopez, P and Robles, A and Nordbotten, N A and Lysne, O and Skeie, T},
  title = {An Efficient Fault-Tolerant Routing Methodology for Meshes and Tori},
  year = {2004},
  abstract = {In this paper we present a methodology to design fault-tolerant routing algorithms for regular direct interconnection networks. It supports fully adaptive routing, does not degrade performance in the absence of faults, and supports a reasonably large number of faults without significantly degrading performance. The methodology is mainly based on the selection of an intermediate node (if needed) for each source-destination pair. Packets are adaptively routed to the intermediate node and, at this node, without being ejected, they are adaptively forwarded to their destinations. In order to allow deadlock-free minimal adaptive routing, the methodology requires only one additional virtual channel (for a total of three), even for tori. Evaluation results for a 4x4x4 torus network show that the methodology is 5-fault tolerant. Indeed, for up to 14 link failures, the percentage of fault combinations supported is higher than 99.96\%. Additionally, network throughput degrades by less than 10\% when injecting three random link faults without disabling any node. In contrast, a mechanism similar to the one proposed in the BlueGene/L, that disables some network planes, would strongly degrade network throughput by 79\%.},
  journal = {Computer Architecture Letters},
  volume = {3}
}

@Article{ND.4.Davik.2004,
  author = {Davik, F and Yilmaz, M and Gjessing, S and Uzun, N},
  title = {IEEE 802.17 Resilient Packet Ring Tutorial},
  year = {2004},
  abstract = {},
  journal = {IEEE Communications Magazine},
  volume = {42},
  number = {3},
  pages = {112--118}
}

@Article{ND.4.Nordbotten.2004,
  author = {Nordbotten, N A and Skeie, T and Aakvaag, N D},
  title = {Methods for Service Discovery in Bluetooth Scatternets},
  year = {2004},
  abstract = {This paper presents methods for service discovery in multi-hop Bluetooth ad hoc networks, so called scatternets. Two service discovery protocols based on filtering of service requests are proposed. Extensive simulation results are presented showing that the protocols significantly reduce network traffic. Reducing the network traffic is important as many Bluetooth devices have limited power sources and, therefore, benefit from keeping links idle in power saving modes. It is also explained how the proposed protocols can interact with reactive routing protocols and effectively assist route discovery. Finally, an implementation providing functionality for both searching and browsing for services is suggested, effectively extending the Bluetooth SDP to the scatternet.},
  journal = {Computer Communications},
  volume = {27},
  number = {11},
  pages = {1087--1096}
}

@Inproceedings{ND.5.Solberg.2004.c,
  author = {Solberg, A and Amundsen, S and Aagedal, J {\O} and Eliassen, F},
  title = {A Framework for QoS-Aware Service Composition},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of ACM International                      Conference on Service Oriented Computing (ICSOC)},
  publisher = {ACM Press}
}

@Inproceedings{ND.5.Theiss.2004,
  author = {Theiss, I and Lysne, O},
  title = {LORE - Local Reconfiguration for Fault Management in Irregular Interconnects},
  year = {2004},
  abstract = {},
  booktitle = {International Parallel and Distributed
                      Processing Symposium},
  publisher = {IEEE Computer Society Press}
}

@Techreport{ND.7.Hansen.2004,
  author = {Hansen, A F and Cicic, T and Gjessing, S and Lysne, O},
  title = {Resilient Routing Layers: A Simple and Flexible Approach for Resilience in Packet Networks},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-13},
  note = {This report has later been updated and refined as "Resilient Routing Layers for Recovery in Packet Networks".}
}

@Inproceedings{ND.5.Davik.2004,
  author = {Davik, F and Gjessing, S},
  title = {The Stability of the Resilient Packet Ring Aggressive Fairness Algorithm},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the 13th IEEE Workshop on Local and                      Metropolitan Area Networks (LANMAN2004)},
  pages = {17--22},
  publisher = {IEEE Computer Society Press},
  address = {Mill Valley, California, USA}
}

@Inproceedings{ND.5.Granmo.2004,
  author = {Granmo, O C},
  title = {Exploiting Content-Based Networking for Fine Granularity Video Streaming},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of ACM Symposium on Applied
                      Computing (SAC)},
  publisher = {ACM Press},
  address = {Nicosia, Cypros}
}

@Inproceedings{ND.5.Skeie.2004,
  author = {Skeie, T and Lysne, O and Flich, J and Lopez, P and Robles, A and Duato, J},
  title = {LASH-TOR: A Generic Transition-Oriented Routing Algorithm},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the IEEE International                      Conference on Parallel and Distributed Systems                      (ICPADS)},
  pages = {595--604},
  publisher = {IEEE Computer Society Press}
}

@Inproceedings{ND.5.Gomez.2004,
  author = {G{\a\'o}mez, M E and Flich, J and Lopez, P and Robles, A and Duato, J and Nordbotten, N A and Lysne, O and Skeie, T},
  title = {An Effective Fault-Tolerant Routing Methodology for Direct Networks},
  year = {2004},
  abstract = {Current massively parallel computing systems are being built with thousands of nodes, which significantly affect the probability of failure. M. E. Gomex proposed a methodology to design fault-tolerant routing algorithms for direct interconnection networks. The methodology uses a simple mechanism: for some source-destination pairs, packets are first forwarded to an intermediate node, and later, from this node to the destination node. Minimal adaptive routing is used along both subpaths. For those cases where the methodology cannot find a suitable intermediate node, it combines the use of intermediate nodes with two additional mechanisms: disabling adaptive routing and using misrouting on a per-packet basis. While the combination of these three mechanisms tolerates a large number of faults, each one requires adding some hardware support in the network and also introduces some overhead. In this paper, we perform an in-depth detailed analysis of the impact of these mechanisms on network behaviour. We analyze the impact of the three mechanisms separately and combined. The ultimate goal of this paper is to obtain a suitable combination of mechanisms that is able to meet the trade-off between fault-tolerance degree, routing complexity, and performance.},
  booktitle = {Proceedings of International Conference on Parallel Processing},
  pages = {222--231},
  publisher = {IEEE Computer Society Press}
}

@Inproceedings{ND.5.Staehli.2004.b,
  author = {Staehli, R and Eliassen, F},
  title = {Compositional Quality of Service Semantics},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of Specification and Verifcation of                      Component Based Systems (SAVCBS04),                      Workshop at ACM SIGSOFT 2004},
  pages = {62--69},
  publisher = {Iowa State University}
}

@Inproceedings{ND.5.Eliassen.2004.b,
  author = {Eliassen, F and Staehli, R and Blair, G and Aagedal, J {\O}},
  title = {QuA: Building with Reusable QoS-Aware Components},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of OOPSLA 2004 (Extended abstract and poster)},
  pages = {154--155},
  publisher = {ACM Press}
}

@Inbook{ND.5.Skeie.2004.b,
  author = {Skeie, T and Johannessen, S and Holmeide, {\O}},
  title = {Switched Ethernet in Automation Networking},
  year = {2004},
  abstract = {},
  booktitle = {The Handbook on Information Technology in
                      Industrial Automation},
  publisher = {CRC Press},
  pages = {49-1--49-15},
  note = {ISBN 0-8493-1985-4}
}

@Inproceedings{ND.5.Amundsen.2004,
  author = {Amundsen, S and Lund, K and Eliassen, F and Staehli, R},
  title = {QuA: Platform-Managed QoS for Components Architectures},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of Norwegian Informatics                      Conference (NIK)},
  publisher = {Tapir}
}

@Misc{ND.8.Musunoori.2004.c,
  author = {Musunoori, S B and Eliassen, F and Staehli, R},
  title = {The Next Generation Global Technology - GRID: A QoS-aware Component-Based Middleware Approach},
  year = {2004},
  abstract = {},
  howpublished = {Misc},
  note = {Presented at the eVITA Workshop on
                      eScience and Applications}
}

@Inbook{ND.5.Aagedal.2004,
  author = {Aagedal, J {\O} and Bezivin, J and Linington, P F},
  title = {Model Driven Development},
  year = {2004},
  abstract = {},
  booktitle = {ECOOP 2004 workhops},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science},
  pages = {148--157}
}

@Inproceedings{ND.5.Solberg.2004.b,
  author = {Solberg, A and Oldevik, J and Aagedal, J {\O}},
  title = {QoS-aware Model Transformation: A Pattern-based Approach},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of International
                      Conference on Distributed Objects and Applications
                      (DOA)}
}

@Inproceedings{ND.5.Sem-Jacobsen.2004,
  author = {Sem-Jacobsen, F O and Reinemo, Sven-Arne and Skeie, T and Lysne, O},
  title = {Achieving Flow Level QoS in Cut-through Networks through Admission Control and DiffServ},
  year = {2004},
  abstract = {Cluster networks will serve as the future access networks for multimedia streaming, massive multiplayer online gaming, e-commerce, network storage etc. And for those application areas provisioning of Quality of Service (QoS) is becoming and important issue. DiffServ as specified by the IETF is foreseen to be the most prominent concept for providing predictability in the future Internet. To enable seamless interoperation with the higher level IETF concepts the QoS architecture of the lower layers should comply with the DiffServ paradigm as well. Previous work on predictability in cut-through networks has only studied class based QoS. In this paper we set out to achieve flow level QoS using flow aware admission control in combination with a flow negligent DiffServ inspired QoS mechanism. Our results show that flow level bandwidth guarantees are achievable with the use of the Link-by-Link and the Probe based schemes. In addition we are able to achieve an order of magnitude improvement in jitter and latency in individual flows.},
  booktitle = {Proceedings of the International Conference on Parallel and Distributed Processing
 Techniques and Applications (PDPTA)},
  editor = {Hamid R. Arabnia},
  pages = {1084--1090},
  publisher = {CSREA Press},
  address = {Las Vegas, Nevada}
}

@Inbook{ND.5.Solberg.2004,
  author = {Solberg, A and Husa, K E and Aagedal, J {\O} and Abrahamsen, E},
  title = {QoS-aware MDA},
  year = {2004},
  abstract = {},
  booktitle = {Electronic Notes in Theoretical Computer Science},
  editor = {M. Mislove},
  publisher = {Imprint: ELSEVIER},
  note = {ISSN:1571-0661}
}

@Inproceedings{ND.5.Kvalbein.2004,
  author = {Kvalbein, A and Gjessing, S and Davik, F},
  title = {Performance Evaluation of an Enhanced Bridging Algorithm in RPR Networks},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings 3rd International Conference on                      Networking (ICN'04)},
  pages = {760--767},
  publisher = {IEEE Computer Society Press},
  address = {Guadeloupe, French Caribbean}
}

@Inproceedings{ND.5.Musunoori.2004.a,
  author = {Musunoori, S B and Eliassen, F and Staehli, R},
  title = {QoS-Aware Component Architecture Support for Grid},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the 13th IEEE International Workshops on                      Enabling Technologies: Infrastructures for                      Collaborative Enterprises (WETICE-2004), Modena, Italy},
  editor = {Giacomo Cabri},
  pages = {277--282},
  publisher = {IEEE Computer Society Press, ISBN 0-7695-2183-5}
}

@Inproceedings{ND.5.Gjessing.2004,
  author = {Gjessing, S and Lysne, O and Hansen, A F and Kvalbein, A},
  title = {The Vine Project: Towards Predictable Communication in Heterogeneous Networks},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the 3rd International Conference on
                      Networking (ICN'04)},
  pages = {830--837},
  publisher = {IEEE Computer Society Press},
  address = {Guadeloupe, French Caribbean}
}

@Inproceedings{ND.5.Nordbotten.2004,
  author = {Nordbotten, N A and G{\a\'o}mez, M E and Flich, J and Lopez, P and Robles, A and Skeie, T and Lysne, O and Duato, J},
  title = {A Fully Adaptive Fault-Tolerant Routing Methodology Based on Intermediate Nodes},
  year = {2004},
  abstract = {Massively parallel computing systems are being built with thousands of nodes. Because of the high number of components, it is critical to keep these systems running even in the presence of failures. Interconnection networks play a key-role in these systems, and this paper proposes a fault-tolerant routing methodology for use in such networks. The methodology supports any minimal routing function (including fully adaptive routing), does not degrade performance in the absence of faults, does not disable any healthy node, and is easy to implement both in meshes and tori. In order to avoid network failures, the methodology uses a simple mechanism: for some source-destination pairs, packets are forwarded to the destination node through a set of intermediate nodes (without being ejected from the network). The methodology is shown to tolerate a large number of faults (e.g., five/nine faults when using two/three intermediate nodes in a 3D torus). Furthermore, the methodology offers a gracious performance degradation: in an 8 {\texttimes} 8 {\texttimes} 8 torus network with 14 faults the throughput is only decreased by 6.49\%.},
  booktitle = {Proceedings of IFIP International Conference on Network and Parallel Computing},
  pages = {341--356},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science 3222}
}

@Inproceedings{ND.5.Eide.2004.a,
  author = {Eide, V S Wold and Eliassen, F and Michaelsen, J A},
  title = {Exploiting Content-Based Networking for Video Streaming (extended abstract and technical demo)},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the 12th ACM
                      International Conference on Multimedia
                      (MM'04)},
  pages = {164--165},
  publisher = {ACM Press},
  address = {New York, USA}
}

@Inproceedings{ND.5.Staehli.2004.d,
  author = {Staehli, R and Eliassen, F and Amundsen, S},
  title = {Designing Adaptive Middleware for Reuse},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of International Workshop on                      Reflective and Adaptive Middleware},
  pages = {189--194},
  publisher = {ACM Press}
}

@Inproceedings{ND.5.Gomez.2004.b,
  author = {G{\a\'o}mez, M E and Duato, J and Flich, J and Lopez, P and Robles, A and Nordbotten, N A and Skeie, T and Lysne, O},
  title = {A New Adaptive Fault-Tolerant Routing Methodology for Direct Networks},
  year = {2004},
  abstract = {Interconnection networks play a key role in the fault tolerance of massively parallel computers, since faults may isolate a large fraction of the machine containing many healthy nodes. In this paper, we present a methodology to design fully adaptive fault-tolerant routing algorithms for direct interconnection networks that can be applied to different regular topologies. The methodology is mainly based on the selection of an intermediate node (if needed) for each source-destination pair. Packets are adaptively routed to the intermediate node and, from this node, they are adaptively forwarded to their destination. This methodology requires only one additional virtual channel, even for tori. Evaluation results show that the methodology is 7-fault tolerant, and for up to 14 faults, more than 99\% of the combinations are tolerated, also without significantly degrading performance in the presence of faults.},
  booktitle = {Proceedings of the International Conference on High Performance Computing},
  pages = {462--473},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science 3296}
}

@Inproceedings{ND.5.Horn.2004,
  author = {Horn, G and Lysne, O and Skeie, T},
  title = {The existence of a network of fixed-sized switches that satisfies any communication needs},
  year = {2004},
  abstract = {},
  booktitle = {The 2004 International Conference},
  pages = {1056--1062},
  publisher = {CSREA Press},
  address = {Las Vegas, Nevada, USA}
}

@Inproceedings{ND.5.Meissner.2004,
  author = {Meissner, A and Musunoori, S B and Wolf, L},
  title = {MGMS / GML - Towards a New Policy Specification Framework for Multicast Group Integrity},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings Applications for the Internet
                      (SAINT 2004)},
  pages = {233--239},
  publisher = {IEEE Computer Society Press},
  address = {Tokyo, Japan},
  note = {ISBN 0-7695-2068-5}
}

@Inproceedings{ND.5.Kvalbein.2004.b,
  author = {Kvalbein, A and Gjessing, S},
  title = {Analysis and improved performance of RPR protection},
  year = {2004},
  abstract = {Resilient Packet Ring (RPR, IEEE 802.17) is designed with a protection mechanism aiming at restoring traffic on the ring within 50 ms in case of a link or station failure. In this article, we evaluate RPR protection with respect to service disruption, packet reordering and packet loss. Different error scenarios are simulated, with both steering and wrapping protection. Unfortunately, the 50 ms restoration time guarantee can not always be met if in order delivery of packets is required, since RPR uses a quite long (default 40 ms) topology stabilisation period to avoid packet reordering. We suggest a novel protection mechanism, that does not have to wait for the new topology to be stable, and that gives sub 50 ms restoration for all traffic. We also show that for in order delivery of packets, our new mechanisms discards a very low number of packets compared to the mechanism of the RPR standard.},
  booktitle = {Proceedings of the 12th IEEE Conference On Networks (ICON'04)},
  pages = {119--124},
  publisher = {IEEE Computer Society Press},
  address = {Singapore}
}

@Techreport{ND.7.Gjorven.2004,
  author = {Gj{\o}rven, E and Staehli, R and Eliassen, F},
  title = {A Platform Independent Model of the QuA Architecture},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report}
}

@Misc{ND.8.Theiss.2004,
  author = {Theiss, I and S{\o}dring, T and Solheim, {\r A} G and Horn, G},
  title = {Deliverable 4.3, Simula research lab switch feasibility and research contribution report},
  year = {2004},
  abstract = {},
  howpublished = {Project "Scaleable Intelligent Video Server System (SIVSS)", Sixth framework programme, Commission of the European Communities, 	  
Proposal/Contract no.: 002075}
}

@Misc{ND.8.Musunoori.2004.d,
  author = {Lysne, O and Skeie, T and Theiss, I and Rognved, B DJohnsenandE and T{\o}rudbakken, O},
  title = {The Interconnection Network: Architectural Challenges for Datacenters in the Computational GRID},
  year = {2004},
  abstract = {},
  howpublished = {Presented at the eVITA Workshop},
  note = {Presented at the eVITA Workshop on
                      eScience and Applications}
}

@Techreport{Davik.2004.1,
  author = {Davik, Fredrik and Gjessing, Stein},
  title = {Applying the DiffServ Model to a Resilient Packet Ring Network},
  year = {2004},
  abstract = {},
  institution = {SRL},
  type = {SLR Tecnical Report},
  number = {2004-14}
}

@Inproceedings{Reinemo.2005.1,
  author = {Reinemo, Sven-Arne and Skeie, Tor},
  title = {Ethernet as a Lossless Deadlock Free System Area Network},
  year = {2005},
  abstract = {The way conventional Ethernet is used today differs in two aspects from how dedicated system area networks are used. Firstly, dedicated system area networks are lossless and only drop frames when bit errors occur, while conventional Ethernet drop frames whenever congestion occur. Secondly, these networks are either deadlock free or use mechanisms which avoids deadlock situations, while still using all available links. Ethernet avoids deadlocks by using a spanning tree protocol which turns any topology into a tree.  A drawback of this approach is that we are left with a lot of unused links and thus wasting resources.

In this paper we describe how to obtain a lossless deadlock free network with the best possible performance, while adhering to the current Ethernet standard and using off-the-shelf Ethernet equipment.  We achieve this by introducing flow control in all network nodes and by taking control over the routing algorithm. Also, we use TCP to illustrate the effect of flow control on higher layer protocols.

Through simulations we verify the following tree improvements. Firstly, the activation of flow control turns Ethernet into a lossless network.  Secondly, taking control over the routing algorithm allows us to build any topology without the limitations of the spanning tree protocol. And thirdly, an overall improvement in throughput is achieved by combining these enhancements.},
  booktitle = {Proceedings of the International Symposium on Parallel and Distributed Processing and Applications, Nanjing, China May 2-5},
  editor = {Yi Pan, Daoxu Chen, Minyi Guo, Jiannon Cao,Jack Dongarra},
  pages = {901-914},
  publisher = {Springer-Verlag GmbH},
  address = {Heidelberg, Germany},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{Cicic.2005.1,
  author = {Cicic, Tarik and Kvalbein, Amund and Hansen, Audun Fosselie and Gjessing, Stein},
  title = {Resilient Routing Layers and p-Cycles: Tradeoffs in Network Fault Tolerance},
  year = {2005},
  abstract = {We compare p-cycles and the recently introduced
Resilient Routing Layers as candidate schemes for network-level
fault protection. Using computational routing trials we show
that RRL has shorter backup path lengths and more successful
double-link fault protection. On the other hand, p-cycles may
require less forwarding state. Several tradeoffs of interest for
network designers are described.},
  booktitle = {Proceedings 2005 Workshop on High Performance Switching and Routing, Hong Kong May 12-14},
  publisher = {IEEE},
  note = {ISBN 0-7803-8924-7}
}

@Inproceedings{Cicic.2005.2,
  author = {Cicic, Tarik and Hansen, Audun Fosselie and Gjessing, Stein and Lysne, Olav},
  title = {Applicability of Resilient Routing Layers for k-Fault Network Recovery},
  year = {2005},
  abstract = {Most networks experience several failures every day, and often multiple failures occur simultaneously. Still, most recovery mechanisms are not designed to handle multiple failures. We recently proposed a versatile recovery method called Resilient Routing Layers, and in this paper we analyze its suitability for handling multiple failures of network components. We propose a simple probabilistic algorithm for RRL layer creation, and evaluate its performance by comparing it with the Redundant Trees recovery mechanism. We show that not only does RRL provide better fault tolerance, but it also has qualitative advantages that make it very interesting in network systems design.},
  booktitle = {Proceedings of International Conference on Networking (ICN), Reunion, France April 17-21},
  pages = {173 - 183},
  publisher = {Springer-Verlag GmbH },
  edition = {Vol. 3421},
  note = {ISSN 0302-9743,  ISBN 3-540-25339-4, }
}

@Article{Lysne.2005.2,
  author = {Lysne, Olav and Pinkston, Timothy Mark and Duato, Jos{\a\'e}},
  title = {A Methodology for Developing Deadlock-Free Dynamic Network Reconfiguration Processes},
  year = {2005},
  abstract = {Dynamic network reconfiguration is defined as the process of changing from one routing function
to another while the network remains up and running. The main challenge is in avoiding
deadlock anomalies while keeping restrictions on packet injection and forwarding minimal. Current
approaches either require virtual channels in the network or they work only for a limited set
of routing algorithms and/or fault patterns. In this paper, we present a methodology for devising
deadlock free and dynamic transitions between old and new routing functions that is consistent
with recently proposed theory. The methodology is independent of topology, can be applied
to any deadlock-free routing function, and puts no restrictions on the routing function changes that
can be supported. Furthermore, it does not require any virtual channels to guarantee deadlock
freedom. This research is motivated by current trends toward using increasingly larger Internet
and transaction processing servers based on clusters of PCs that have very high availability and
dependability requirements, as well as other local, system, and storage area network-based computing
systems.},
  journal = {IEEE Transactions on Parallel and Distibuted Systems},
  volume = {16},
  number = {5},
  pages = {428-443}
}

@Article{Duato.2005.1,
  author = {Duato, Jos{\a\'e} and Lysne, Olav and Pang, Ruoming and Pinkston, Timothy Mark},
  title = {A Theory for Deadlock-Free Dynamic Network Reconfiguration},
  year = {2005},
  abstract = {This paper develops theoretical support useful for determining deadlock properties of dynamic
network reconfiguration techniques and serves as a basis for the development of design methodologies
useful for deriving such techniques. It is applicable to interconnection networks typically
used in multiprocessor servers, network-based computing clusters and distributed storage systems,
and also has potential application to system-on-chip networks. This theory builds on basic principles
established by previous theories while pioneering new concepts fundamental to the case of
dynamic reconfiguration.},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {16},
  number = {5},
  pages = {412-427}
}

@Inproceedings{Kvalbein.2005.1,
  author = {Kvalbein, Amund and Hansen, Audun Fosselie and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {Fast Recovery from Link Failures using Resilient Routing Layers},
  year = {2005},
  abstract = {We present a novel scheme for network recovery, named Resilient
Routing Layers (RRL). Our proposed scheme is based on calculating
fully connected topology subsets, termed layers, which are used to
forward traffic in case of a network failure. For the purpose of this
work, the layers are created to protect against link failures only.
RRL keeps pre-calculated backup routing information in the network
stations. This allows local response to network failures, which gives
recovery in the order of milliseconds.

The main strengths of our approach are its flexibility, as it is
independent of the network technology used, and its simplicity, as it
offers the network operator a simple and coherent view of the
resources available after a link failure. We also show that our scheme
scales well for networks of several hundred nodes.
},
  booktitle = {10th IEEE Symposium on Computers and Communications (ISCC 2005)},
  pages = {554--560},
  publisher = {IEEE Communications Society},
  address = {Cartagena, Spain, June 27-30},
  note = {ISSN 1530-1346, ISBN 0-7695-2373-0}
}

@Inproceedings{Hansen.2005.1,
  author = {Hansen, Audun Fosselie and Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {Resilient Routing Layers for Recovery in Packet Networks},
  year = {2005},
  abstract = {The existing methods for network recovery are often complex and seldom used by network administrators. In 
this paper we present a novel approach for global and local recovery named Resilient Routing Layers (RRL). The 
method is supported by algorithms, but also simple enough for a network administrator to implement by hand for 
reasonably sized networks. The idea in our approach is that for each node in the network there is a topology subset 
called a {\textquotedblleft}safe layer{\textquotedblright}, which can handle any traffc affected by a fault in the node itself, or any of its links. 
By analysis using different network topologies, we demonstrate that our approach performs well compared 
to other comparable methods. Particularly, we demonstrate RRLs performance for backup-path lengths and state 
information overhead that are assumed to be the weakest parameters for our method. We discuss implementation 
issues of RRL, and demonstrate its applicability to MPLS networks.},
  booktitle = {International Conference on Dependable Systems and Networks (DSN 2005) Yokohama, Japan, June 28-July 1},
  editor = {Bob Werner},
  publisher = {IEEE Computer Society},
  note = {ISBN 0-7695-2282-3}
}

@Inproceedings{Hansen.2005.2,
  author = {Hansen, Audun Fosselie and Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein},
  title = {Resilient Routing Layers for Network Disaster Planning},
  year = {2005},
  abstract = {Most research on network recovery has been centered around two common assumptions regarding failure characteristics: Failures do not occur simultaneously and failures do mostly strike links. Even this may be the characteristics of everyday failures, we argue that disasters like earthquakes, power outages and terrorist attacks impose other failure characteristics. In this paper we demonstrate how our method, called Resilient Routing Layers, can be used as a tool for recovery from failures adhering to such disaster characteristics.},
  booktitle = {Networking - ICN 2005: 4th International Conference on Networking, Reunion April 17-21},
  editor = {Pascal Lorenz, Petre Dini },
  pages = {1097--1105},
  publisher = {Springer-Verlag GmbH },
  series = {Lecture Notes in Computer Science, Volume 3421 / 2005  },
  note = {ISSN 0302-9743, ISBN 3-540-25339-4,}
}

@Inproceedings{Hansen.2005.3,
  author = {Hansen, Audun Fosselie and Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein and Lysne, Olav and Jensen, Terje and {\O}sterb{\o}, Olav Norvald},
  title = {Fast, Effective and Stable IP Recovery using Resilient Routing Layers},
  year = {2005},
  abstract = {Recovery at the IP layer is hampered by the slow convergence of IP rerouting. Recovery times
in the range of seconds do not adhere to the requirements of many Internet applications today. To
offer fast, pre-configured and loop-free IP recovery we have proposed a new method named Resilient
Routing Layers (RRL). In this paper we demonstrate how RRL also can provide resource-effective
recovery in IP networks. We compare the performance of RRL with what intuitively should be the
most resource-effective method: Full global rerouting.},
  booktitle = {The 19th International Teletraffic Congress (ITC19) Beijing, China, August 29-September 2},
  editor = {Xiongjian Liang, Zhanhong Xin, V. B. Iversen and G. S. Kuo},
  pages = {1631--1640},
  publisher = {Beijing University of Posts and Telecommunications Press},
  note = {ISBN 7-5635-1141-5}
}

@Inproceedings{Hansen.2005.4,
  author = {Hansen, Audun Fosselie and Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {Resilient Routing Layers: an Overview of Technology and Applications},
  year = {2005},
  abstract = {A thorough study of the network recovery
methods available today reveals that there still exist a great
potential for improvements. Most methods are optimized
for handling single link failures, thus providing inadequate
protection for node failures. Methods are often too complex
to be adopted by network administrators in practice.
Recovery in connectionless IP networks is still hampered
by inappropriate delayed recovery due to slow convergence
of IP rerouting.
The authors have recently proposed Resilient Routing
Layers (RRL) as an answer to some of the deficiencies
observed from current methods. In this paper we will
describe main features of RRL, present some recent
research results and sketch our plans for future work.},
  booktitle = {EURO NGI Workshop on Traffic Engineering, Protection and Restoration, Rome April 18-20},
  publisher = {EuroNGI}
}

@Inproceedings{Davik.2005.1,
  author = {Davik, Fredrik and Kvalbein, Amund and Gjessing, Stein},
  title = {An Analytical Bound for Convergence of the Resilient Packet Ring Aggressive Mode Fairness Algorithm},
  year = {2005},
  abstract = {Resilient Packet Ring (RPR) is a new standard, designated IEEE standard number 802.17, for MAN and WAN dual ring topologies. RPR uses the buffer insertion principle as a basis for its medium access control protocol. In this paper, we analyze parts of the aggressive mode of the RPR fairness protocol. We look at a congested node, and utilize control systems theory to analyze the stability of the associated fairness algorithm. In particular, we discuss how the settings of the two important parameters ageCoef and lpCoef influence the stability of an RPR-network. At the end of the paper we present simulated scenarios in order to illustrate our results.},
  booktitle = {Proceedings of the 40th annual IEEE International Conference on Communications, Seoul, Korea, May 16-20},
  publisher = {IEEE},
  note = { ISBN 0-7803-8938-7}
}

@Inproceedings{Davik.2005.2,
  author = {Davik, Fredrik and Gjessing, Stein},
  title = {Applying the DiffServ Model to a Resilient Packet Ring Network},
  year = {2005},
  abstract = {In this paper we introduce a formal specification of parts of the service differentiation mechanisms of the recent IEEE 802.17 Resilient Packet Ring (RPR) standard, and assess RPR's suitability for use in a DiffServ environment. We propose a simple mapping between RPR's traffic classes and three standardized DiffServ Per Hop Behavior groups.  When using this mapping, we discuss, by use of an analytical example and simulation results, the behavior of the traffic assigned to each PHB group in terms of its throughput (all PHBs) and delay properties (for the EF PHB only). For the simulation part, we use our OPNET implementation of the RPR standard.},
  booktitle = {Proceedings of Networking 2005, Waterloo, Ontario, Canada May 2-6},
  editor = {R. Boutaba et al},
  pages = {1461--1464},
  publisher = {Springer},
  series = {LNCS 3462},
  note = {ISSN 0302-9743, ISBN 3-540-25809-4}
}

@Inproceedings{Davik.2005.3,
  author = {Davik, Fredrik and Kvalbein, Amund and Gjessing, Stein},
  title = {Resilent Packet Ring Low Priority Traffic Latency},
  year = {2005},
  abstract = {Resilient Packet Ring (RPR - IEEE 802.17) is an insertion buffer, dual ring technology, utilizing a back pressure based fairness algorithm to distribute bandwidth when congestion occurs. The fairness algorithm may oscillate and under some conditions the oscillations continue indefinitely even under stable load conditions. In this paper, we evaluate the latency experienced by packets sent during such oscillations. We analyze transient behavior and how the oscillations of the fairness algorithm influence the jitter caused by  unfair access to the ring, as well as jitter caused by the insertion  buffers around the ring. We conclude that, in most cases, latency and jitter are within acceptable bounds.  A modification to the RPR fairness algorithm has previously been proposed by the authors, but its implications on latency has never before been demonstrated.  We compare the improved fairness algorithm to the original, and find that the modified algorithm, for all evaluated scenarios, perform at least as well as the original with respect to latency and jitter.  In some problem scenarios, we find that the modified algorithm performs significantly better than  the original.},
  booktitle = {Proceedings of the 2005 International Conference on  Communications in Computing: CIC 2005, Los Angeles CA, USA, February 2-4},
  publisher = {CSREA Press},
  note = {ISBN-1-932415-48-3}
}

@Inproceedings{Davik.2005.4,
  author = {Davik, Fredrik and Kvalbein, Amund and Gjessing, Stein},
  title = {Performance Evaluation and Improvement of Non-Stable Resilent Packet Ring Behavior},
  year = {2005},
  abstract = {Resilient Packet Ring (RPR) is a new networking standard developed by the IEEE LAN/MAN working group. RPR is an insertion buffer, dual ring technology, utilizing a back pressure based fairness algorithm to distribute bandwidth when congestion occurs. In its attempt to distribute bandwidth fairly, the calculated fair rate in general oscillates and under some conditions the oscillations continue indefinitely even under stable load conditions. In this paper, we evaluate the performance of the RPR ring during oscillations. In particular, we analyze transient behavior and how the oscillations of the fairness algorithm influence the throughput, both on a per node basis and for the total throughput of the ring.  For congestion-situations, we conclude that, in most cases, RPR allows for full link-utilization and fair bandwidth distribution of the congested link.  A modification to the RPR fairness algorithm has previously been proposed by the authors.  We compare the improved fairness algorithm to the original, and find that the modified algorithm, for all evaluated scenarios perform at least as well as the original. In some problem scenarios, we find that the modified algorithm performs significantly better than the original.},
  booktitle = {Proceedings of the 4th International Conference on Networking (ICN'05), Reunion, France, April 17-21 2005},
  editor = {Pascal Lorenz and Petre Dini},
  pages = {551--563},
  publisher = {Springer-Verlag GmbH},
  series = {LNCS 3421},
  note = {ISSN 0302-9743, ISBN 3-540-25339-4}
}

@Techreport{Davik.2005.5,
  author = {Davik, Fredrik and Kvalbein, Amund and Gjessing, Stein},
  title = {Congestion Domain Boundaries in Resilient Packet Rings},
  year = {2005},
  abstract = {In June 2004, the IEEE approved a new standard for Resilient Packet Rings (RPR). The standard is maintained in the 802 LAN/MAN Standards Committee, and is designated the standard number 802.17. In this paper, we analyze and discuss performance aspects of the Resilient Packet Ring fairness mechanism. We explain that, if the ring is not configured correctly, the fairness mechanism fails to stabilize at a fair division of bandwidth between the active nodes. We present a novel addition to the fairness algorithm, and show that with this modification, RPR reaches a stable state with more optimal parameter settings. We also show that our proposed modification gives shorter convergence time for the Resilient Packet Ring fairness algorithm.},
  institution = {Simula Research Laboratory},
  type = {Technical Report},
  number = {2005-03}
}

@Techreport{Davik.2005.6,
  author = {Davik, Fredrik and Gjessing, Stein},
  title = {Improvement of Resilient Packet Ring Fairness},
  year = {2005},
  abstract = {Resilient Packet Ring (RPR) is a recent networking standard developed by the IEEE LAN/MAN working group. RPR is an insertion buffer, dual ring technology, utilizing a back pressure based fairness algorithm to distribute bandwidth when congestion occurs. The fairness algorithm has two modes of operation, called respectively the aggressive and the conservative fairness modes. For some scenarios, the aggressive mode fairness suffers from severe performance deficiencies.

In this paper, we propose two novel contributions. The first is a measurement method which enables a node to determine its operating context. The second contribution is a fair rate calculation method, termed the moderate fairness mode, which solves the aggressive mode performance deficiencies while retaining several other properties provided by the aggressive mode fairness.

We compare the performance of the moderate fairness mode to that of the aggressive and the conservative modes by simulations, and find that for some scenarios the moderate mode outperforms the aggressive and the conservative modes. For some other scenarios, the convergence time of the moderate mode is somewhat longer than that of the aggressive mode.},
  institution = {Simula Research Laboratory},
  type = {Technical Report},
  number = {2005-02}
}

@Article{Griwodz.2004.1,
  author = {Griwodz, Carsten and Fiksdal, Steffen and Halvorsen, P{\r a}l},
  title = { Translating Scalable Video Streams from Wide-Area to Access Networks},
  year = {2004},
  abstract = {},
  journal = {Campus Wide Information Systems},
  volume = {21},
  number = {5},
  pages = {205--210}
}

@Article{Zink.2004.1,
  author = {Zink, Michael and Schmitt, Jens and Griwodz, Carsten},
  title = { Layer-encoded video streaming: A proxy's perspective },
  year = {2004},
  abstract = {},
  journal = {IEEE Communications Magazine},
  volume = {42},
  number = {8},
  pages = {96--103}
}

@Inproceedings{DMJ\_MMCN2005,
  author = {Eide, Viktor S Wold and Eliassen, Frank and Michaelsen, J{\o}rgen Andreas},
  title = {Exploiting Content-Based Networking for Fine Granularity Multi-Receiver Video Streaming},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of the 12th Annual Conference on Multimedia Computing and Networking (MMCN '05), San Jose, California, USA, January 19-20},
  editor = {Surendar Chandra and               Nalini Venkatasubramanian},
  pages = {155-166},
  publisher = {SPIE}
}

@Inproceedings{Horn.2005.1,
  author = {Horn, Geir and Oommen, B. John},
  authorURLs = {/people/geirho and http://www.scs.carleton.ca/\~oommen/index.html},
  title = {A Fixed-Structure Learning Automaton Solution to the Stochastic Static Mapping Problem},
  year = {2005},
  abstract = {This paper considers the problem of distributing the processes of a parallel application onto a set of computing nodes. This problem called the Static Mapping Problem (SMP) is known to be NP-Hard, and has been tackled using heuristic solutions. The objective of this paper is to present the first reported Learning Automaton (LA) based solution to the SMP, generated by the close resemblance of the SMP to the equipartitioning problem. The LA in question is of the so-called Fixed-Structure family, solution to the equipartitioning problem is then modified to solve the SMP. Several algorithmic variants of this solution have been implemented, and these have all been rigorously tested and evaluated through extensive simulations on randomly generated parallel applications. The focus in this work is to demonstrate the applicability of LA to the SMP, not to optimise and evaluate the performance of the proposed strategy. The results presented here clearly demonstrate that LA provide a promising tool that can effectively solve the mapping problem.},
  booktitle = {Proceedings of the 19th IEEE International Parallel and Distributed Processing Symposium (IPDPS 2005)},
  editor = {H. J. Siegel, David A. Bader, Jean-Luc Gaudiot},
  pages = {297b - 297b},
  publisher = {IEEE},
  address = {Denver, Colorado, USA, April 3-8}
}

@Inproceedings{Amundsen.2005.1,
  author = {Amundsen, Sten Lundesgaard and Lund, Ketil and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {QoS-aware Mobile Middleware for Video Streaming},
  year = {2005},
  abstract = {},
  booktitle = {31st EUROMICRO Conference on Software Engineering and Advanced Applications (SEAA), Portugal August 30-September 3 },
  pages = {54-61},
  publisher = {IEEE Computer Society}
}

@Inproceedings{Davik.2005.7,
  author = {Davik, Fredrik and Kvalbein, Amund and Gjessing, Stein},
  title = {Improvement of Resilient Packet Ring Fairness},
  year = {2005},
  abstract = {Resilient Packet Ring (RPR, IEEE std. 802.17- 2004) is a recent networking standard developed by the IEEE LAN/MAN working group. RPR is an insertion buffer, dual ring technology, utilizing a back pressure based fairness algorithm to distribute bandwidth when congestion occurs. In its attempt to control a set of nodes  sending behavior over a congested link, the RPR fairness algorithm suffers from two severe performance deficiencies. The first concerns how the node closest to a congested link calculates a fair rate estimate, the second deficiency relates to the method used to distribute this fair rate estimate to nodes upstream from the congested node. In this paper, we analyze these deficiencies and propose improvements to resolve them.},
  booktitle = {Proceedings of the 48th annual IEEE Global Telecommunications Conference (GLOBECOM 2005), St. Lous, Missouri, USA, November 28-December 2},
  publisher = {IEEE},
  note = {ISBN 0-7803-9414-3}
}

@Inproceedings{Kvalbein.2005.2,
  author = {Kvalbein, Amund and Gjessing, Stein},
  title = {Protection of RPR strict order traffic},
  year = {2005},
  abstract = {  Resilient Packet Ring (RPR, IEEE 802.17) is designed with a
  protection mechanism aiming at restoring traffic on the ring within
  50 ms in case of a link or node failure. However, the total
  experienced disruption time often exceeds this if strict order
  delivery of packets is required, since a 40 ms (configurable from 10
  ms to 100 ms) topology stabilisation timer is used to avoid packet
  reordering. In this article, we present three alternative ways to
  avoid packet reordering for strict order traffic in RPR networks.
  The three suggested methods are discussed analytically, and
  simulations are used to compare their performance.  Our simulation
  results suggest that the new methods give up to 90\\% reduction in
  packet loss during a failure situation compared to the current RPR
  standard.
},
  booktitle = {Proceedings 14th IEEE Workshop on Local and Metropolitan Area Networks},
  publisher = {IEEE},
  address = {Chania, Crete September 18-21}
}

@Inproceedings{Kvalbein.2006.1,
  author = {Kvalbein, Amund and Hansen, Audun Fosselie and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {Fast IP Network Recovery using Multiple Routing Configurations},
  year = {2006},
  abstract = {  As the Internet takes an increasingly central role in our
  communications infrastructure, the slow convergence of routing
  protocols after a network failure becomes a growing problem. To
  assure fast recovery from link and node failures in IP networks, we
  present a new recovery scheme called Multiple Routing Configurations
  (MRC). MRC is based on keeping additional routing information in the
  routers, and allows packet forwarding to continue on an alternative
  output link immediately after the detection of a failure.  Our
  proposed scheme guarantees recovery in all single failure scenarios,
  using a single mechanism to handle both link and node failures, and
  without knowing the root cause of the failure. MRC is strictly
  connectionless, and assumes only destination based hop-by-hop
  forwarding. It can be implemented with only minor changes to
  existing solutions.  In this paper we present MRC, and analyze its
  performance with respect to scalability, backup path lengths, and
  load distribution after a failure.
},
  booktitle = {INFOCOM 2006},
  editor = {Arturo Azcorra, Joe Touch, Zhili Zhang},
  pages = {23--29},
  publisher = {IEEE},
  address = {Barcelona, Spain},
  isbn = {1-4244-0222-0}
}

@Inproceedings{Musunoori.2005.1,
  author = {Musunoori, Sharath Babu and Eliassen, Frank and Eide, Viktor S. Wold},
  title = {QoS-Driven Service Configuration in Computational Grids},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of the 6th IEEE/ACM International Workshop on Grid Computing held in Conjunction with the International Conference for High Performance Computing, Neworking and Strorage (SC|05), November 12-18, Seattle, WA, USA},
  editor = {Daniel S. Katz},
  pages = {304-307},
  publisher = {IEEE Computer Society Press, ISBN 0-7803-9493-3}
}

@Inproceedings{Gjessing.2005.1,
  author = {Gjessing, Stein and Maus, Arne},
  title = {Discrete Event Simulation of a Large OBS Network},
  year = {2005},
  abstract = {Optical Burst Switching (OBS) is a much researched paradigm for the next generation optical Internet. We have made a detailed discrete event simulation model of OBS networks. Among other things our model includes self similar traffic sources, burst assembly with fixed and variable length bursts, burst scheduling, wavelength conversion and fiber delay lines. In this paper we simulate a large optical burst-switched backbone network using the COST 239 network topology that connects 11 European cities. The performance of this realistic network is investigated, mainly by varying the load and the number of channels (lambdas). We propose and evaluate a new method for the utilization of otherwise unused network capacity by very low priority traffic. Other interesting results include how total burst loss increase when high priority bursts are used.},
  booktitle = {Proceedings 2005 IEEE International Conference on Systems, Man and Cybernetics, Big Island, Hawaii, October 10-12},
  publisher = {IEEE},
  note = {ISBN 0-7803-9299-X }
}

@Inproceedings{Sem-jacobsen.2005.1,
  author = {Sem-Jacobsen, Frank Olaf and Skeie, Tor and Lysne, Olav and T{\o}rudbakken, Ola and Rongved, Eivind and Johnsen, Bj{\o}rn},
  title = {Siamese-Twin: A Dynamically Fault-Tolerant Fat Tree},
  year = {2005},
  abstract = {Fat-trees are a special case of multistage interconnection
networks with quite good static fault tolerance capabilities.
They are however straightforwardly unable to provide
local dynamic fault tolerance. In this paper we propose
a network topology based on the fat-tree using two parallel
networks with crossover links between them in an effort
to enable dynamic fault tolerance. We evaluate and compare
this topology with two other similar fat-tree topologies
and show through simulations that the new topology is
able to improve slightly upon the ability to tolerate faults
statically. More importantly, we show that the new network
topology is the only one of the evaluated topologies able to
tolerate one fault dynamically, with a superior network performance
in the face of dynamically handled faults.},
  booktitle = {International Parallel and Distributed Processing Symposium (IPDPS), Denver, Colorado, USA, April 4-8},
  editor = {Unknown},
  publisher = {IEEE Computer Society},
  isbn = {0-7695-2312-9}
}

@Inproceedings{Sem-jacobsen.2005.2,
  author = {Sem-Jacobsen, Frank Olaf and Skeie, Tor and Lysne, Olav},
  title = {A Dynamic Fault-Torlerant Routing Algorithm for Fat-trees},
  year = {2005},
  abstract = {The fat tree is a network topology well suited for use as
the interconnection network in systems such as parallel computers.
Its large number of paths between every source/destination
pair gives the fat tree the ability to provide high throughput.
This also gives it a high probability of tolerating network faults
statically, but few algorithms to dynamically tolerate faults in fattrees
have previously been proposed. In this paper we present
a deadlock free routing method for providing dynamic fault
tolerance through misrouting downwards in the network. We
show that the algorithm is one fault-tolerant, and that it with a
certain probability can tolerate a large number of faults.},
  booktitle = {International Conference on Parallel and Distributed Processing Techniques and Applications, Las Vegas, Nevada, USA, June 27-30},
  editor = {Hamid R. Arabnia},
  pages = {318-324},
  publisher = {CSREA Press},
  isbn = {1-932415-58-0}
}

@Inproceedings{Musunoori.2005.2,
  author = {Musunoori, Sharath Babu and Horn, Geir},
  title = {A Fixed-Structure Learning Automaton Solution to the Quality Aware Application Service Configuration in a Grid Environment},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of the 17th International Conference on Parallel and Distributed Computing and Systems (PDCS 2005), November 14-16, Phoenix, AZ, USA},
  editor = {S.Q. Zheng},
  pages = {7-12},
  publisher = {ACTA Press, ISBN 0-88986-527-2}
}

@Article{Floch.2006.1,
  author = {Floch, Jacqueline and Hallsteinsen, Svein and Stav, Erlend and Eliassen, Frank and Lund, Ketil and Gj{\o}rven, Eli},
  title = {Using architecture models for runtime adaptability},
  year = {2006},
  abstract = {Developers typically use software architecture models at design time to capture significant decisions about a software system's organization and to describe and establish a common understanding about the system's abstract properties. Architectural information isn't usually explicitly represented at runtime; developers transform and realize the architectural properties through runtime artifacts. Recently, the introduction of software platforms supporting component plug-in and dynamic binding has facilitated adaptation of software systems at runtime. Preserving the properties described by the architecture model during adaptation is an important task. We propose a self-adaptation approach that exploits architecture models to reason about and control adaptation at runtime. We can derive runtime models from design models facilitating the developers{\textquoteright} task. We developed the approach in the context of mobile computing.

This article is part of a focus section on software architecture.
},
  journal = {IEEE Software},
  volume = {23},
  number = {2},
  pages = {62--70}
}

@Phdthesis{Eide.2005.3,
  author = {Eide, Viktor S. Wold},
  title = {Exploiting Event-Based Communication for Real-Time Distributed and Parallel Video Content Analysis},
  year = {2005},
  abstract = {},
  school = {Faculty of Mathematics and Natural Sciences, University of Oslo, Norway}
}

@Techreport{Hansen.2005.6,
  author = {Hansen, Audun Fosselie and Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {A Comparison of Different Approaches for Calculating Resilient Routing Layers and Multiple Routing Configurations},
  year = {2005},
  abstract = {Fast proactive recovery has for years been a very import-
ant research field due to increased reliability in the Internet for business
critical and real-time communications. Resilient Routing Layers (RRL)
and Multiple Routing Configurations (MRC) has been proposed as a
new approach providing local and proactive recovery. RRL and MRC
also provides a network manager with a simple set of sub-topologies as
abstractions for the recovery routing. This paper will compare different
methods for generating such layers and configurations along two para-
meters, scalability and effects on routing of the recovered traffic.},
  institution = {Simula Research Laboratory},
  type = {Technical Report},
  number = {2005-15}
}

@Inproceedings{Musunoori.2005.3,
  author = {Musunoori, Sharath Babu and Eliassen, Frank},
  title = {QoS-Aware Application Service Configuration in a Grid Environment},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of the 9th International Conference on Software Engineering and Applications (SEA2005), November 14-16, Phoenix, AZ, USA},
  editor = {W.T. Tsai and M.H. Hamza},
  pages = {363-370},
  publisher = {ACTA Press, ISBN 0-88986-531-0}
}

@Inproceedings{Musunoori.2005.4,
  author = {Musunoori, Sharath Babu},
  title = {Quality Aware Service Planning in Computational Grids},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of the 2nd international doctoral symposium on Middleware, Grenoble, France, November 28-December 2},
  editor = {Ackbar Joolia and S{\a\'e}bastien Jean},
  pages = {1-5},
  publisher = {ACM International Conference Proceeding Series; Vol. 114}
}

@Phdthesis{Davik.2005.8,
  author = {Davik, Bj{\o}rn Fredrik},
  title = {Fairness Aspects of Buffer-Insertion Rings in General and Resilient Packet Rings in Particular},
  year = {2005},
  abstract = {The use of ring topologies in computer networks, was introduced in the late 1960s/early 1970s, with the Newhall ring being an example of an early implementation. Since then, many different solutions for ring networks have been proposed and implemented. 

One of the fundamental problems in such networks, in the presence of several users with high demands for bandwidth, is to simultaneously achieve high utilization, low delay and fair access to the ring's bandwidth resources.

In this thesis, we cover issues related to the above problems in ring networks. However, we limit our focus to the above problems in the context of buffer-insertion rings and a particular type of buffer-insertion rings -- namely the IEEE 802.17 Resilient Packet Ring. 

As such, our published results have a strong focus on Resilient Packet Rings. However the general findings from our experiments should be applicable to buffer-insertion (as well as slotted) rings.

In our research contributions, we start by proving an introduction to the RPR standard and its problem area.  This contribution provides a starting-point of study for the RPR novice, be it an network engineer or a university professor looking for an introduction to RPR technology.

Then, we proceed by presenting an analytical model of the RPR aggressive fairness mode. By this model, a starting point is provided for a safer configuration of a Resilient Packet Ring, as well as reducing the number of heuristics needed.

Next, we provide several contributions in which we analyze and propose improvements to various performance deficiencies in Resilient Packet Rings. Some of these deficiencies are commonly known in the RPR research community, while others are not so well known.

And finally, at the end, we present an application of RPR, which is made possible by the workings of the fairness and service differentiation capabilities provided by the RPR standard.},
  school = {Faculty of Mathematics and Natural Sciences at the University of Oslo August}
}

@Book{Skeie.2005.2,
  editor = {Skeie, Tor and Yang, Chu-Sing},
  title = {Proceedings of the 2005 International Conference on Parallel Processing Workshops},
  year = {2005},
  abstract = {},
  publisher = {IEEE Computer Society Conference Publishing Services},
  isbn = {0-7695-2381-1}
}

@Inproceedings{Gjessing.2006.1,
  author = {Gjessing, Stein},
  title = {Implementation of two Resilience Mechanisms  using Multi Topology Routing and Stub Routers},
  year = {2006},
  abstract = {Resilient Routing Layers (RRL) and Multiple
Routing Configurations (MRC) have been proposed as
methods to achieve fast recovery from router and link
failures in connectionless networks. In this article we show
how RRL and MRC can be implemented using the Multi
Topology routing scheme and the Stup Router advertisements
currently developed within the IETF. This makes
RRL and MRC very viable candidates for protection of
IP traffic in the next generation Internet.

},
  booktitle = {Advanced Internationl Confernece on Telecommunications (AICT'06)},
  publisher = {IEEE Computer Scociety Press},
  note = {ISBN 0-7695-2522-9}
}

@Techreport{Davik.2005.9,
  author = {Davik, Fredrik and Gjessing, Stein},
  title = {Applying the DiffServ Model to a Resilient Packet Ring Network},
  year = {2005},
  abstract = {In June 2004, the IEEE approved a new standard called Resilient Packet Ring (RPR), that is maintained in the 802 LAN/MAN Committee and designated standard number IEEE 802.17-2004. Among the features provided by the RPR technology are built-in QoS capabilities for traffic class differentiation, bidirectional transfer of data with destination stripping and spatial reuse and fast protection against node and link failure(s). In this paper, we introduce a framework used to specify the throughput of RPR, and propose a simple mapping between RPR's service classes and DiffServ Per Hop Behavior groups. We evaluate this mapping analytically, using a simple generic example, and by simulation using some selected scenarios. All our findings support that our proposed mapping between RPR's traffic classes and the PHB groups is indeed a viable one.},
  institution = {Simula Research Laboratory},
  type = {Technical Report},
  number = {2005-01}
}

@Inproceedings{Mejia.2006.1,
  author = {Mej{\a\'\i}a, Andres and Flich, Jos{\a\'e} and Duato, Jos{\a\'e} and Reinemo, Sven-Arne and Skeie, Tor},
  title = {Segment-Based Routing: An Efficient Fault-Tolerant Routing Algorithm for Meshes and Tori},
  year = {2006},
  abstract = {Computers get faster every year, but the demand for computing resources seems to grow at an even faster rate. Science keeps demanding more processing power for calculations and simulations, growth in E-commerce requires powerful servers to offer seamless online shopping, and massive multiplayer online games requires powerful and stable systems to keep their virtual worlds running 24 hours a day. Depending on the problem domain, this demand for more power can be satisfied by either, massively parallel computers, or clusters of computer.  Common for both approaches is the dependence on high performance interconnect networks such as Myrinet, Infiniband, or 10 Gigabit Ethernet. While high throughput and low latency are key features of interconnection networks, the issue of fault-tolerance is now becoming increasingly important.  As the number of network components grows so does the probability for failure, thus it becomes important to also consider the fault-tolerance mechanism of interconnection networks.  The main challenge then lies in combining performance and fault-tolerance, while still keeping cost and complexity low. 

This paper proposes a new deterministic routing methodology for tori and meshes, which achieves high performance without the use of virtual channels. Furthermore, it is topology agnostic in nature, meaning it can handle any topology derived from any combination of faults when combined with static reconfiguration. The algorithm, referred to as Segment-based Routing (SR), works by partitioning a topology into subnets, and subnets into segments. This allows  us to place bidirectional turn restrictions locally within a segment. As segments are independent, we gain the freedom to place turn restrictions within a segment independently from other segments. This results in a larger degree of freedom when placing turn restrictions compared to other routing strategies. 

In this paper a way to compute segment-based routing tables is presented and applied to meshes and tori. Preliminary evaluation results show that the concept of segments leads to an increase in performance by a factor of 1.8 over FX and up*/down* routing. },
  booktitle = {20th IEEE International Parallel \& Distributed Processing Symposium},
  editor = {Shoukat Ali (Proceedings Chair)},
  pages = {1--10},
  publisher = {IEEE Computer Society},
  address = {Washington, USA},
  isbn = {1-4244-0054-6}
}

@Article{Lysne.2006.1,
  author = {Lysne, Olav and Skeie, Tor and Reinemo, Sven-Arne and Theiss, Ingebj{\o}rg Thelin},
  title = {Layered Routing in Irregular Networks},
  year = {2006},
  abstract = {Freedom from deadlock is a key issue in Cut-Through, Wormhole and Store and Forward networks, and such freedom is usually obtained through careful design of the routing algorithm. Most existing deadlock-free routing methods for irregular topologies do, however, impose severe limitations on the available routing paths.  We present a method called Layered Routing, which gives rise to a series of routing algorithms, some of which perform considerably better than previous ones.  Our method groups virtual channels into network layers, and to each layer it assigns a limited set of source/destination address pairs. This separation of traffic yields a significant increase in routing efficiency.  We show how the method can be used to improve the performance of irregular networks, both through load balancing and by guaranteeing shortest-path routing. The method is simple to implement, and its application does not require any features in the switches other than the existence of a modest number of virtual channels.  The performance of the approach is evaluated through extensive experiments within three classes of technologies. These experiments reveal a need for virtual channels as well as an improvement in throughput for each technology class.},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {17},
  number = {1},
  pages = {51-65}
}

@Techreport{Cicic.2005.3,
  author = {Cicic, Tarik},
  title = {On Upper Bounds on the State Requirements of Fault-Tolerant Multi-Topology Routing},
  year = {2005},
  abstract = {Multi-topology routing is an increasingly popular concept in computer communications, and can be successfully applied to the network fault tolerance. Multi-topology routing implies an increase in routing state requirements proportional to the number of logical topologies used. The network routing state is often subject to stringent system limits and should therefore be deterministically controllable. In this paper we show that the minimal number of logical topologies needed to guarantee fault-tolerant multi-topology routing is related to the size of the largest minimal cycle (LMC) in the network. Link-fault tolerance and node-fault tolerance are discussed separately. For link-fault tolerant routing, the LMC size is the upper bound. For node-fault tolerant routing, the upper bound is the greatest of 6 and the LMC size of an arbitrary planar sub-topology. We provide formal proofs of validity of these bounds in arbitrary biconnected networks. We also evaluate the quality of these bounds, showing that they are reasonably strict.},
  institution = {Simula Research Laboratory},
  type = {Technical Report },
  number = {2005-17},
  note = {Please note that after this report was published a counter-example has been found for a claim stated in Sec. 4.2. Please contact the author for details.}
}

@Inproceedings{Halvorsen.2005.1,
  author = {Halvorsen, P{\r a}l and Tom Anders, Dalseng and Griwodz, Carsten},
  title = {Assessment of Data Path Implementations for Download and Streaming},
  year = {2005},
  abstract = {Distributed multimedia streaming systems are increasingly popular due to technological advances, and numerous streaming services are available today. On servers or proxy caches, there is a huge scaling challenge in supporting thousands of concurrent users that request delivery of high-rate, time-dependent data like audio and video, because this requires transfers of large amounts of data through several sub-systems within a streaming node. Since the speed increase for memory accesses does not follow suite with the CPU speed, copy operations can be a severe limiting factor on the streaming performance of off-the-shelf operating systems, which still have only limited support for data paths that have been optimized for streaming despite previous research proposals. We observe furthermore that while CPU speed continues to increase, system call overhead has grown as well, adding to the cost of data movement. In this paper, we therefore revisit the data movement problem and provide a comprehensive evaluation of possible streaming data I/O paths in Linux 2.6 kernel. We have implemented and evaluated enhanced mechanisms and show how to provide support for more ef cient memory usage and reduction of user/kernel space switches for streaming applications.},
  booktitle = {International Conference on Distributed Multimedia Systems (DMS)},
  editor = {Shi-Kuo Chang and Timothy Arndt},
  pages = {228--233},
  publisher = {Knowledge Systems Institute},
  address = {3420 Main Street, Skokie, IL  60076, USA},
  isbn = {1-891706-17-9}
}

@Inproceedings{Oyvind.2005.1,
  author = {Hvamstad, {\O}yvind and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Offloading Multimedia Proxies using Network Processors},
  year = {2005},
  abstract = {In this paper, we present a system that aims at of oading multimedia proxies using network processing technology for applications like media-on-demand and distributed on-line games. In particular, we have designed, implemented and evaluated a proof-of-concept prototype on the Intel IXP1200 network processor. Our results show that the prototype succeeds in of oading the host machine as no data packets have to be processed by the host CPU, and the prototype is able to perform application layer forwarding using only a fraction of the cycles compared to a traditional architecture, where all packets are processed by the host CPU.},
  booktitle = {International Network Conference (INC)},
  editor = {S. M. Furnell and P. S. Dowland and G. Kormentzas},
  pages = {113--120},
  publisher = {University of Plymouth},
  isbn = {960-7475-32-1}
}

@Inproceedings{Horn.2005.3,
  author = {Horn, Geir and S{\o}dring, Thomas},
  title = {SH: A Simple Distributed Bandwidth Broker for Source-Routed Loss-Less Networks},
  year = {2005},
  abstract = {This paper presents SH (Simple Host), a new and relatively simple method to deal with bandwidth brokering within a loss-less and source routed network, where a percentage of the trafﬁc can be regarded as background noise. It achieves this by categorizing two types of ﬂows within a network and provides end-to-endbandwidth brokering for one while accepting that the other simply consumes a limited and de terminate amount of bandwidth. To exemplify our proposal we detail how it could be applied to the recently standard ized Advanced Switching speciﬁcation.

},
  booktitle = {Proceedings of the IASTED International Conference on Computer, Networks and Information Security (CNIS), November 14-16, Phoenix, AZ, USA},
  editor = {M.H. Hamza},
  pages = {133--139},
  publisher = {ACTA Press, ISBN 0-88986-537-X}
}

@Article{Gomez.2006.1,
  author = {G{\a\'o}mez, Maria Engracia and Nordbotten, Nils Agne and Flich, Jose and L{\a\'o}pez, Pedro and Robles, Antonio and Duato, Jose and Skeie, Tor and Lysne, Olav},
  title = {A Routing Methodology for Achieving Fault Tolerance in Direct Networks},
  year = {2006},
  abstract = {Massively parallel computing systems are being built with thousands of nodes. The interconnection network plays a key role for the performance of such systems. However, the high number of components significantly increases the probability of failure. Additionally, failures in the interconnection network may isolate a large fraction of the machine. It is therefore critical to provide an efficient fault-tolerant mechanism to keep the system running, even in the presence of faults. This paper presents a new fault-tolerant routing methodology that does not degrade performance in the absence of faults and tolerates a reasonably large number of faults without disabling any healthy node. In order to avoid faults, for some source-destination pairs, packets are first sent to an intermediate node and then from this node to the destination node. Fully adaptive routing is used along both subpaths. The methodology assumes a static fault model and the use of a checkpoint/restart mechanism. However, there are scenarios where the faults cannot be avoided solely by using an intermediate node. Thus, we also provide some extensions to the methodology. Specifically, we propose disabling adaptive routing and/or using misrouting on a per-packet basis. We also propose the use of more than one intermediate node for some paths. The proposed fault-tolerant routing methodology is extensively evaluated in terms of fault tolerance, complexity, and performance.},
  journal = {IEEE Transactions on Computers},
  volume = {55},
  number = {4},
  pages = {400-415}
}

@Article{Skeie.2006.1,
  author = {Skeie, Tor and Johannessen, Svein and Holmeide, {\O}yvind},
  title = {Timeliness of Real-time IP Communication in Switched Industrial Ethernet Networks},
  year = {2006},
  abstract = {Since its invention at Xerox PARC in 1973, the Ethernet technology has proven to be both robust and adaptable. Through several giant evolution steps Ethernet has become an almost ubiquitous communication technology, spanning from enterprise or local area net-working through high performance backplane interconnects (a very recent initiative) to metropolitan (telecommunication) networking. Being nimble enough to maneuver into new application areas, it is now making inroads in factory communication. 
Automation systems are, however, different from many of the other application areas mentioned, first and foremost since they require real-time performance from the network technology. In this article we will look at some critical aspects of Ethernet as an automa-tion network, usually referred to as Industrial Ethernet. More specifically, we focus on the application-to-application delay and jitter characteristics of such networks, when us-ing Internet protocols such as UDP and TCP. We show the importance of taking control of the latency in the station nodes, since the main communication delays are inside the nodes, and present different solutions for controlling these delays. In particular, a prior-ity-based protocol stack is assessed. Our results show a significant evolution in the appli-cability of real-time Ethernet based IP communication, which is now adequate even for demanding automation applications. In this paper we use substation automation (power distribution) as an example of a demanding automation system
},
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {2},
  number = {1},
  pages = {25-39}
}

@Inproceedings{Gjorven.2006.1,
  author = {Gj{\o}rven, Eli and Eliassen, Frank and Lund, Ketil and Aagedal, Jan {\O}yvind and Staehli, Richard},
  title = {A Mirror Based Approach to Building Reflective, Adaptive Middleware},
  year = {2006},
  abstract = {},
  booktitle = {3. MiNEMA Workshop, Feb 7-8, Leuven, Belgium},
  publisher = {MiNEMA Research Network}
}

@Inproceedings{Cicic.2006.1,
  author = {Cicic, Tarik},
  title = {An Upper Bound on the State Requirements of Link-Fault Tolerant Multi-Topology Routing},
  year = {2006},
  abstract = {Multi-topology routing is an increasingly popular concept in 
computer communications, and can be successfully applied to network fault tolerance. Multi-topology routing implies an increase in routing state requirements proportional to the number of logical topologies used. The network routing state is often subject to stringent system limits and should therefore be deterministically controllable.

In this paper we show that the number of logical topologies needed to guarantee link-fault tolerant multi-topology routing has an upper bound determined by the network's largest minimal cycle. We provide a formal proof of validity of this bound in arbitrary biconnected networks. We also evaluate the quality of this bound, showing that it is reasonably strict.
},
  booktitle = {International Conference on Communications (ICC)},
  editor = {Lutfi Yenel},
  pages = {Electronic},
  publisher = {IEEE},
  isbn = {1-4244-0355-3}
}

@Inproceedings{Amundsen.2006.1,
  author = {Amundsen, Sten Lundesgaard and Eliassen, Frank},
  title = {Combined Resource and Context Model for QoS-aware Mobile Middleware},
  year = {2006},
  abstract = {Mobile computing systems are increasingly difficult to configure, operate, and manage. To reduce operation and maintenance cost plus meet user{\textquoteright}s expectation with respect to QoS, the computing system and its building blocks should be self-managed. When addressing the challenges associated with architecting self-managed mobile computing systems, one must take a holistic view on QoS management and the heterogonous entities in the mobile environment. This paper presents a novel model that combines resources and context elements. It helps us in modelling the environment and design resource and context managers that support functions for adapting the application to changes in the environment. The model is applied on a video streaming application for mobile terminals: i) resource and context elements are classified, ii) their QoS characteristics and context properties are modelled, and iii) weakly integrated resource and context managers are presented and validated. 
},
  booktitle = {19th International Conference on Architecture of Computing Systems},
  editor = {Werner Grass, Bernhard Sick, Klaus Waldschmidt },
  pages = {84--98},
  publisher = {Springer-Verlag GmbH },
  series = {LNCS 3894}
}

@Phdthesis{Rafaelsen.2006.1,
  author = {Rafaelsen, Hans Ole},
  title = {Towards a framework for autonomic support of heterogeneous multimedia applications},
  year = {2006},
  abstract = {},
  school = {Faculty of Science, University of Troms{\o}, Norway}
}

@Misc{Theiss.2005.2,
  author = {Theiss, Ingebj{\o}rg Thelin and S{\o}dring, Thomas and Solheim, {\r A}shild Gr{\o}nstad},
  title = {SIVSS Deliverable 5.3, Simula Switch feasibility and research contribution report},
  year = {2005},
  abstract = {},
  howpublished = {Sixth framework programme, Commission of the European Communities, Proposal/Contract no.: 002075}
}

@Misc{Theiss.2005.3,
  author = {Theiss, Ingebj{\o}rg Thelin and S{\o}dring, Thomas and Solheim, {\r A}shild Gr{\o}nstad and Sem-Jacobsen, Frank Olaf},
  title = {SIVSS Deliverable 9.2, Research report on overall system archicecture resilience},
  year = {2005},
  abstract = {},
  howpublished = {Sixth framework programme, Commission of the European Communities, Proposal/Contract no.: 002075}
}

@Misc{Sodring.2005.1,
  author = {S{\o}dring, Thomas and Solheim, {\r A}shild Gr{\o}nstad and Theiss, Ingebj{\o}rg Thelin},
  title = {SIVSS Deliverable 5.6, Simulation model showing MIN architecture that can deliver over 1 Tb/s performance using single stage 640 Gb/s switches},
  year = {2005},
  abstract = {},
  howpublished = {Sixth framework programme, Commission of the European Communities, Proposal/Contract no.: 002075}
}

@Inproceedings{Frost-urstad.2005.1,
  author = {Frost Urstad, Joan and Davis, Kim and Horn, Geir},
  authorURLs = {http://www.frost-urstad.no and   and /people/geirho},
  title = {Frogs in a wheelbarrow? The role and challenges of the Project Manager throughout the life cycle of a European Union funded R\&D Project from the proposal to the completion stage},
  year = {2005},
  abstract = {The paper highlights the unique role of the project manager engaged in an EU funded R\&D project. The challenges have a dual facet. Project management skill is particularly demanding due to the necessity of pulling together a project proposal based on the wishes and desires of from six to thirty consortium partners. This is in addition to handling the language and cultural barriers of the partners that come from diverse countries and disciplines. Most tasks are conducted in a virtual project setting via the Internet. The project manager must at all times adhere to the rules and regulations of the CEC, the Commission of the European Communities,
which are stringent and uncompromising regarding reporting and project control, as well as release of funds. The Brussels based Project Officer autocratically represents the EC with full decisive power over the project and thus autocratically represents the primary EC stakeholder. Languages and cultural barriers of the partners from diverse countries and disciplines are daunting.},
  booktitle = {PMI Global Congress EMEA 2005, Edinburg, Scotland, May 23-25},
  chapter = {PER 03 Session 2},
  publisher = {Project Management Institute (PMI)},
  address = {EMEA Service Centre, 300, Avenue Tervueren, B-1150 Brussels, Belgium, http://www.pmi.org}
}

@Misc{Horn.2005.4,
  author = {Horn, Geir},
  title = {A New Science for Digital Ecosystems?},
  year = {2005},
  abstract = {The research on Digital Ecosystems is inspired by the evolution of biologic ecosystems. This easily leads to the conclusion that the science needed to analyse and model these system has to be new and does not exist. This talk challenges this view arguing that the toolbox of science build over the ages is sufficient, and it emphasise a thorough approach to build lasting results. The claim is that we do not yet understand the business ecosystems, and consequently we can no transpose it to the digital world. A new theory is needed integrating results from socio-economic research on the role of the regional and national governments in facilitating and stimulating growth; a better understanding of dynamically evolving complex systems; and finally knowledge of what drives business collaboration and the interaction of SMEs with in a global marketplace. },
  howpublished = {http://digital-ecosystems.org/events/2005.05/workshop\_3ya2.html},
  note = {This is an invited talk at the workshop organised by the European Commission 18th May 2005,  CCAB / 3D, Centre Albert Borchette, Rue Froissart 36, Brussels: "Towards a network of digital ecosystems: which technology, which research and which instruments? Review of the technology and research activities needs". The official position paper resulting from the workshop can be found at http://digital-ecosystems.org/events/2005.05/de\_position\_paper\_vf.pdf}
}

@Misc{Horn.2005.5,
  author = {Horn, Geir},
  title = {Themes for the Future... (MADAM \& MUSIC FP7 Position statement)},
  year = {2005},
  abstract = {The position statement postulates that the key areas for research over the next ten years in Framework Programme 7 will be on simplification and reliability of complex systems making them socially accpetable when the focus shifts from the individual user to groups. Making systems self configurable and autonomic may contradict the user's desire to be able to control the digital environment.},
  howpublished = {Misc},
  note = {The presentation was given at the Consultation Workshop on "Future Service Platforms and Software Development Technologies" organised by the European Commission 7 December in Brussels for the Technology Pillar "Software, Grids, security and dependability" of ICT Theme of the FP7 proposal. Participation to the workshop was by invitation only. }
}

@Inproceedings{Horn.2005.6,
  author = {Horn, Geir and Oommen, B. John},
  authorURLs = {/people/geirho and http://www.scs.carleton.ca/\~oommen/},
  title = {Generalised Pursuit Learning Automata for Non-Stationary Environments Applied to the Stochastic Static Mapping Problem},
  year = {2005},
  abstract = {In this paper, we propose the first variable-structure Learning-Automata (LA) based approach to solve the Stochastic Static Mapping Problem (SMP). The problem is known to be NP-hard and involves distributing the processes of a parallel application onto a set of computing nodes. Our solution has salient characteristics novel to both the fields of LA and process
allocation. Thus, while the solution attempts to optimise the inter-process communication costs and the workload allocated to each machine, it achieves this without artificially generating potential pairings which are to be used by our previous fixed-structure LA-based solution. Furthermore, unlike all the reported estimator-based LA, we propose the utilization of the recently introduced weak estimators suitable for non-stationary environments. We report the results obtained by simulating our algorithm on various problems where the number of processes and nodes is "small". In each case, the accuracy of the strategy to converge to a feasible partitioning is remarkable - sometimes even
as high as 96\%. We are currently investigating how the current solution can be enhanced to be rendered scalable.},
  booktitle = {The 11th International Conference on Information Systems Analysis and Synthesis (ISAS 2005) and The 2nd International Conference on Cybernetics and Information Technologies, Systems and Applications (CITSA 2005)},
  editor = {Jos{\a\'e} Aguilar, Hsing-Wei Chu, Johnny St-Amand, Ivan Galkin, Periklis Chantzimisios},
  pages = {91--97},
  publisher = {International Institute of Informatics and Systemics (IIIS)},
  address = {Orlado, Florida, USA July 14-17},
  edition = {Volume 1},
  note = {ISBN 980-6560-42-6}
}

@Inproceedings{Hansen.2006.1,
  author = {Hansen, Audun Fosselie and Cicic, Tarik and Gjessing, Stein},
  title = {Alternative Schemes for Proactive IP Recovery},
  year = {2006},
  abstract = {Recovery at the IP layer has originally been
handled by the slow process of IP re-convergence. As the
dependence on the Internet broadens and real time applications
like VoIP become a common service of the Internet,
fast proactive recovery becomes an important property
of the communication protocols. There are currently two
IETF initiatives for proactive recovery drawing considerable
attention, IP Fast Reroute and Multi-Topology Routing
using Multiple Routing Configurations. In this paper
we evaluate and compare these approaches.},
  booktitle = {2nd Conference on Next Generation Internet Design and Engineering, Valencia, Spain April 3-5},
  editor = {Vicente Casares Giner},
  pages = {1--8},
  publisher = {IEEE},
  isbn = {0-7803-9455-0}
}

@Inproceedings{Hansen.2006.2,
  author = {Hansen, Audun Fosselie and Cicic, Tarik and Engelstad, Paal Einar},
  title = {Profiles and Multi-Topology Routing in Highly Heterogeneous Ad Hoc Networks},
  year = {2006},
  abstract = {This paper points out multi-parameter heterogeneity
as one of the main research challenges for
future ad hoc network scenarios. Current research is
often focusing on dealing with heterogeneity in terms of
only one property, such as battery life or link capacity.
Instead, we argue for a more holistic approach that can
deal with heterogeneity in terms of a number of different
parameters simultaneously. Profiled routing and multitopology
routing are proposed as possible solutions.},
  booktitle = {INFOCOM 2006, Poster and Demo session, Barcelona, Spain April 23-29},
  publisher = {IEEE}
}

@Inproceedings{Thomas.2005.2,
  author = {Thomas, Plagemann Peter and Jon, Andersson and Ovidiu, Drugan Valentin and Vera, Goebel and Carsten, Griwodz and P{\r a}l, Halvorsen and Ellen, Munthe-Kaas and Matija, Puzar and Norun, Sanderson Christine and Katrine, Skjelsvik S},
  title = {Middleware Services for Information Sharing in Mobile Ad-Hoc Networks},
  year = {2005},
  abstract = {},
  booktitle = {Broadband Satellite Communication Systems and the Challenges of Mobility},
  editor = {Gayraud, T.; Mazella, M.; Fernandes, F.P.; Monteiro, E.; Orvalho, J.G.d.M.},
  pages = {225-236},
  publisher = {Springer Publishing Company},
  series = {IFIP International Federation for Information Processing}
}

@Article{Musunoori.2006.1,
  author = {Musunoori, Sharath Babu and Eliassen, Frank},
  title = {The 2nd International Middleware Doctoral Symposium: Quality-Driven Application Service Planning},
  year = {2006},
  abstract = {},
  journal = {IEEE Distributed Systems Online},
  volume = {7},
  number = {3},
  note = {art. no. 0603-o3004}
}

@Inproceedings{Amundsen.2006.2,
  author = {Amundsen, Sten Lundesgaard and Lund, Ketil and Eliassen, Frank},
  title = {Service Plans for Context- and QoS-aware Dynamic Middleware},
  year = {2006},
  abstract = {State-of-the-art context- and QoS-aware dynamic middleware platforms use information about the environment, in order to evaluate alternative configurations of an application and select the one that best meets the user{\textquoteright}s QoS requirements. The specification of the alternatives is prepared at designtime and associated with the software during deployment. From the information and requirements in the specification, the middleware can synthesis, filter, and compare the alternative application configurations. This paper presents a platform independent specification, referred to as service plan, which contains information elements for specifying configurations, dependencies on the environment, and QoS characteristics. The service plan is specified at a conceptual level to ensure that it can be implemented in a wide range of middleware platforms. The paper describes how the concept is used during deployment, instantiation, and reconfiguration. From the implementation and validation the expressiveness and usefulness of the service plan concept is assessed. },
  booktitle = {The Second International Workshop on Services and Infrastructure for the Ubiquitous and Mobile Internet (SIUMI'06)},
  editor = {Antonio Corradi, Philip S. Yu},
  pages = {70--75},
  publisher = {IEEE},
  isbn = {0-7695-2541-5}
}

@Inproceedings{Lysne.2004.1,
  author = {Lysne, Olav and Montanana, Jose Miguel and Pinkston, Timothy Mark and Duato, Jose and Skeie, Tor and Flich, Jose},
  title = {Simple Deadlock-Free Dynamic Network Reconfiguration},
  year = {2004},
  abstract = {},
  booktitle = {High Performance Computing - HiPC 2004: 11th International Conference, Bangalore, India, December 19-22, 2004},
  editor = {Luc Bouge, Victor Prasanna},
  pages = {504-515},
  publisher = {Springer Verlag},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{Amundsen.2006.3,
  author = {Amundsen, Sten Lundesgaard and Lund, Ketil and Eliassen, Frank},
  title = {Utilising Alternative Application Configurations in Context- and QoS-aware Mobile Middleware},
  year = {2006},
  abstract = { State-of-the-art dynamic middleware uses information about the environment in order to evaluate alternative configurations of an application and select one, according to some criteria. In the context of applications sensitive to Quality of Service, we have identified the need for a platform-independent description of configurations that includes non-functional behaviour, and that scales to handle a large number of application configurations. In this paper, we  present a modelling principle and a service plan concept, which together represent such as description. The modelling principle and plan concept extends state-of-the-art with i) a model of the alternative configurations that ensure a minimum of reconfiguration steps, ii) a specification that contains information elements of the configuration, dependencies to the environment, and QoS characteristics, and iii) a platform independent specification. In the paper, we also perform a qualitative assessment of our app  roach, and we describe a proof-of-concept implementation.
},
  booktitle = {6th IFIP International Conference on Distributed Applications and Interoperable Systems (DAIS'06), June 13-16, Bologna, Italy},
  pages = {228--241},
  publisher = {Lecture Notes in Computer Science, Vol. 4025, Springer Verlag }
}

@Inproceedings{Gjorven.2006.2,
  author = {Gj{\o}rven, Eli and Eliassen, Frank and Lund, Ketil and Eide, Viktor S. Wold and Staehli, Richard},
  title = {Self-Adaptive Systems: A Middleware Managed Approach},
  year = {2006},
  abstract = {Recent works on self-adaptivity use a middleware-based approach where the adaptation mechanisms and meta-level information are separated and externalized from the application code. Current solutions generally target individual life-cycle phases of an application in isolation, preventing easy integration of design-time and run-time adaptability.
Integration is needed in order to support the introduction of new adaptive behavior during run-time. Self-adapting systems therefore need
to support both planning, instantiation and maintenance of applications
throughout their life-time.
In this paper we propose middleware managed adaptation, in which services are specified by their behavior, and planned, instantiated and maintained by middleware services in such a way that the behavioral requirements
are satisfied throughout the service life-time. Central to this approach is mirror-based reflection, which supports introspection and intercession on an application, or any service, through all the phases of its life-cycle, including pre-runtime. The mirror of a service may contain
information about its implementation, including the developer's knowledge
about how this implementation will perform in different contexts.
By making this knowledge available to the middleware, we facilitate the
implementation of a wide range of self-adaptive behaviors.},
  booktitle = {2nd IEEE International Workshop on Self-Managed Networks, Systems \& Services (SelfMan 2006)},
  editor = {Alexander Keller,  Jean-Philippe Martin-Flatin},
  publisher = {Springer},
  isbn = {978-3-540-34739-2}
}

@Article{Reinemo.2006.2,
  author = {Reinemo, Sven-Arne and Skeie, Tor and S{\o}dring, Thomas and Lysne, Olav and T{\o}rudbakken, Ola},
  title = {An overview of QoS capabilities in InfiniBand, Advanced Switching Interconnect, and Ethernet},
  year = {2006},
  abstract = {A recent trend in interconnection network technologies is the inclusion of various mechanisms to support a variety of Quality of Service concepts. This has been necessitated by an increasing number of application areas that require some level of performance guarantees from the network for parts of its traffic. In this paper we describe and compare the capabilities and support for Quality of Service of the three most important interconnection network technology standards of today. Equalities between the technologies are explained and differences are clarified.},
  journal = {IEEE Communications Magazine},
  volume = {44},
  number = {7},
  pages = {32--38}
}

@Article{Solheim.2006.1,
  author = {Solheim, {\r A}shild Gr{\o}nstad and Lysne, Olav and Skeie, Tor and S{\o}dring, Thomas and Theiss, Ingebj{\o}rg Thelin and Johnsen, Ian},
  title = {Routing for the ASI Fabric Manager},
  year = {2006},
  abstract = {},
  journal = {IEEE Communications Magazine},
  volume = {44},
  number = {7},
  pages = {39--44}
}

@Book{Eliassen.2006.1,
  editor = {Eliassen, Frank and Montresor, Alberto},
  title = {Distributed Applications and Interoperable Systems},
  year = {2006},
  abstract = {},
  publisher = {Lecture Notes in Computer Science, Vol. 4025, Springer Verlag},
  volume = {4025},
  isbn = {3-540-35126-4}
}

@Inproceedings{Gjorven.2006.3,
  author = {Gj{\o}rven, Eli and Eliassen, Frank and Aagedal, Jan {\O}yvind},
  title = {Quality of Adaptation},
  year = {2006},
  abstract = {},
  booktitle = {SELF - Self-adaptability and self-management of context-aware systems, July 19-21, 2006, Silicon Valley, USA},
  editor = {Petre Dini, Dhouha Ayed, Cosmin Dini, Yolande Berbers},
  publisher = {IEEE},
  isbn = {9780769526539}
}

@Inproceedings{Musunoori.2006.2,
  author = {Musunoori, Sharath Babu and Horn, Geir and Eliassen, Frank and Alia, Mourad},
  title = {On the Challenge of Allocating Service Based Applications in a Grid Environment},
  year = {2006},
  abstract = {},
  booktitle = {International Conference on Autonomic and Autonomous Systems (ICAS 2006), July 19-21, Silicon Valley, USA},
  editor = {Petre Dini, Dhouha Ayed, Cosmin Dini, Yolande Berbers},
  publisher = {IEEE Computer Society Press, ISBN 0-7695-2653-5}
}

@Inproceedings{Hansen.2006.3,
  author = {Hansen, Audun Fosselie},
  title = {Recent Advances in Fast Proactive IP Recovery},
  year = {2006},
  abstract = {},
  booktitle = {third Workshop on Traffic Engineering, Protection and Restoration for NGI},
  publisher = {euroNGI}
}

@Inproceedings{Musunoori.2006.3,
  author = {Musunoori, Sharath Babu and Horn, Geir},
  title = {Ant-Based Approach to the Quality Aware Application Service Partitioning in a Grid Environment},
  year = {2006},
  abstract = {},
  booktitle = {2006 IEEE Congress on Evolutionary Computation (CEC 2006), July 16-21, Vancouver, BC, Canada},
  pages = {2604-2611},
  publisher = {IEEE, ISBN 0-7803-9487-9}
}

@Article{Horn.2006.1,
  author = {Horn, Geir and Kvalbein, Amund and Blomsk{\o}ld, Joakim and Nilsen, Erlend},
  title = {An Empirical Comparison of Generators for Self Similar Simulated Traffic},
  year = {2007},
  abstract = {It is generally recognised that aggregated network traffic is self similar and that self similar traffic models should be used in simulation experiments when assessing the performance of a network. Many generators have been proposed to synthetically produce self similar simulation input; however most of them require the trace length to be known a priori. Four generators that allow continuous generation of self similar time series are evaluated in this work with respect to their ability to reproduce the desired level of self similarity. This extensive investigation uses ten times as many traces and twice the number of parameter values as previously reported. Three of the tested generators perform well but surprisingly the generator supplied with a widely used commercial network simulator is unusable. The reported results indicate that the generator based on multiplexing strictly alternating ON/OFF sources may perform better than generators based on chaotic maps, provided that more than 100 ON/OFF sources can be used. Three estimators for the degree of self similarity of a time series have been evaluated as part of the process, and the only acceptable is based on a Wavelet decomposition of the traffic trace.},
  journal = {Elsevier Performance Evaluation},
  volume = {64},
  number = {2},
  pages = {162--190}
}

@Inproceedings{Reinemo.2006.4,
  author = {Reinemo, Sven-Arne and Skeie, Tor},
  title = {Effective Shortest Path Routing for Gigabit Ethernet},
  year = {2007},
  abstract = {Since its invention at Xerox PARC in 1973, Ethernet technology has proven to be both robust and adaptable. Through several evolutionary steps Ethernet has become an almost ubiquitous communication technology, spanning from local area networking through high performance backplane interconnects (a recent initiative) to metropolitan networking. However, an obstacle still remains for Ethernet to effectively make inroads in application areas such as interconnection and backbone networks. Ethernet's native routing algorithm, the Spanning Tree Protocol, becomes a major performance and utilization bottleneck when network connectivity increases. Since the Spanning Tree Protocol avoids deadlocks and infinitely looping packets by turning any topology into tree, it leaves a large portion of links unused and thus wastes bandwidth. In this paper we address this weakness by proposing a new routing algorithm which achieves the same goals as the Spanning Tree Protocol, but without disabling any links or prohibiting any turns, and at the same time guaranteeing shortest path routing.

Through the use of layered routing we show how to improve performance with respect to both the Spanning Tree Protocol and a more recent proposal called Tree-Based Turn-Prohibition. Extensive simulations show that we are able to increase throughput by a factor of more than 3.5 compared to the Spanning Tree Protocol and a factor of 1.8 compared to Tree-Based Turn-Prohibition. Our concept relies on features introduced in IEEE standards 802.1Q, 802.1D and 802.3x, as well as changes currently discussed in IEEE task forces. We also discuss backwards compatibility toghether with the changes necessary for enabling layered shortest path routing in Ethernet.},
  booktitle = {Proceedings of the IEEE International Conference on Communications 2007},
  editor = {Ivan Andonovic and John Thompson},
  pages = {6419--6424},
  publisher = {IEEE Communications Society},
  isbn = {1-4244-0353-7}
}

@Misc{Hansen.2006.4,
  author = {Hansen, Audun Fosselie and See Notes for complete author list},
  title = {Survey of recent achievements in the resilience methods for multi-layer networks},
  year = {2006},
  abstract = {},
  howpublished = {Euro-Ngi Deliverable D.JRA.3.3.5, 6th Framework Programme, Project no: 507613},
  note = {Author list:

﻿Frank Lehrieder, 	 
R{\"u}diger Martin, 	 
Michael Menth, 	 
Jens Milbrandt, 	 
Paola Iovanna, 	 
Roberto Sabella, 	 
Gianpaolo Oriolo, 	 
Laura Sanit{\a\'a}, 	 
Qitao Gan, 	 
Bjarne E. Helvik, 	 
Audun Fosselie Hansen, 	 
Piotr Cho{\l}da, 	 
Andrzej Jajszczyk
}
}

@Misc{Hansen.2004.1,
  author = {Hansen, Audun Fosselie and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {A Method for Recovery in Packet Networks },
  year = {2004},
  abstract = {},
  howpublished = {Norwegian Network Research Seminar 2004},
  note = {The same content as the Simula Technical Report 2004-13}
}

@Inproceedings{Musunoori.2006.4,
  author = {Musunoori, Sharath Babu and Horn, Geir},
  title = {Intelligent Ant-Based Solution to the Application Service Partitioning Problem in a Grid Environment},
  year = {2006},
  abstract = {This paper presents a decentralised multi-agent method based the metaphor of foraging intelligent ants, for solving the problem of application service partitioning onto the execution nodes of the grid environment such that all services of the application satisfy some minimum quality requirements. Fundamentally this an NP-hard problem. The proposed algorithms have been rigorously tested and evaluated through extensive simulations on randomly generated application services and grid environment. The results show that intelligent ants perform significantly better than what could be achieved with simple unintelligent random ants.},
  booktitle = {The 6th International Conference on Intelligent Systems Design and Applications (ISDA'06), October16-18, Jinan, Shandong, China},
  editor = {Yuehui Chen, Ajith Abraham},
  pages = {416--422},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  isbn = {0-7695-2528-8}
}

@Article{Eide.2006.1,
  author = {Eide, Viktor S. Wold and Granmo, Ole-Christoffer and Eliassen, Frank and Michaelsen, J{\o}rgen Andreas},
  title = {Real-time Video Content Analysis : QoS-aware Application Composition and Parallel Processing},
  year = {2006},
  abstract = {Real-Time content-based access to live video data requires content analysis applications that are able to process video streams in real-time and with an acceptable error rate. Statements such as this express quality of service (QoS) requirements. In general, control of the QoS provided can be achieved by sacrificing application quality in one QoS dimension for better quality in another, or by controlling the allocation of processing resources to the application. However, controlling QoS in video content analysis is particularly difficult, not only because main QoS dimensions like accuracy are nonadditive, but also because both the communication- and the processing-resource requirements are challenging.This article presents techniques for QoS-aware composition of applications for real-time video content analysis, based on dynamic Bayesian networks. The aim of QoS-aware composition is to determine application deployment configurations which satisfy a given set of QoS requirements. Our approach consists of: (1) an algorithm for QoS-aware selection of configurations of feature extractor and classification algorithms which balances requirements for timeliness and accuracy against available processing resources, (2) a distributed content-based publish/subscribe system which provides application scalability at multiple logical levels of distribution, and (3) scalable solutions for video streaming, filtering/transformation, feature extraction, and classification.We evaluate our approach based on experiments with an implementation of a real-time motion vector based object-tracking application. The evaluation shows that the application largely behaves as expected when resource availability and selections of configurations of feature extractor and classification algorithms vary. The evaluation also shows that increasing QoS requirements can be met by allocating additional CPUs for parallel processing, with only minor overhead. },
  journal = {ACM Transactions on Multimedia Communication, Computing and Processing},
  volume = {2},
  number = {2},
  pages = {149--172}
}

@Inproceedings{Sodring.2006.1,
  author = {S{\o}dring, Thomas John and Horn, Geir and Martinez, Raul},
  title = {A statistical approach to traffic management in  source routed  loss-less networks },
  year = {2006},
  abstract = {The evolution of high-performance networks has resulted in the development of new applications with Quality of Service (QoS) requirements. In this paper we review and evaluate the Simple Host(SH) traffic management mechanism that enables QoS provisioning within source-routed loss-less interconnect networks. SH provides statistical QoS guarantees for brokered multimedia traffic while supporting  limited amounts of unbrokered background traffic. The results obtained from simulation show how to configure SH to provide statistical QoS based on jitter and latency requirements in an efficient manner.},
  booktitle = { The 2006 International Conference on High Performance Computing and Communications},
  editor = {Michael Gerndt and Dieter Kranz{\"u}ller},
  pages = {190-199},
  publisher = {Springer},
  address = {New York},
  isbn = {3-540-39368-4}
}

@Inproceedings{Palant.2006.1,
  author = {Palant, Wladimir and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Evaluating dead reckoning variations with a multi-player game simulator},
  year = {2006},
  abstract = {One of the most difficult tasks when creating an online multi-player game is to provide the players with a consistent view of the virtual world despite the network delays. Most current games use prediction algorithms to achieve this, but usually it does not go beyond applying the DIS dead reckoning algorithm proposed in the mid-90s. In this paper we introduce a simulator called GLS that allows us to evaluate different aspects of DIS and its variations. We examine the impact of prediction and clock synchronization on game consistency. We also evaluate the convergence algorithm we introduce here. Furthermore we look into ways for compensating increasing delays to keep the player's view of the game state sufficiently consistent with other players.},
  booktitle = {Network and Operating System Support for Digital Audio and Video (NOSSDAV 2006)},
  editor = {Brian Neil Levine and Mark Claypool},
  pages = {20--25},
  publisher = {ACM Press},
  isbn = {1-59593-285-2}
}

@Inproceedings{Griwodz.2006.1,
  author = {Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {The Fun of using TCP for an MMORPG},
  year = {2006},
  abstract = {Massive multi-player online games have become a popular, fast growing, multi-million industry with a very high user mass supporting hundreds or thousands of concurrent players. In many cases, these games are centralized and every player communicates with the central server through a time-critical unicast event stream. Funcom's Anarchy Online is one of these; it is based on TCP. We find that its kind of traffic has some interesting properties that inspire changes to protocol or architecture. In these game streams, using TCP is not TCP-friendly, using TCP does not have to be slower than using UDP, and only repeated timeouts ruin game experience. Improving the latter in the sender implementation does not impose any remarkable penalty on the network. Alternatively, a proxy architecture for multiplexing could save about 40\% resources at the server, allow congestion control to work and also reduce the lag of the game.},
  booktitle = {Network and Operating System Support for Digital Audio and Video (NOSSDAV 2006)},
  editor = {Brian Neil Levine and Mark Claypool},
  pages = {1--7},
  publisher = {ACM Press},
  isbn = {1-59593-285-2},
  note = {received the best paper award}
}

@Inproceedings{Majewski.2006.1,
  author = {Majewski, Chris and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Translating Latency Requirements into Resource Requirements for Game Traffic},
  year = {2006},
  abstract = {Networked   multi-player  games  constitute   a  demanding   class  of
interactive  distributed   multimedia  applications  with   very  high
commercial  relevance.  As such,  they  attract  a  growing number  of
researchers in multimedia networking.  Most games use a client-server
architecture, mainly to prevent cheating. By analyzing the traffic of such  games, we  confirm that  individual client-server  flows consume relatively little bandwidth.  Thus latency, rather than bandwidth, is the critical parameter when provisioning this class of applications.   In  order  for  commercial  game  services  to  ensure low-latency operation, resource reservation must be explored. In this paper, we investigate options for a DiffServ-style reservation on part of the path between a game server and sets of clients. We show  how a token bucket shaper  can be parameterized based on a target end-to-end latency, and discuss the implications for a network infrastructure.  We use the shaper to quantify the burstiness of game traffic and the correlation between individual flows, with a view to the limitations this imposes on resource reservation for aggregate (multiplexed) flows.
},
  booktitle = {International Network Conference (INC 2006)},
  editor = {Steven Furnell and Paul Dowland},
  pages = {113--120},
  publisher = {University of Plymouth},
  isbn = {1-84102-157-1}
}

@Inproceedings{Halvorsen.2006.1,
  author = {Halvorsen, P{\r a}l and Sunde, {\O}ystein Yri and Petlund, Andreas and Griwodz, Carsten},
  title = {Programmable Subsystems: A Traffic Shaping \& Packet Prioritizing Case Study },
  year = {2006},
  abstract = {Due to a faster speed increase of networks than processors, we have today a trend towards the distribution of functionality and workload for network processing on several processing units. For this purpose, network processors are being developed which are special processor architectures aimed for demanding networking tasks such as backbone routing and switching. In this paper, we investigate the possibility of improving the scalability of intermediate nodes by offloading the packet processing workload on the host, and in particular, we present a traffic shaper and packet prioritizer implemented on the Intel IXP2400.
},
  booktitle = {International Network Conference (INC 2006)},
  editor = {Steven Furnell and Paul Dowland},
  pages = {97--104},
  publisher = {University of Plymouth},
  isbn = {1-84102-157-1}
}

@Inproceedings{Griwodz.2006.2,
  author = {Griwodz, Carsten and Vik, Knut-Helge and Halvorsen, P{\r a}l},
  title = {Multicast Tree Reconfiguration in Distributed Interactive Applications },
  year = {2006},
  abstract = {Communication in highly interactive distributed applications, such as massive multiplayer online games, can often be performed efficiently using multicast, i.e., application level multicast. However, in applications with a very dynamic group management, the multicast tree will have frequent changes, and in applications that have stringent latency requirement, this operation needs to be fast. Current multicast approaches either have no notion of reconfiguration, they do not care about tree reconstruction latency or wrongly assume that this is a fast, atomic operation. In this paper, we have focused on dynamic reconfiguration and have tested different ways for a node to join a tree. Our results show that this is an important issue for the class of  highly interactive distributed applications.
},
  booktitle = {Networking Issues in Multimedia Entertainment (NIME 2006)},
  editor = {Marco Roccetti and Giovanni Pau and Abdennour El-Rhalibi},
  pages = {1219--1223},
  publisher = {IEEE},
  isbn = {1-4244-0086-4}
}

@Book{Chandra.2006.1,
  editor = {Chandra, Surendar and Griwodz, Carsten},
  title = {Multimedia Computing and Networking 2006},
  year = {2006},
  abstract = {This year's MMCN is the 13th conference in the series and we have an
exciting program that comprises a keynote speech by Klaus Hofrichter of
Sony Playstation Corporation, a panel organized by Roger Zimmermann of USC
that addresses the technological and societal challenges of multimedia
sensors, and the presentations of 17 full papers and 6 short papers. We
would like to thank the authors of all submissions to MMCN for
contributing to the conference and allowing us to create this program. We
also want to thank SPIE, IS\&T and ACM SIG Multimedia for making MMCN
possible again this year.

Our publicity co-chairs Andreas Mauthe, Wei Tsang Ooi and Michael Zink did
a great job making the conference known and with the additional support of
the ACM SIG Multimedia, the program committee had a good number of papers
to chose from. The average quality of the submissions was very high, so
that we allowed for an acceptance rate of 25.1\% this year.

Each original submission was reviewed by at least three reviewers,
conflicting review results were addressed in emails discussions, and all
papers that received favorable reviews were intensely discussed during the
program committee's one-day meeting on August 6th that committee members
attended either in person, by video conference, or IP or POTS audio
conference. We would like to express our gratitude to the Computer Science
and Engineering Department of the University of Notre Dame that hosted the
PC meeting, and the Systems Group in particular for providing these most
flexible means of participation and being gracious hosts in all other
respects as well. Our thanks go also to the program committee members and
the external reviewers for their excellent job in evaluating the
submissions, and discussing the evaluations before and during the program
committee meeting.

We have come up with a program that addresses multimedia systems issues
under the topics streaming, distribution, peer-to-peer, and
application-dependent transfer. We hope that you find the program
attractive and the presentations instructive, and that you enjoy MMCN
2006!

Surendar Chandra
Carsten Griwodz
},
  publisher = {SPIE},
  address = {Bellingham, WA, USA},
  volume = {6071},
  isbn = {0-8194-6111-3}
}

@Article{Petrini.2006.1,
  author = {Petrini, Fabrizio and Lysne, Olav and Brightwell, Ron},
  title = {High Performance Interconnects. Guest Editors Introduction},
  year = {2006},
  abstract = {},
  journal = {IEEE Micro},
  volume = {26},
  number = {3},
  pages = {7--9}
}

@Article{Amundsen.2006.4,
  author = {Amundsen, Sten Lundesgaard and Eliassen, Frank},
  title = {A Resource and Context Model for Mobile Middleware},
  year = {2008},
  abstract = {Mobile computing systems are increasingly difficult to configure, operate, and manage. To reduce operation and maintenance cost plus meet user{\textquoteright}s expectation with respect to QoS, the computing system and its building blocks should be self-managed. When addressing the challenges associated with architecting self-managed mobile computing systems, one must take a holistic view on QoS management and the heterogonous entities in the mobile environment. This paper presents a novel model that combines resources and context elements. To illustrate the usefulness of the model, it is applied to a video streaming application by: i) modelling context elements and resources in the target environment, ii) specifying context dependencies and QoS characteristics of the application, and iii) designing weakly integrated resource and context managers. We describe a middleware that uses the developed managers when performing context and QoS management of the application. Specifically we show how the middleware can acquire the required information about the environment to evaluate context dependencies and predict offered QoS of alternative implementations of the application. In order to select the one that can operate in the current environment and that best satisfies given user preferences. },
  journal = {Personal and Ubiquitous Computing, Springer Verlag},
  volume = {12},
  number = {2},
  pages = {143--153}
}

@Inproceedings{Rafaelsen.2006.2,
  author = {Rafaelsen, Hans Ole and Eliassen, Frank and Musunoori, Sharath Babu},
  title = {Towards self-organising distribution structures for streaming media},
  year = {2006},
  abstract = {Efficient multi-receiver delivery of video data over computer networks, such as the Internet is still challenging, due to heterogeneity in involved network technologies, end-node capabilities and receiver preferences. While adaptive and scalable video encoding can solve many of the problems caused by heterogeneity, media gateway systems are needed for performing video filtering and transformation in order to handle cases not solved by scalable coding. The omplexity of such systems make them difficult to manage by humans. Hence streaming media delivery systems, including media gateway systems should be self-managed only guided by policies provided by system managers and users.

In this paper, we describe a media gateway systems framework for developing binding managers for gateway-based stream bindings. These bindings constructs an overlay network for media stream distribution. The framework is open, enabling insertion of specific binding policies that suits particular user preferences. Our framework contributes towards self-management of multi-user streaming services by supporting self-configuration and re-configuration of multi-receiver bindings. It also contributes to easier development of business streaming applications by supporting the plugging in of binding policies targeting particular application domains.We describe the implementation of a prototype of the framework and evaluate the framework by observing and comparing the effect on the property of bindings when different binding policies are applied. Finally we discuss issues of gateway service placement and scalability when bindings are deployed in a large scale infrastructure.},
  booktitle = {8th International Symposium on Distributed Objects and Applications (DOA)},
  editor = {Robert Meersman, Zahir Tari},
  pages = {1825--1842},
  publisher = {Springer Verlag},
  series = {Lecture Notes in  Computer Science},
  isbn = {978-3-540-48274-1}
}

@Inproceedings{Alia.2006.1,
  author = {Alia, Mourad and Horn, Geir and Eliassen, Frank and Khan, Mohammad Ullah and Fricke, Rolf and Reichle, Roland},
  title = {A Component-based Planning Framework for Adaptive Systems},
  year = {2006},
  abstract = {In recent times, many researchers have focused on designing generic
and reusable middlewares to overcome the complexity in building adaptive systems. There is a general agreement that the openness provided by componentbased approahes coupled with reflection mechanisms is the minimum prerequisites for supporting dynamic reconfigurations. However, this is not sufficient to implement the heart of the adaptation loop namely the decision making on the required reconfiguration that adapts the system in a given context. In this regard, this paper proposes a planning framework that subsumes and automates the decision-making in reflective component-based adaptive systems. The salient feature of this framework is to model the variability of the adaptive system as a set of variation points at which alternative component compositions and implementations
can be selected. to form an application configuration. The selection
of a feasible configuration in a given context is based on the concept of component wise utility functions that estimates the user benefit of including a specific implementation alternative at a variation point in the composition. We show that the selection problem can be modelled as a multi constraint shortest path that
can be found in polynomial time. Our approach is validated through a real world example implementing adaptive scenarios in the domain of mobile computing.},
  booktitle = {8th International Symposium on Distributed Objects and Applications (DOA)},
  editor = {Robert Meersman and Zahir Tari},
  pages = {1686--1704},
  publisher = {Springer},
  isbn = {3-540-48274-1}
}

@Inproceedings{Sem-jacobsen.2006.1,
  author = {Sem-Jacobsen, Frank Olaf and Skeie, Tor and Lysne, Olav and Duato, Jose},
  title = {Dynamic Fault Tolerance with Misrouting in Fat Trees},
  year = {2006},
  abstract = {Fault tolerance is critical for efficient utilisation of large computer systems.  Dynamic fault tolerance allows the network to remain available through the occurance of faults as opposed to static fault tolerance which requires the network to be halted to reconfigure it.  Although dynamic fault tolerance may lead to less efficient solutions than static fault tolerance, it allows for a much higher availability of the system.  In this paper we devise a dynamic fault tolerant adaptive routing algorithm for the fat tree, a much used interconnect topology, which relies on misrouting around link faults. We show that we are guaranteed to tolerate any combination of less than (num\_switch\_ports/2) link faults without the need for additional network resources for deadlock freedom. There is also a high probability of tolerating an even larger number of link faults. Simulation results show that network performance degrades very little when faults are dynamically tolerated.},
  booktitle = {Proceedings of the International Conference on  Parallel Processing (ICPP)},
  editor = {Wu-chi Feng},
  pages = {33-45},
  publisher = {IEEE Computer Society},
  isbn = {22}
}

@Inproceedings{Sem-jacobsen.2006.2,
  author = {Sem-Jacobsen, Frank Olaf and Lysne, Olav and Skeie, Tor},
  title = {Combining Source Routing and Dynamic Fault Tolerance},
  year = {2006},
  abstract = {An increasing amount of current and emerging interconnect technologies rely on source routing to forward packets through the network.  It is therefore important to develop methods for fault tolerance that are well suited for source routed networks.  Dynamic fault tolerance allows the network to remain available through the occurrence of faults, as opposed to static fault tolerance which requires the network to be halted to reconfigure it. Source routing readily supports the source node choosing a different path when a fault occurs, but using this approach, packets already in the network will be lost.  Local dynamic fault tolerance, where the packet is routed around the fault locally, would prevent much of the traffic being lost during failures, but this is cumbersome to achieve in source routed networks since packets encountering a fault will need to follow a path different from that encoded in the packet header.  In this paper we present a mechanism to achieve local dynamic fault tolerance in source routed fat trees, a topology that has widespread use in supercomputer systems, and compare it with endpoint dynamic fault tolerance. We also show that by combining the two approaches we achieve performance superior to any of the two individually.},
  booktitle = {Proceedings of The 18'th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
  editor = {Alberto F. De Souza, Rajkumar Buyya, and Wagner Meira Jr.},
  pages = {151--158},
  publisher = {IEEE Computer Society},
  address = {Washington, DC, USA},
  isbn = {0-7695-2704-3}
}

@Inproceedings{Bai.2006.1,
  author = {Bai, Aleksander and Selvig, Bj{\o}rn and Skeie, Tor and Engelstad, Paal Einar},
  title = {A Class Based Dynamic Admitted Time Limit Admission Control Algorithm for 802.11e EDCA},
  year = {2006},
  abstract = {This paper presents a class based dynamic admission control algorithm for the IEEE 802.11e Enhanced Distributed Channel Acccess (EDCA) standard. The strength of our admission control is the dynamic and flexibility of the algorithm, which adapts to the situation and thus achieves higher throughput than other admission controls for 802.11 EDCA. The achievements of our admission control are presented and evaluated. Class and flow utilization is discussed before a special case of our admission control algorithm aimed at flow utilization is given.},
  booktitle = {Proceedings of the 6th International Workshop on Applications and Services in Wireless Networks (ASWN)},
  editor = {Stefan Arbanowski},
  pages = {243-249},
  publisher = {Fraunhofer FOKUS},
  isbn = {ISBN 10:3-8167-7111-4, ISBN 13:978-3-8167-7111-1}
}

@Inproceedings{Bai.2006.2,
  author = {Bai, Aleksander and Skeie, Tor and Engelstad, Paal Einar},
  title = {On the Use of Rate Configuration in the Interoperation between DiffServ and 802.11e EDCA},
  year = {2006},
  abstract = {This paper investigates rate configuration of the Expedited Forwarding (EF) class of Differentiated Services (DiffServ) when used with 802.11e EDCA. The rate configuration problem is presented, and several approaches are tested and evaluated in order to solve the problem. Results reveal that the contention window makes rate configuration very hard for the highest priority class (AC\_VO) in 802.11e EDCA. Our evaluations show that 802.11e EDCA is not able to conform to DiffServ{\textquoteright}s EF PHB specifications.},
  booktitle = {Proceedings of the 15th IST Mobile \& Wireless Communications},
  editor = {Emmanuel N. Protonotarios},
  publisher = {National Technical University of Athens},
  address = {Heroon Polytechniou, 15773 Athens, Greece},
  isbn = {NA}
}

@Misc{Holmeide.2006.1,
  author = {Holmeide, {\O}yvind and Skeie, Tor},
  title = {Zeitsynchronisierung im Indudusrtial Ethernet},
  year = {2006},
  abstract = {In industriellen automatisierungsanwendungen begrenzt der latenzzeit-jitter den einsatz von Ethernet-netzwerken bei der {\"u}bertragung der datenpakete im innern der switches. Je nach netzwerklast, gr{\"o}sse des datenpakets und anzahl der switches zwischen server und client kann dieser bis zu mehreren ms betragen. Die zur automatisierung erforderliche pr{\"a}zise datenauswertung, die z.B. zur synchronisierung mehrerer achsen in montargemaschinen ben{\"o}tigt wird, ist dadurch nicht m{\"o}glich. Eine l{\"o}sung f{\"u}r dieses problem sund pr{\"a}zisions-Ethernet-switches mit zeitstempeln. Der vorliegende artikel besschreibt die prinzipien des SNTP/NTP-Internet-zeitprotokolls und dessen implementierung in einem Ethernet-switch und einem von Ethernet aktivierten endknoten zum erhalt einer von der netzwerklast unabh{\"a}ngigen zeitgenauigkeit von unter 1 micro second. },
  howpublished = {etz - Elektrotechnik und Automation, VDE Verlag GMBH, Heft 8/2006}
}

@Techreport{Kvalbein.2006.2,
  author = {Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein},
  title = {Routing Efficiency with Link Failures using Multiple Routing Configurations},
  year = {2006},
  abstract = {The slow convergence of IGP routing protocols after a topology change
has led to several proposals for proactive recovery schemes in IP
networks. These proposals are limited to guaranteeing loop-free
connectivity after a link or node failure, and do not take into
account the resulting load distribution in the network. This can lead
to congestion and packet drops. In this work, we show how a good load
distribution can be achieved in pure IP networks immediately after a
link failure, when Multiple Routing Configurations (MRC) is used as a
fast recovery mechanism. This paper is the first attempt to improve
the load balancing when a proactive recovery scheme is used.  Unlike
load balancing methods used with normal IP rerouting, our method does
not compromise on the routing performance in the failure free
case. Our method is evaluated using simulations on several real and
synthetically generated network topologies. The evaluation shows that
our method yields good routing performance, making it feasible to use
MRC to handle transient network failures.},
  institution = {Simula Research Laboratory},
  number = {02-2006}
}

@Mastersthesis{Alia.2001.1,
  author = {Alia, Mourad},
  title = {Requ{\^e}tes sur Objets R{\a\'e}partis},
  year = {2001},
  abstract = {},
  school = {UFR-IMA (Informatique et Math{\a\'e}matiques Appliqu{\a\'e}s), Universtit{\a\'e} Joseph Fourier de Grenoble}
}

@Article{Theiss.2006.1,
  author = {Theiss, Ingebj{\o}rg Thelin Thelin and Lysne, Olav},
  title = {FRoots, A Fault Tolerant and Topology Agnostic Routing Technique},
  year = {2006},
  abstract = {Existing solutions for fault tolerant routing in interconnection networks either work for only one given regular topology, or they require slow and costly network reconfigurations which cannot allow full and continuous network access. In this paper we present FRroots, a routing method for fault tolerance in topology-agnostic network technologies. Our method is based on redundant paths, and it can handle single dynamic faults without the need for sending control messages. In the fault-free networks under non-uniform traffic it performs comparable to, or even better than, topology specific and non-fault tolerant routing algorithms in regular networks like meshes and tori. FRoots does not require any other features in the switches than a flexible routing table, and a modest number of virtual channels. For that reason it can be directly applied to several present day technologies like InfiniBand and Advanced Switching.},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {17},
  number = {10},
  pages = {1136-1150}
}

@Inproceedings{Alia.2006.2,
  author = {Alia, Mourad and Eliassen, Frank and Hallsteinsen, Svein and Stav, Erlend},
  title = {MADAM: Towards a Flexible Planning-based Middleware (extended abstract)},
  year = {2006},
  abstract = {By exploiting software components, component frameworks and
architectural reflection, the European IST project MADAM aims
to overcome the complexity of developing mobile context-aware
adaptive systems. The MADAM middleware defines a domain
and platform-independent flexible reference architecture that
supports dynamic reconfiguration of both applications and the
middleware itself. Furthermore it implements flexible context
monitoring, adaptation planning and dynamic reconfiguration
frameworks that embodies much of the complex logic of dynamic
adaptation.},
  booktitle = {ICSE 2006  Workshop on Software Engineering for Adaptive and Self-Managing Systems (SEAMS), May 21-22, 2006, Shanghai, China},
  editor = {Betty H. C. Chang, Rogerio de Lemos, David Garlan},
  pages = {96--96},
  publisher = {ACM Press},
  isbn = {1-59593-403-0}
}

@Misc{Skeie.2003.1,
  author = {Skeie, Tor and Johannessen, Svein and Holmeide, {\O}yvind},
  title = {Introducing End-to-End Priority Across Switched Ethernet},
  year = {2003},
  abstract = {},
  howpublished = {Industrial Ethernet Book (IEB), 14:10-16}
}

@Misc{Holmeide.2001.1,
  author = {Holmeide, {\O}yvind and Skeie, Tor},
  title = {Switched Synchronization},
  year = {2001},
  abstract = {},
  howpublished = {Industrial Ethernet Book (IEB), 7:24-27}
}

@Misc{Holmeide.2001.2,
  author = {Holmeide, {\O}yvind and Skeie, Tor},
  title = {VoIP Drives the Realtime Ethernet},
  year = {2001},
  abstract = {},
  howpublished = {Industrial Ethernet Book (IEB), 5}
}

@Misc{Alia.2002.1,
  author = {Alia, Mourad},
  title = {MEDOR: Requ{\^e}tes sur objets r{\a\'e}partis},
  year = {2002},
  abstract = {},
  howpublished = {S{\a\'e}minaire LIFO (Laboratoire d'Informatique Fondamentale d'Orl{\a\'e}ans)}
}

@Inproceedings{vikACMM05,
  author = {Vik, Knut-Helge},
  title = {Game state and event distribution using proxy technology and application layer multicast},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of the ACM International Multimedia Conference (ACM MM)},
  editor = {Hongjiang Zhang, Tat-Seng Chua},
  pages = {1041--1042},
  publisher = {ACM Press},
  isbn = {1-59593-044-2}
}

@Article{halvorsen-dsonline-2004a,
  author = {Halvorsen, Pl and Griwodz, Carsten and Goebel, Vera and Lund, Ketil and Plagemann, Thomas and Walpole, Jonathan},
  title = {Storage System Support for Continuous-Media Applications, Part 1: Requirements and Single-Disk Issues},
  year = {2004},
  abstract = {},
  journal = {IEEE Distributed Systems Online},
  volume = {5},
  number = {1}
}

@Article{halvorsen-dsonline-2004b,
  author = {Halvorsen, Pl and Griwodz, Carsten and Goebel, Vera and Lund, Ketil and Plagemann, Thomas and Walpole, Jonathan},
  title = {Storage System Support for Continuous-Media Applications, Part 2: Multiple Disks, Memory, and Integration},
  year = {2004},
  abstract = {},
  journal = {IEEE Distributed Systems Online},
  volume = {5},
  number = {2}
}

@Inproceedings{Eliassen.2006.2,
  author = {Eliassen, Frank and Gj{\o}rven, Eli and Eide, Viktor S. Wold and Michaelsen, J{\o}rgen Andreas},
  title = {Evolving Self-Adaptive Services using Planning-Based Reflective Middleware},
  year = {2006},
  abstract = {Self-adaptive systems often use a middleware-based approach where adaptation mechanisms and policies are separated and externalized from the application code. Such separation facilitates the independent analysis of application and adaptation. In the QuA middleware, we use mirror-based reflection and service planning to support the development and execution of self-adaptive systems. A mirror provides meta information about a service{\textquoteright}s behavior and implementation hroughout all life-cycle phases, including its performance in ifferent contexts. Service planning supports dynamic discovery, utility-based and context-aware evaluation, and selection of alternative implementations of a given service.

Here we argue that the QuA middleware is also able to support certain forms of evolution of adaptive systems. Since in QuA new implementation alternatives or updated versions of software are automatically discovered and considered during service planning, evolution both during run time and load time is supported. Experimental
results from evolving a state-of-the-art adaptive media streaming application using our middleware are also presented.},
  booktitle = {The 5th annual Workshop on Adaptive and Reflective Middleware (ARM 2006)},
  editor = {Geoff Coulson, Nalini Venkatasubramanian},
  pages = {1--6},
  publisher = {ACM Press},
  isbn = {1-59593-419-7}
}

@Inproceedings{Eide.2007.1,
  author = {Eide, Viktor S. Wold and Eliassen, Frank and Michaelsen, J{\o}rgen Andreas and Jensen, Frank},
  title = {Fine Granularity Adaptive Multi-Receiver Video Streaming},
  year = {2007},
  abstract = {Efficient delivery of video data over computer networks has been
studied extensively for decades. Still, multi-receiver video delivery
is challenging, due to heterogeneity and variability in network
availability, end node capabilities, and receiver preferences. Our
earlier work has shown that content-based networking is a viable
technology for fine granularity multi-receiver video streaming. By
exploiting this technology, we have demonstrated that each video
receiver is provided with fine grained and independent selectivity
along the different video quality dimensions region of interest,
signal to noise ratio for the luminance and the chrominance planes,
and temporal resolution. Here we propose a novel adaptation scheme
combining such video streaming with state-of-the-art techniques from
the field of adaptation to provide receiver-driven multi-dimensional
adaptive video streaming. The scheme allows each client to
individually adapt the quality of the received video according to its
currently available resources and own preferences. The proposed
adaptation scheme is validated experimentally. The results demonstrate
adaptation to variations in available bandwidth and CPU resources
roughly over two orders of magnitude and that fine grained adaptation
is feasible given radically different user preferences.},
  booktitle = {Fourteenth annual Conference on Multimedia Computing and Networking (MMCN07)},
  editor = {Roger Zimmermann and Carsten Griwodz},
  publisher = {SPIE},
  isbn = {9780819466174}
}

@Inproceedings{Reinemo.2007.1,
  author = {Reinemo, Sven-Arne and Skeie, Tor and Mej{\a`\i}a, Andr{\a`e}s and Flich, Jos{\a`e} and Duato, Jos{\a`e}},
  title = {Boosting Ethernet Performance by Segment-Based Routing},
  year = {2007},
  abstract = {In this paper we embed an efficient topology agnostic routing algorithm with fault tolerance capabilities into back-pressured Ethernet technology. This makes it possible to use off-the-shelf equipment to build cost-effective systems with an efficient use of all network components. This stands in contrast to the inefficient use of network resources (links) supported by the Spanning Tree Protocol (STP). The Segment-Based Routing Algorithm (SR) is a deterministic routing algorithm that achieves high performance without the use of virtual channels. Furthermore, it is topology agnostic, meaning it can handle any topology and any combination of faults derived from the original topology when combined with static reconfiguration. Through simulations we verify an overall improvement in throughput by a factor of 1.2 to 10.0 compared to the conventional Ethernet routing algorithm, the STP, and other topology agnostic routing algorithms such as Up*/Down* and Tree-based Turn-prohibition, which both are  applicable to Ethernet. },
  booktitle = {Proceedings of the 15th Euromicro Conference on Parallel, Distributed and Network-based Processing (PDP 2007)},
  editor = {Pasqua D'Ambra and Mario Rosario Guarracino},
  pages = {55--62},
  publisher = {IEEE Computer Society Press},
  isbn = {0-7695-2784-1}
}

@Article{Musunoori.2007.1,
  author = {Musunoori, Sharath Babu and Horn, Geir},
  title = {Application Service Placement in Stochastic Grid Environments Using Learning and Ant Based Methods},
  year = {2007},
  abstract = {Achieving acceptable application performance in a grid environment remains a difficult challenge. In particular this is true for applications composed of services requiring some quality requirements to be fulfilled in order to satisfy users' needs. The problem considered here is partitioning of application services onto the available execution nodes of grid environment in such a way that they satisfy some minimum quality requirements. Fundamentally this is an NP-hard problem. In this paper we  propose three algorithms that are the first ones to be based on the concepts of learning automata and the metaphor of foraging ants. The algorithms naturally follow a decentralised multi-agent method for solving the service partitioning problem. Moreover they establish a distributed problem solving mechanism that does not require the use of a central controller. The proposed algorithms have been rigorously tested and evaluated through extensive simulations on randomly generated application services and grid environments. The results indicate that learning is an essential component to achieve scalability and efficiency in nature inspired systems.},
  journal = {Multiagent and Grid Systems},
  volume = {3},
  number = {1},
  pages = {19-41},
  note = {Special Issue on Nature Inspired Systems for Parallel, Asynchronous and Decentralised Environments}
}

@Inproceedings{Musunoori.2007.2,
  author = {Musunoori, Sharath Babu and Horn, Geir},
  title = {Unintelligent Guided Ant-Based Solution to the Application Service Mapping Problem in a Grid Environment},
  year = {2007},
  abstract = {},
  booktitle = {International Conference on Computing: Theory and Applications (ICCTA 2007), March 5-7, Kolkata, India},
  editor = { },
  pages = {170-175},
  publisher = {IEEE Computer Society Press},
  address = {Los Alamitos, CA, USA},
  isbn = {0-7695-2770-1}
}

@Book{Zhang.2006.2,
  editor = {Zhang, Y and Luo, J and Hu, H},
  title = {Wireless Mesh Networking: Architectures, Protocols and Standards},
  year = {2006},
  abstract = {},
  publisher = {Auerbach Publications, Taylor \& Francis Group},
  address = {New York},
  edition = {first},
  isbn = {0849373999}
}

@Inproceedings{Kvalbein.2007.1,
  author = {Kvalbein, Amund and Cicic, Tarik and Gjessing, Stein},
  title = {Post-Failure Routing Performance with Multiple Routing Configurations},
  year = {2007},
  abstract = {The slow convergence of IGP routing protocols after a topology change
has led to several proposals for proactive recovery schemes in IP
networks. These proposals are limited to guaranteeing loop-free
connectivity after a link or node failure, and do not take into
account the resulting load distribution in the network. This can lead
to congestion and packet drops. In this work, we show how a good load
distribution can be achieved in pure IP networks immediately after a
link failure, when Multiple Routing Configurations (MRC) is used as a
fast recovery mechanism. This paper is the first attempt to improve
the load balancing when a proactive recovery scheme is used.  Unlike
load balancing methods used with normal IP rerouting, our method does
not compromise on the routing performance in the failure free
case. Our method is evaluated using simulations on several real and
synthetically generated network topologies. The evaluation shows that
our method yields good routing performance, making it feasible to use
MRC to handle transient network failures.},
  booktitle = {IEEE INFOCOM 2007},
  editor = {George Kesidis, Eytan Modiano and R Srikant},
  publisher = {IEEE},
  isbn = {1-4244-1047-0}
}

@Inproceedings{Vik.2006.1,
  author = {Vik, Knut-Helge and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Applicability of Group Communication for Increased Scalability in MMOGs},
  year = {2006},
  abstract = {Massive multiplayer online games (MMOGs) are today the driving factor for the development of distributed interactive applications, and they are increasing in size and complexity. Even a small MMOG supports thousands of players, the biggest support hundreds of thousands of concurrent players. Since they are typically built as strict client-server systems, they suffer from the inherent scalability problem of the architecture. Computing power and bandwidth limitations close to the server limit the possible number of players. Also, the latency of communication between players through the server will be higher than using direct communication. In the paper, we address these issues and investigate improvement options.

A typical MMOG consists of a virtual world with a concept of time and space that is similar to the real world. In it, players are represented by avatars. Only subsets of these avatars interact with each other at any given time. This allows us to divide them into groups, and communication among group members becomes a multi-party communication problem. Thus, to reduce resource consumption, we compare the performance of several algorithms for group communication with the current central server approach. We use overlay multicast as the means of providing group communication, and research algorithms for creating shortest path trees, spanning trees, delay-bounded spanning trees and, more specific, applying Steiner tree heuristics. 

Our experimental results indicate that different approaches are useful to reduce resource consumption while achieving a good perceived quality under varying conditions, such as frequent changes in group membership and the demand for low latency.

},
  booktitle = {Network \& System Support for Games (NetGames 2006)},
  editor = {Adrian David Cheok, Yutaka Ishibashi, Stephane Natkin and Keiichi Yasumoto},
  pages = {1--12},
  publisher = {ACM Press},
  edition = {Fifth},
  isbn = {1-59593-589-4}
}

@Inproceedings{Pedersen.2006.1,
  author = {Pedersen, Jon and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Considerations for SCTP Retransmission Delays for Thin Streams},
  year = {2006},
  abstract = {The popularity of distributed interactive applications has exploded in
the last few years. For example, massive multi-player online games
have become a fast growing, multi-million industry with a very high
user mass supporting hundreds or thousands of concurrent players.
Today, such games are usually client-server applications that use TCP
for time-dependent communication. Similar multimedia applications also
choose TCP frequently. Very thin data streams are sent over
each of these TCP connections, which means that they consume very
little bandwidth. TCP has several shortcomings with respect to the
latency requirements of such thin streams because of its retransmission
handling.

An alternative to TCP may be SCTP which was developed
to answer the requirements for signaling transport. SCTP has
subsequently also been considered more appropriate than TCP for
congestion controlled streaming of other time-dependent data.
Important reasons are its maintenance of packet boundaries and partial
reliability.

In this paper, we evaluate the performance of the Linux SCTP
implementation for thin streams. Like others before, we identify
latency challenges. We also propose some enhancements for reducing the
latency compared to the original Linux implementation. We argue for
separate handling of thin and thick data streams in SCTP.
},
  booktitle = {Local Computer Networks (LCN 2006)},
  editor = {Ken Christensen and Matthias Frank},
  pages = {135 -- 142},
  publisher = {IEEE},
  isbn = {1-4244-0419-3},
  note = {(best paper award)}
}

@Inproceedings{Griwodz.2006.3,
  author = {Griwodz, Carsten and Johnsen, Frank T and Rekkedal, Simen and Halvorsen, P{\r a}l},
  title = {Caching of Interactive Multiple Choice MPEG-4 Presentations},
  year = {2006},
  abstract = {},
  booktitle = {International Workshop on Multimedia Systems and Networking (WMSN 2006)},
  editor = {Hossam Hassanein and Golden G. Richards III},
  pages = {659--666},
  publisher = {IEEE},
  edition = {25th},
  isbn = {1-4244-0198-4},
  note = {In proceedings of IEEE International Performance Computing and Communications Conference (IPCCC 2006)}
}

@Inproceedings{Johnsen.2006.1,
  author = {Johnsen, Frank T and Hafs{\o}e, Trude and Griwodz, Carsten},
  title = {Analysis of Server Workload and Client Interactions in a News-on-Demand Streaming System},
  year = {2006},
  abstract = {Internet services are continuing to grow along with the number of
clients connecting to the Internet and the transfer rates of their
connections. News is one of the main areas of usage of clients today.  We
investigate several aspects of streaming in News-on-Demand services on the
Internet today by analyzing log files of a news service from one of Norway's
largest online newspapers.  These logs span almost two years of streaming,
with a total of 4.6 million client requests after cleaning the logs. These
requests are for over 3000 unique audio and video files.

In this paper we investigate both short-term and long-term effects of client
usage of the streaming material.  We examine the users' interaction with the
content, in particular user behavior and object popularity in terms of news
streaming media. By comparing our results with earlier work, we found that
variations in server load depends strongly on local culture.  Furthermore,
we found that there are slight variations between client usage of the audio
and video material.  We also investigated traffic peaks, and how rapidly
they form.
},
  booktitle = {International Symposium on Multimedia (ISM 2006)},
  editor = {Philipp Sheu and Max M{\"u}hlh{\"a}user and Kinji Mori},
  pages = {724--727},
  publisher = {IEEE},
  isbn = {0-7695-2746-9},
  note = {accepted for poster}
}

@Inproceedings{Chen.2007.1,
  author = {Chen, Y and Yuen, C and Zhang, Y and Zhang, Z},
  title = {Cross-Correlation Analysis of Generalized Distributed Antenna Systems with Cooperative Diversity},
  year = {2007},
  abstract = {},
  booktitle = {the 65th IEEE Vehicular Technology Conference (VTC2007-Spring)},
  editor = {IEEE},
  pages = {309-313},
  publisher = {IEEE},
  isbn = {1-4244-0266-2}
}

@Inproceedings{Chen.2007.2,
  author = {Chen, Y and Yuen, C and Zhang, Y and Zhang, Z},
  title = {Diversity Gains of Generalized Distributed Antenna Systems with Cooperative Users},
  year = {2007},
  abstract = {},
  booktitle = {IEEE Wireless Communications and Networking Conference (WCNC), WCNC'2007},
  editor = {IEEE},
  pages = {798-803},
  publisher = {IEEE},
  isbn = {1-4244-0659-5}
}

@Inproceedings{Zhang.2007.1,
  author = {Zhang, Y and Zheng, J and Li, W},
  title = {A Simple and Effective QoS Differentiation Scheme in IEEE 802.16 WiMAX Mesh Networking},
  year = {2007},
  abstract = {},
  booktitle = { IEEE Wireless Communications and Networking Conference (WCNC), WCNC'2007},
  editor = {IEEE},
  pages = {3216-3220},
  publisher = {IEEE},
  isbn = {1-4244-0659-5}
}

@Inproceedings{Zhang.2007.2,
  author = {Zhang, Y},
  title = {Call Admission Control in Wireless Networks over Rayleigh Fading Channel},
  year = {2007},
  abstract = {},
  booktitle = { IEEE Wireless Communications and Networking Conference (WCNC), WCNC'2007},
  editor = {IEEE},
  pages = { 3030-3034},
  publisher = {IEEE},
  isbn = {1-4244-0659-5}
}

@Inproceedings{Zhang.2007.3,
  author = {Zhang, Y},
  title = {Performance Modeling of Energy Management Mechanism in IEEE 802.16e Mobile WiMAX},
  year = {2007},
  abstract = {},
  booktitle = { IEEE Wireless Communications and Networking Conference (WCNC), WCNC'2007},
  editor = {IEEE},
  pages = {3205-3209},
  publisher = {IEEE},
  isbn = {1-4244-0659-5}
}

@Inproceedings{Musunoori.2007.3,
  author = {Musunoori, Sharath Babu and Horn, Geir},
  title = {Co-ordination in Intelligent Ant-Based Application Service Mapping in Grid Environments},
  year = {2007},
  abstract = {},
  booktitle = {IEEE Swarm Intelligence Symposium (SIS 2007)},
  editor = { },
  pages = {303-309},
  publisher = {IEEE Computer Society Press},
  isbn = {1-4244-0708-7}
}

@Inproceedings{Mora.2006.1,
  author = {Mora, Gaspar and Flich, Jos{\a\'e} and Duato, Jos{\a\'e} and Lopez, Pedro and Lysne, Olav},
  title = {Towards and Efficient Switch Architecture for High-Radix Switches},
  year = {2006},
  abstract = {},
  booktitle = {ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
  editor = {Michel Dubois and Will Eatherton},
  pages = {11-20},
  publisher = {ACM Press},
  isbn = {1-59593-580-0}
}

@Inproceedings{Palant.2006.2,
  author = {Palant, Wladimir and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Consistency requirements in Multiplayer Online Games},
  year = {2006},
  abstract = {Multi-player online games are becoming increasingly popular as
broadband internet connections replace the old modems. However,
while available bandwidth grows steadily according to Moore's law,
the latency of the internet connections remains almost constant,
making it difficult to maintain a consistent game state over a large
number of clients that have to be synchronized with each other and
with the server(s). This paper introduces a possible solution to
the problem by defining the necessary level of consistency through
user's perception of the game. While the resulting set of
requirements is somewhat difficult to formalize, it is not too
restrictive and leaves many options open, some of which are
discussed here. Ideally a game where all the requirements are met
will appear like a local game to the user.},
  booktitle = {Network \& System Support for Games (NetGames 2006)},
  editor = {Adrian David Cheok, Yutaka Ishibashi, Stephane Natkin and Keiichi Yasumoto},
  pages = {1--4},
  publisher = {ACM Press},
  isbn = {1-59593-589-4}
}

@Inproceedings{Palant.2006.3,
  author = {Palant, Wladimir and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {GLS: Simulator for Online Multi-Player Games},
  year = {2006},
  abstract = {  One of the most difficult tasks when creating an online multi-player game is
  to provide the players with a consistent view of the virtual world despite the
  network delays. Most current games use prediction algorithms to achieve this,
  however measuring the effect of different approaches is difficult. To solve
  this problem we introduce a simulator called GLS that gives us a
  fully controlled environment and allows large-scale experiments to evaluate
  different aspects of the algorithms.},
  booktitle = {ACM Multimedia (MM 2006)},
  editor = {Kenji Mase},
  pages = {805--806},
  publisher = {ACM Press},
  isbn = {1-59593-498-7}
}

@Book{Simula.ND.2,
  editor = {Hu, H and Zhang, Y and Luo, J},
  title = {Distributed Antenna Systems: Open Architecture for Future Wireless Communications},
  year = {2007},
  abstract = {},
  publisher = {Auerbach Publications, Taylor\&Francis Group},
  address = {New York},
  edition = {first},
  isbn = {1420042882}
}

@Article{Simula.ND.4,
  author = {Kvalbein, Amund and Hansen, Audun Fosselie and Cicic, Tarik and Gjessing, Stein and Lysne, Olav},
  title = {Multiple Routing Configurations for Fast IP Network Recovery},
  year = {2008},
  abstract = {  As the Internet takes an increasingly central role in our
  communications infrastructure, the slow convergence of routing
  protocols after a network failure becomes a growing problem. To
  assure fast recovery from link and node failures in IP networks, we
  present a new recovery scheme called Multiple Routing Configurations
  (MRC). Our proposed scheme guarantees recovery in all single failure
  scenarios, using a single mechanism to handle both link and node
  failures, and without knowing the root cause of the failure. MRC is
  strictly connectionless, and assumes only destination based
  hop-by-hop forwarding. MRC is based on keeping additional routing
  information in the routers, and allows packet forwarding to continue
  on an alternative output link immediately after the detection of a
  failure.  It can be implemented with only minor changes to existing
  solutions.  In this paper we present MRC, and analyze its
  performance with respect to scalability, backup path lengths, and
  load distribution after a failure. We also show how an estimate of
  the traffic demands in the network can be used to improve the
  distribution of the recovered traffic, and thus reduce the chances
  of congestion when MRC is used.},
  journal = {IEEE Transactions on Networking}
}

@Inproceedings{Simula.ND.5,
  author = {Zhang, Y},
  title = {Handoff Probability in Wireless Networks over Rayleigh Fading Channel: a Cross-layer Approach},
  year = {2007},
  abstract = {},
  booktitle = {the IEEE International Conference on Communications (ICC), ICC{\textquoteright}2007},
  editor = {IEEE},
  pages = {5660-5664},
  publisher = {IEEE},
  isbn = {1-4244-0353-7}
}

@Inproceedings{Simula.ND.6,
  author = {Bai, Aleksander and Skeie, Tor and Engelstad, Paal E},
  title = {A Model-Based Admission Control for 802.11e EDCA using Delay Predictions},
  year = {2007},
  abstract = {This paper presents a unique approach for a model-based admission control algorithm for the IEEE 802.11e Enhanced Distributed Channel Access (EDCA) standard. The analytical model used as the foundation for the algorithm covers both non-saturation and saturation conditions. This allows us to keep the system out of saturation by monitoring several variables. Since the medium access delay represents the service time of the system, it is used as the threshold condition to ensure that the queuing delay is within reasonable bounds. The paper describes the admission control algorithm and several simulation results are presented and discussed. },
  booktitle = {The 26th IEEE International Performance Computing and Communications Conference (IPCCC)},
  editor = {NA},
  pages = {226-235},
  publisher = {IEEE Computer Society Press},
  isbn = {1-4244-1138-6}
}

@Inproceedings{Simula.ND.7,
  author = {Hansen, Audun Fosselie and Lysne, Olav and Cicic, Tarik and Gjessing, Stein},
  title = {Fast Proactive Recovery from Concurrent Failures},
  year = {2007},
  abstract = {Recovery of traffic in connectionless pure IP networks
has traditionally been handled by a full re-convergence of
the network state. This process operates in a time scale that is not
compatible with new real time and highly dependable services.
Recently, schemes for fast local and proactive recovery in connectionless
IP networks have been proposed. All these schemes are
designed to guarantee recovery of the failure of one component.
As IP protocols are used to carry more highly dependable services
and new wireless infrastructures are approaching, guaranteed
failure coverage of more than one failure becomes necessary. In
this paper we present and evaluate a scheme that guarantees to
handle any two concurrent failures in a network. We are not
aware of any other schemes that addresses such guarantees. We
evaluate and compare it with other known recovery schemes, and
we show how it gives substantially better recovery success rates
than the schemes designed for one fault tolerance, also for more
than two failures.},
  booktitle = {IEEE International Conference on Communications (ICC 2007)},
  editor = {IEEE},
  publisher = {IEEE},
  isbn = {1-4244-0353-7}
}

@Article{Simula.ND.8,
  author = {Halvorsen, P{\r a}l and Dalseng, Tom Anders and Griwodz, Carsten},
  title = {Assessment of Linux' Data Path Components for Download and Streaming},
  year = {2007},
  abstract = {Distributed multimedia streaming systems are increasingly popular due to technological advances, and numerous streaming services are available today. On servers or proxy caches, there is a huge scaling challenge in supporting thousands of concurrent users that request delivery of high-rate, time-dependent data like audio and video, because this requires transfers of large amounts of data through several sub-systems within a streaming node.
Unnecessary copy operations in the data path can therefore contribute significantly to the resource consumption of streaming operations. Despite previous research, off-the-shelf operating systems have only limited support for data paths that have been optimized for streaming. Additionally, system call overhead has grown with newer operating systems editions, adding to the cost of data movement. Frequently, it is argued that these issues can be ignored because of the continuing growth of CPU speeds.
However, such an argument fails to take problems of modern streaming systems into account. The dissipation of heat generated by disks and high-end CPUs is a major problem of data centers, which would be alleviated if less power-hungry CPUs could be used. The power budget of mobile devices, which are increasingly used for streaming as well, is tight, and reduced power consumption an important issue.
In this paper, we prove that these operations consume a large amount of resources, and we therefore revisit the data movement problem and provide a comprehensive evaluation of possible streaming data I/O paths in the Linux 2.6 kernel. We have implemented and evaluated several enhanced mechanisms and show how to provide support for more efficient memory usage and reduction of user/kernel space switches for content download and streaming applications. In particular, we are able to reduce the CPU usage by approximately 27\% compared to the best approach without kernel modifications, by removing copy operations and system calls for a streaming scenario in which RTP headers must be added to stored data for sequence numbers and timing.
},
  journal = {The International Journal of Software Engineering and Knowledge Engineering},
  volume = {17},
  number = {4},
  pages = {456--481}
}

@Inproceedings{Simula.ND.9,
  author = {Alia, Mourad and Eide, Viktor S. Wold and Paspallis, Nearchos and Eliassen, Frank and Hallsteinsen, Svein and A. Papadopoulos, George},
  title = {A Utility-based Adaptivity Model for Mobile Applications},
  year = {2007},
  abstract = {Mobile environments are characterized by resource fluctuations and limitations, and variations in user preferences. Therefore mobile applications need to be adaptive to retain usability, usefulness and reliability. In our approach to support adaptivity, we combine context awareness, reflection and component composition planning. The planning is done by generic middleware and supports dynamic discovery, utility-based and context-aware evaluation, and selection of the best implementation alternative of a given mobile application. In this paper we present a formal model of our approach and use this model to show the expressiveness of utility-based adaptation policies. To demonstrate the feasibility and expressiveness of our approach we include a case study based on a real adaptive application built using our model and middleware.
},
  booktitle = {The IEEE International Symposium on Ubisafe Computing (UbiSafe-07)},
  editor = {L.T.Yang, V.Chaudhary, R.Huang},
  pages = {556--563},
  publisher = {IEEE Computer Society Press},
  isbn = {0-7695-2847-3}
}

@Inproceedings{Simula.ND.10,
  author = {Ning, H and Cong, Y and Xu, Z and Hong, T and Zhao, J and Zhang, Y},
  title = {Performance Evaluation of RFID Anti-Collision Algorithm with FPGA Implementation},
  year = {2007},
  abstract = {},
  booktitle = {the Second IEEE International Symposium on Pervasive Computing and Ad Hoc Communications (PCAC-07)},
  editor = {IEEE},
  publisher = {IEEE Computer Society Press},
  isbn = {0-7695-2847-3}
}

@Phdthesis{SC.3.Moe.2001,
  author = {Moe, H},
  title = {Effect of topography on flows, tides and vortices},
  year = {2001},
  abstract = {},
  school = {Department of Mathematics, University of Oslo}
}

@Article{SC.4.Chen.2001.a,
  author = {Chen, W},
  title = {Orthonormal RBF Wavelet and Ridgelet-like Series and Transforms for High-Dimensional Pkroblems},
  year = {2001},
  abstract = {},
  journal = {International Journal of Nonlinear Sciences
                   and Numerical Simulation},
  volume = {2},
  number = {2},
  pages = {163-168}
}

@Article{SC.4.Chen.2001.b,
  author = {Chen, W and He, W},
  title = {A note on radial basis function computing},
  year = {2001},
  abstract = {},
  journal = {Int. J. Nonlinear Modelling Sci. \& Engng.},
  volume = {1},
  pages = {59-65}
}

@Article{SC.4.Karlsen.2001.a,
  author = {Haugse, V and Karlsen, K H and Lie, K -A and Natvig, J R},
  title = {Numerical solution of the polymer system by front tracking},
  year = {2001},
  abstract = {},
  journal = {Transp. Porous Media},
  volume = {44},
  number = {1},
  pages = {63-83}
}

@Article{SC.4.Karlsen.2001.b,
  author = {Karlsen, K H and Lie, K -A and Natvig, J R and Nordhaug, H F and Dahle, H K},
  title = {Operator splitting methods for systems of convection-diffusion equations: nonlinear error mechanisms and correction strategies},
  year = {2001},
  abstract = {},
  journal = {J. Comp. Phys},
  volume = {173},
  number = {2},
  pages = {636-663}
}

@Article{SC.4.Langtangen.2001,
  author = {Langtangen, H P and Munthe, O},
  title = {Solving Systems of Partial Differential Equations Using Object-Oriented Programming Techiques with Coupled Heat and Fluid Flow as Example},
  year = {2001},
  abstract = {},
  journal = {Transactions on Mathematical Software},
  volume = {27},
  number = {1},
  pages = {1-26}
}

@Article{SC.4.Nilssen.2001,
  author = {Nilssen, T K and Tai, X -C and Winther, R},
  title = {A robust Nonconforming H2-Element},
  year = {2001},
  abstract = {},
  journal = {Mathematics of Computation},
  volume = {70},
  pages = {489-505}
}

@Article{SC.4.Shu.2001,
  author = {Shu, C and Chen, W and Xue, H and Du, H},
  title = {Numerical Study of Grid Distribution Effect on Accuracy of DQ Analysis of Beams and Plates by Error Estimation of Derivative Approximations},
  year = {2001},
  abstract = {},
  journal = {Int. J. of Numer. Methods Engng.},
  volume = {51},
  number = {2},
  pages = {159-179}
}

@Article{SC.4.Sundnes.2001,
  author = {Sundnes, J and Lines, G and Tveito, A},
  title = {Efficient solution of ordinary differential equations modeling electrical activity in cardiac cells},
  year = {2001},
  abstract = {},
  journal = {Mathematical Biosciences 172},
  pages = {55-72}
}

@Article{SC.4.Tanaka.2001.a,
  author = {Tanaka, M and Chen, W},
  title = {Dual reciprocity BEM applied to transient elastodynamic problems with differential quadrature method in time},
  year = {2001},
  abstract = {},
  journal = {Computer Methods in Applied Mechanics and
                   Engineering},
  volume = {190},
  pages = {2331-2347}
}

@Article{SC.4.Tanaka.2001.b,
  author = {Tanaka, M and Chen, W},
  title = {Coupling dual reciprocity BEM and differential quadrature method for time-dependent diffusion problems},
  year = {2001},
  abstract = {},
  journal = {Applied Mathematical Modelling},
  volume = {25},
  number = {3},
  pages = {257-268}
}

@Inproceedings{SC.5.Bouhmala.2001,
  author = {Bouhmala, N and Cai, X},
  title = {Partition of Unstructured Finite Element Meshes by a Multilevel Approach},
  year = {2001},
  abstract = {},
  booktitle = {Applied Parallel Computing - New Paradigms
                   for HPC in Industry and Academia, 5th
                   International Conference, PARA 2000},
  editor = {T. S{\o}revik et al},
  pages = {187-195},
  publisher = {Springer-Verlag},
  address = {Bergen, Norway},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SC.5.Bounaim.2001,
  author = {Bounaim, A},
  title = {On the Solution Of Optimal Control Problems Using Domain Decomposition Techniques},
  year = {2001},
  abstract = {},
  booktitle = {International Conference on Numerical Algorithms},
  address = {Marrakesh, Morocco},
  note = {Abstract page 26}
}

@Inproceedings{SC.5.Cai.2001,
  author = {Cai, X and {\O}deg{\r a}rd, {\r A}},
  title = {On the Performance of PC Clusters in Solving Partial Differential Equations},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of the Tenth SIAM Conference on
                   Parallel Processing for Scientific Computing}
}

@Inproceedings{SC.5.Chen.2001.a,
  author = {Chen, W and Tanaka, M},
  title = {Solution of some inverse heat conduction problems by the dynamic programming filter and BEM},
  year = {2001},
  abstract = {},
  booktitle = {International Symposium on Inverse Problems in
                   Engineering Mechanics},
  pages = {23-28},
  address = {Nagano, Japan}
}

@Inproceedings{SC.5.Chen.2001.b,
  author = {Chen, W},
  title = {RBF-based meshless boundary knot method and boundary particle method},
  year = {2001},
  abstract = {},
  booktitle = {Proc. of China Congress on Computational
                   Mechanics},
  pages = {319-326}
}

@Inproceedings{SC.5.Chen.2001.c,
  author = {Chen, W},
  title = {Boundary knot method for Laplace and biharmonic problems},
  year = {2001},
  abstract = {},
  booktitle = {The Proceedings of the 14th Nordic Workshop on
                   Computational Mechanics},
  pages = {117-120},
  address = {Sweden}
}

@Inbook{SC.5.Daehlen.2001,
  author = {D{\ae}hlen, M and Fimland, M and Hjelle, {\O}},
  title = {A Triangle-based Carrier for Geographical Data},
  year = {2001},
  abstract = {},
  booktitle = {Spatial Information and the Environment},
  editor = {Peter Halls},
  publisher = {Taylor and Francis Books Ltd},
  pages = {105-120}
}

@Inproceedings{SC.5.Holden.2001,
  author = {Holden, H and Karlsen, K H and Lie, K -A and Risebro, N H},
  title = {Operator splitting for convection-dominated nonlinear partial differential equations},
  year = {2001},
  abstract = {},
  booktitle = {Godunov Methods:Theory and Applications},
  editor = {E. F. Toro},
  pages = {469-475}
}

@Inproceedings{SC.5.Holden.2001.b,
  author = {Holden, L and Nielsen, B F and V{\a\'a}zquez, A Almendral},
  title = {Stochastic structural modeling in Havana},
  year = {2001},
  abstract = {},
  booktitle = {EAGE annual conference, 2001 Amsterdam abstracts
                   book},
  publisher = {European Association of Geoscientists \& Engineers}
}

@Inproceedings{SC.5.Ingebrigtsen.2001,
  author = {Ingebrigtsen, L and Bounaim, A and Langtangen, H P and Tveito, A},
  title = {Moving Mesh Techniques for Fluid Flow Simulations},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings from the First SIAM-EMS Conference on
                   Applied Mathematics in our Changing World},
  address = {Berlin}
}

@Inproceedings{SC.5.Langtangen.2001,
  author = {Langtangen, H P and Pedersen, G},
  title = {Propagation of Large Destructive Waves},
  year = {2001},
  abstract = {},
  booktitle = {Computational Mechanics (MekIT'01)},
  editor = {H. I. Andersson and B. Skallerud},
  publisher = {Tapir}
}

@Inproceedings{SC.5.Langtangen.2001.a,
  author = {Langtangen, H P and Cai, X},
  title = {A Software Framework for Easy Parallelization of PDE Solvers},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of Parallel Computational Fluid
                   Dynamics 2000},
  editor = {C. B. Jensen and T. Kvamsdal and H. I. Andersson and
                   B. Pettersen and A. Ecer and J. Periaux and N. Satofuka
                   and P. Fox},
  publisher = {North Holland}
}

@Inproceedings{SC.5.Mardal.2001,
  author = {Mardal, K -A and Langtangen, H P},
  title = {An efficient parallel iterative approach to a fully implicit mixed finite element formulation for the Navier-Stokes equations},
  year = {2001},
  abstract = {},
  booktitle = {ECCOMAS CFD 2001, Computational Fluid Dynamics
                   Conference Proceedings},
  publisher = {Swansea, 2001, CD-ROM, European Community on Computational Methods in Applied Sciences, ISBN 0 905 091 12 4}
}

@Inproceedings{SC.5.Sevaldrud.2001,
  author = {Sevaldrud, T and Aasgaard, R},
  title = {Distributed handling of level of detail surfaces with binary triangle trees},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of ScanGIS},
  pages = {45-58}
}

@Inproceedings{SC.6.Cai.2001,
  author = {Cai, X and Langtangen, H P},
  title = {How modern programming techniques can greatly simplify the development of parallel simulation codes in computational mechanics},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of the MekIT'01 Conference},
  publisher = {Tapir}
}

@Inproceedings{SC.6.Mardal.2001,
  author = {Mardal, K -A and Langtangen, H P and Winter, R},
  title = {An Optimal Iterative Method for the Time-Dependent Stokes Problem},
  year = {2001},
  abstract = {},
  booktitle = {Tenth Conference Virtual Proceedings, Copper
                   Mountain Conference on Multigrid Methods},
  publisher = {MGNet},
  address = {Copper Mountain, CO, USA}
}

@Misc{SC.7.Bounaim.2001,
  author = {Bounaim, A},
  title = {On the solution of an unsteady optimal control problem using domain decomposition techniques},
  year = {2001},
  abstract = {},
  howpublished = {Preprint}
}

@Techreport{SC.7.Bruaset.2001,
  author = {Bruaset, A M},
  title = {Heat flow phenomenon in a SOFC unit cell},
  year = {2001},
  abstract = {},
  institution = {Numerical Objects AS},
  type = {Internal report},
  note = {Report on simulation of solid oxide fuel cells written
                   for Mitsubishi Material Corporation in Japan}
}

@Misc{SC.7.Chen.2001.a,
  author = {Chen, W},
  title = {Analytical solution of transient scalar wave and diffusion problems of arbitrary dimensionality and geometry by RBF wavelet series},
  year = {2001},
  abstract = {},
  howpublished = {Preprint}
}

@Misc{SC.7.Chen.2001.b,
  author = {Chen, W},
  title = {Direct linearization method for nonlinear PDEs and the related kernel RBFs},
  year = {2001},
  abstract = {},
  howpublished = {Preprint}
}

@Misc{SC.7.Daehlen.2001,
  author = {D{\ae}hlen, M},
  title = {Simula Research Laboratory -- Early days!},
  year = {2001},
  abstract = {},
  howpublished = {Misc}
}

@Misc{SC.7.Daehlen.2001.b,
  author = {D{\ae}hlen, M},
  title = {SIMULA Research Laboratory AS -- profilbeskrivelse},
  year = {2001},
  abstract = {},
  howpublished = {Misc}
}

@Techreport{SC.7.Lines.2001,
  author = {Lines, G and Cai, X and Tveito, A},
  title = {A parallel solution of the bidomain equations modeling the electrical activity of the heart},
  year = {2001},
  abstract = {},
  institution = {Simula Research Laboratory}
}

@Misc{SC.8.Bounaim.2001,
  author = {Bounaim, A},
  title = {Ultrasound Simulation in Medicine},
  year = {2001},
  abstract = {},
  howpublished = {Internal Report, Numerical and Mathematical Modeling
                   of Medical Ultrasound Wave Propagation Project,
                   December}
}

@Misc{SC.8.Bruaset.2001,
  author = {Bruaset, A M},
  title = {PDEWizard - integration of Diffpack and Mathematica},
  year = {2001},
  abstract = {},
  howpublished = {Internal seminar at Wolfram Research Inc. in
                   Champaign, Illinois, USA discussing a prototype
                   implementation of a PDE-oriented software environment
                   based on integration of Diffpack and Mathematica}
}

@Misc{SC.8.Cai.2001,
  author = {Cai, X and Langtangen, H P},
  title = {How Modern Programming Techniques Can Greatly Simplify the Development of Parallel Simulation Codes in Computational Mechanics},
  year = {2001},
  abstract = {},
  howpublished = {Talk at the National Conference on Computational
                   Mechanics (MekIT'01), Trondheim, Norway},
  note = {Presented by X. Cai}
}

@Misc{SC.8.Daehlen.2001,
  author = {D{\ae}hlen, M},
  title = {NRK m{\r a} forske mer},
  year = {2001},
  abstract = {},
  howpublished = {Dagens N{\ae}ringsliv, kronikk}
}

@Misc{SC.8.Gabrielsen.2001,
  author = {Gabrielsen, R H and D{\ae}hlen, M},
  title = {Utnytt potensialet i norsk forskning},
  year = {2001},
  abstract = {},
  howpublished = {Bergens Tidene}
}

@Misc{SC.8.Holden.2001,
  author = {Holden, L and Nielsen, B F and V\&aacute;zquez, A},
  title = {Stochastic structural modeling in Havana},
  year = {2001},
  abstract = {},
  howpublished = {Presented at the 2001 EAGE conference,
                   Amsterdam, The Netherlands}
}

@Misc{SC.8.Langtangen.2001,
  author = {Langtangen, H P},
  title = {Some Efficiency Considerations when Using C++ and Object-Oriented Programming in Numerical Codes},
  year = {2001},
  abstract = {},
  howpublished = {Invited minisymposium talk at the GAMM 2001
                   Conference in Zurich, Switzerland}
}

@Misc{SC.8.Langtangen.2001.b,
  author = {Langtangen, H P},
  title = {Rapid Prototyping in Multi-Physics Simulation},
  year = {2001},
  abstract = {},
  howpublished = {Keynote lecture at Advanced Environments and
                   Tools for High Performance Computing; EuroConference on
                   Problem-Solving Environments for Numerical Mathematics,
                   Science and Engineering Applications, Castelvecchio
                   Pascoli, Italy}
}

@Misc{SC.8.Nielsen.2001,
  author = {Nielsen, B F and Holden, L and Mostad, P and Skorstad, A and V\&aacute;zquez, A and Townsend, C and Ottesen, S},
  title = {A stochastic structural model},
  year = {2001},
  abstract = {},
  howpublished = {Presented at the Modelling Permeable Rocks
                   conference in Cambridge, England}
}

@Misc{SC.8.Nielsen.2001.b,
  author = {Nielsen, B F and Skavhaug, O and Tveito, A},
  title = {Numerical Solution of American Option Problems Using Penalty Methods},
  year = {2001},
  abstract = {},
  howpublished = {Presented at the SIAM Annual Meeting in San
                   Diego, USA}
}

@Misc{SC.8.Nilssen.2001,
  author = {Nilssen, T K and Mannseth, T and Tai, X --C},
  title = {Permeability Estimation with the Augmented Lagrangian Method for Nonlinear Diffusion Equations},
  year = {2001},
  abstract = {},
  howpublished = {Talk at the International Parallel CFD 2001
                   Conference, Egmond aan Zee, The Netherlands}
}

@Misc{SC.8.Pedersen.2001,
  author = {Pedersen, G and Langtangen, H P},
  title = {Propagation of Large Destructive Waves (Tsunamis)},
  year = {2001},
  abstract = {},
  howpublished = {Talk at the National Conference on Computational
                   Mechanics (MekIT'01), Trondheim, Norway},
  note = {Presented by G. Pedersen}
}

@Book{SC.1.Tveito.2002,
  author = {Tveito, A and Winther, R},
  title = {Einf{\"u}hrung in die Theorie der partiellen Differentialgleichungen; Ein numerischer Zugang},
  year = {2002},
  abstract = {},
  publisher = {Springer-Verlag},
  note = {430 pages}
}

@Phdthesis{SC.3.Sundnes.2002,
  author = {Sundnes, J},
  title = {Numerical methods for simulating the electrical activity of the heart},
  year = {2002},
  abstract = {},
  school = {Department of informatics, University of Oslo}
}

@Article{SC.4.Chen.2002.a,
  author = {Chen, W and Tanaka, M},
  title = {A meshless, integration-free, and boundary-only RBF technique},
  year = {2002},
  abstract = {},
  journal = {Computers and Mathematics with Applications},
  volume = {43},
  pages = {379-391}
}

@Article{SC.4.Chen.2002.b,
  author = {Chen, W and Tanaka, M},
  title = {A study on time schemes for DRBEM analysis of scalar impact wave},
  year = {2002},
  abstract = {},
  journal = {Comput. Mech.},
  volume = {28},
  pages = {331-338}
}

@Article{SC.4.Chen.2002.c,
  author = {Chen, W},
  title = {High-order fundamental and general solutions of convection-diffusion equation and their applications with boundary particle method},
  year = {2002},
  abstract = {},
  journal = {Engng. Anal. Bound. Elem.},
  volume = {26},
  number = {7},
  pages = {571-575}
}

@Article{SC.4.Chen.2002.d,
  author = {Chen, W},
  title = {Symmetric boundary knot method},
  year = {2002},
  abstract = {},
  journal = {Engng. Anal. Bound. Elem.},
  volume = {26},
  number = {6},
  pages = {489-494}
}

@Article{SC.4.Chen.2002.e,
  author = {Chen, W},
  title = {Meshfree boundary particle method applied to Helmholtz problems},
  year = {2002},
  abstract = {},
  journal = {Engng. Anal. Bound. Elem.},
  volume = {26},
  number = {7},
  pages = {577-581}
}

@Article{SC.4.Gjevik.2002,
  author = {Gjevik, B and Moe, H and Ommundsen, A},
  title = {Idealized Model Simulations of Barotropic Flow on the Catalan Shelf},
  year = {2002},
  abstract = {},
  journal = {Continental Shelf Research},
  volume = {22},
  pages = {173-198}
}

@Article{SC.4.Langtangen.2002.a,
  author = {Langtangen, H P and Pedersen, G},
  title = {Propagation of Large Destructive Waves},
  year = {2002},
  abstract = {},
  journal = {International Journal of Applied Mechanics and
                   Engineering},
  volume = {7},
  number = {1},
  pages = {187-204}
}

@Article{SC.4.Langtangen.2002.b,
  author = {Langtangen, H P and Mardal, K -A and Winther, R},
  title = {Numerical Methods for Incompressible Viscous Flow},
  year = {2002},
  abstract = {},
  journal = {Advances in Water Resources},
  volume = {25},
  number = {8},
  pages = {1125-1146}
}

@Article{SC.4.Lines.2002.b,
  author = {Lines, G and Sundnes, J and Tveito, A},
  title = {A Domain Embedding Strategy for Solving the Bidomain Equations on Complicated Geometries},
  year = {2002},
  abstract = {},
  journal = {International Journal of Bioelectromagnitism},
  volume = {4},
  number = {2},
  pages = {53-54}
}

@Article{SC.4.Lyche.2002,
  author = {Lyche, T and Nilssen, T K and Winther, R},
  title = {Preconditioned iterative methods for scattered data interpolation},
  year = {2002},
  abstract = {},
  journal = {Advances in Computational mathematics},
  volume = {17},
  pages = {237-256}
}

@Article{SC.4.Mardal.2002,
  author = {Mardal, K -A and Tai, X -C and Winther, R},
  title = {A robust finite element method for Darcy-Stokes flow},
  year = {2002},
  abstract = {},
  journal = {SIAM J. Numerical Analysis},
  volume = {40},
  pages = {1605-1631}
}

@Article{SC.4.Moe.2002,
  author = {Moe, H and Ommundsen, A and Gjevik, B},
  title = {A high resolution tidal model for the area around The Lofoten Islands, norhern Norway},
  year = {2002},
  abstract = {},
  journal = {Continental Shelf Research},
  volume = {22},
  pages = {485-504}
}

@Article{SC.4.Nielsen.2002,
  author = {Nielsen, B F and Skavhaug, O and Tveito, A},
  title = {Penalty and front-fixing methods for the numerical solution of American option problems},
  year = {2002},
  abstract = {},
  journal = {The Journal of Computational Finance},
  volume = {5},
  number = {4},
  pages = {69-97}
}

@Article{SC.4.Sundnes.2002,
  author = {Sundnes, J and Lines, G and Mardal, K -A and Tveito, A},
  title = {Multigrid block preconditioning for a coupled system of partial differential equations modeling the electrical activity in the heart},
  year = {2002},
  abstract = {},
  journal = {Computer Methods in Biomechanics and
                   Biomedical Engineering},
  volume = {5},
  number = {6}
}

@Article{SC.4.Sundnes.2002.a,
  author = {Sundnes, J and Lines, G and Tveito, A},
  title = {A Second Order Scheme for Solving the Bidomain and Forward Problem},
  year = {2002},
  abstract = {},
  journal = {International Journal of Bioelectromagnitism},
  volume = {4},
  number = {2},
  pages = {51-52}
}

@Article{SC.4.Sundnes.2002.b,
  author = {Sundnes, J and Lines, G and Tveito, A},
  title = {ODE Solvers for a Stiff System Arising in the Modeling of the Electrical Activity of the Heart},
  year = {2002},
  abstract = {},
  journal = {International Journal of Nonlinear Sciences
                   and Numerical Simulation},
  volume = {3},
  number = {3}
}

@Article{SC.4.Tsompanopoulou.2002,
  author = {Tsompanopoulou, Y and Houstis, E N and Catlin, C A and Gottfried, D and Balakrishnan, G and Su, K and Rice, J R},
  title = {GASTURBNLAB: A Multidiciplinary Problem Solving Environment for Gas Turbine Engine Design on a Network of Non-Homogeneous Machines},
  year = {2002},
  abstract = {},
  journal = {J. of Comp. and Applied Mathematics}
}

@Inproceedings{SC.5.Cai.2002.a,
  author = {Cai, X and Langtangen, H P},
  title = {Developing Parallel Object-Oriented Simulation Codes in Diffpack},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Fifth World Congress on
                   Computational Mechanics},
  address = {Vienna University of Technology},
  note = {ISBN 3-9501554-0-6}
}

@Inproceedings{SC.5.Cai.2002.b,
  author = {Cai, X and Lines, G},
  title = {Enabling Numerical and Software Technologies for Studying the Electrical Activity in Human Heart},
  year = {2002},
  abstract = {},
  booktitle = {Applied Parallel Computing - Advanced
                   Scientific Computing, 6th International
                   Conference, PARA 2002},
  editor = {J. Fagerholm et al},
  pages = {3-17},
  publisher = {Springer-Verlag},
  address = {Espoo, Finland},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SC.5.Cai.2002.c,
  author = {Cai, X and Sosonkina, M and Saad, Y},
  title = {Parallel Iterative Methods in Modern Physical Applications},
  year = {2002},
  abstract = {},
  booktitle = {Computational Science - ICCS 2002},
  editor = {P.M.A. Sloot et al},
  pages = {345-355},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SC.5.Chen.2002.a,
  author = {Chen, W},
  title = {Some recent advances on the RBF},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of BEM 24},
  pages = {125-134},
  address = {Portugal}
}

@Inproceedings{SC.5.Chen.2002.b,
  author = {Chen, W},
  title = {Kernel and wavelets RBFs based on fundamental and general solutions of partial differential equations},
  year = {2002},
  abstract = {},
  booktitle = {The Fifth International Conference of Curves
                   \& Surfaces},
  address = {France},
  note = {Abstract pp. 12}
}

@Inproceedings{SC.5.Chen.2002.c,
  author = {Chen, W and Holm, S and Bounaim, A and {\O}deg{\r a}rd, {\r A} and Tveito, A},
  title = {Frequency decomposition time-domain model of broadband frequency-dependent absorption},
  year = {2002},
  abstract = {},
  booktitle = {The 9th Workshop of the Finite Element
                   Method in Biomedical Engineering, Biomechanics
                   and Related Fields},
  address = {Ulm, Germany}
}

@Inproceedings{SC.5.Hollund.2002,
  author = {Hollund, K and Mostad, P and Nielsen, B F and Holden, L and Gjerde, J and Contursi, M G and McCann, A J and Townsend, C and Sverdrup, E},
  title = {HAVANA - a fault modeling tool},
  year = {2002},
  abstract = {},
  booktitle = {Hydrocarbon Seal Quantification, NPF Special
                   Publication 11},
  editor = {A. G. Koestler and R. Hunsdale},
  pages = {157-171},
  publisher = {Elsevier Science}
}

@Inproceedings{SC.5.Langtangen.2002,
  author = {Langtangen, H P and Mardal, K -A},
  title = {A Software Framework for Mixed Finite Element Programming},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the 2nd International Conference
                   on Computational Science},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SC.5.Lie.2002.a,
  author = {Lie, K -A and Noelle, S},
  title = {High resolution nonoscillatory central difference schemes for the 2D Euler equations via artificial compression},
  year = {2002},
  abstract = {},
  booktitle = {Progress in Industrial Mathematics at ECMI 2000},
  editor = {M. Anile and V. Capasso and A. Greco},
  pages = {318-324},
  series = {Mathematics in Industry}
}

@Inproceedings{SC.5.Lie.2002.b,
  author = {Lie, K -A and Noelle, S and Rosenbaum, W},
  title = {On the resolution and stability of central difference schemes},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Third International Symposium on:
                   FINITE VOLUMES FOR COMPLEX APPLICATIONS - PROBLEMS AND
                   PERSPECTIVES, Porquerolles},
  pages = {793-800},
  publisher = {Hermes Penton Ltd}
}

@Inproceedings{SC.5.Nielsen.2002,
  author = {Nielsen, B F and Skavhaug, O and Tveito, A},
  title = {A penalty scheme for solving American option problems},
  year = {2002},
  abstract = {},
  booktitle = {Progress in industrial mathematics at ECMI 2000},
  editor = {A. M. Anile and V. Capasso and A. Greco},
  pages = {608--612},
  publisher = {Springer-Verlag}
}

@Inproceedings{SC.6.Chen.2002,
  author = {Chen, W},
  title = {New RBF collocation schemes and kernel RBFs with applications},
  year = {2002},
  abstract = {},
  booktitle = {Lecture Notes in Computational Science and
                   Engineering},
  pages = {75-86}
}

@Techreport{SC.7.Acklam.2002.a,
  author = {Acklam, E and Langtangen, H P},
  title = {Tools for Simplified Programming with Staggered Grids},
  year = {2002},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research report},
  number = {2002-06}
}

@Techreport{SC.7.Bounaim.2002,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A} and Tveito, A},
  title = {Simulation of the CARI technique for breast tumor detection using ultrasound wave propagation},
  year = {2002},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2002-11}
}

@Techreport{SC.7.Calhoun.2002,
  author = {Calhoun, D and Langtangen, H P},
  title = {Writing C++ Interfaces to FORTRAN Packages},
  year = {2002},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research report},
  number = {2002-07}
}

@Misc{SC.7.Chen.2002.a,
  author = {Chen, W},
  title = {Distance function wavelets - Part II: Extended results and conjectures},
  year = {2002},
  abstract = {},
  howpublished = {CoRR preprint}
}

@Misc{SC.7.Chen.2002.b,
  author = {Chen, W},
  title = {Distance function wavelets -- Part I: Helmholtz and convection-diffusion transforms and series},
  year = {2002},
  abstract = {},
  howpublished = {CoRR preprint}
}

@Misc{SC.7.Chen.2002.c,
  author = {Chen, W},
  title = {Distance function wavelets - Part III: "Exotic" transforms and series},
  year = {2002},
  abstract = {},
  howpublished = {CoRR preprint}
}

@Techreport{SC.7.Daehlen.2002.a,
  author = {D{\ae}hlen, M and Maartmann-Moe, E and R{\o}sj{\o}, B},
  title = {EFFEKT - en ny modell for forskningsbasert nyskaping},
  year = {2002},
  abstract = {},
  institution = {Simula Research Laboratory}
}

@Misc{SC.7.Daehlen.2002.b,
  author = {D{\ae}hlen, M and Gropen, O and Sk{\r a}lin, R},
  title = {Tungregning mot 2010},
  year = {2002},
  abstract = {},
  howpublished = {Norges forskningsr{\r a}d}
}

@Techreport{SC.7.Langtangen.1996,
  author = {Langtangen, H P},
  title = {Tips and frequently asked questions about Diffpack},
  year = {2002},
  abstract = {},
  institution = {Numerical Objects A.S.},
  type = {Numerical Objects Report Series}
}

@Techreport{SC.7.Mardal.2002,
  author = {Mardal, K -A and Acklam, E and Calhoun, D},
  title = {How to use Matlab in C++ Programs},
  year = {2002},
  abstract = {},
  institution = {Simula Research Laboratory},
  note = {Simula 2002-08}
}

@Misc{SC.8.Bounaim.2002,
  author = {Bounaim, A},
  title = {Simulation of a Simple Attenuation Model in Breast Tumor Detection},
  year = {2002},
  abstract = {},
  howpublished = {Internal Report, Numerical and Mathematical Modeling
                   of Medical Ultrasound Wave Propagation Project, January}
}

@Misc{SC.8.Bruaset.2002,
  author = {Bruaset, A M},
  title = {Finite volume methods in Diffpack},
  year = {2002},
  abstract = {},
  howpublished = {Internal seminar at ChevronTexaco in San Ramon,
                   California, USA discussing a prototype implementation
                   of a finite volume framework in Diffpack}
}

@Misc{SC.8.Cai.2002,
  author = {Cai, X and Langtangen, H P},
  title = {Developing Parallel Object-Oriented Simulation Codes in Diffpack},
  year = {2002},
  abstract = {},
  howpublished = {Invited talk at the Fifth World Congress on
                   Computational Mechanics, Vienna, Austria},
  note = {Presented by X. Cai}
}

@Misc{SC.8.Daehlen.2002.a,
  author = {D{\ae}hlen, M},
  title = {Programmeringsspr{\r a}ket som forandret verden},
  year = {2002},
  abstract = {},
  howpublished = {Kronikk, Aftenposten 7 februar},
  note = {Ogs{\r a} i Norsk regnesentral, 1952-2002, side 139-142}
}

@Misc{SC.8.Daehlen.2002.b,
  author = {D{\ae}hlen, M},
  title = {Tallene og datamaskinen},
  year = {2002},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 23. Juni}
}

@Misc{SC.8.Daehlen.2002.c,
  author = {D{\ae}hlen, M and Savaldrud, T},
  title = {H{\r a}ndtering av store datamengder},
  year = {2002},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 30. Juni}
}

@Misc{SC.8.Daehlen.2002.d,
  author = {D{\ae}hlen, M and Hanneborg, A},
  title = {Mikroteknologi},
  year = {2002},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 6. Juli}
}

@Misc{SC.8.Kvamsdal.2002,
  author = {Kvamsdal, T and Okstad, K M and Bruaset, A M},
  title = {EXTRUD - Aluminium extrusion software built on Diffpack},
  year = {2002},
  abstract = {},
  howpublished = {Invited minisymposium talk at the 20th CAD-FEM
                   User's Meeting, Friedrichshafen, Germany},
  note = {Presented by A. M. Bruaset}
}

@Misc{SC.8.Langtangen.2002,
  author = {Langtangen, H P and Tveito, A},
  title = {How Should We Prepare the Students of Science and Technology for a Life in the Computer Age?},
  year = {2002},
  abstract = {},
  howpublished = {Invited talk at the Department of Mathematics,
                   University of Coimbra, Portugal},
  note = {Presented by H. P. Langtangen}
}

@Misc{SC.8.Lines.2002,
  author = {Lines, G T and Sundnes, J and Tveito, A and Cai, X and Mardal, K -A and Bruaset, A M},
  title = {Diffpack simulation of the electrical activity in the heart},
  year = {2002},
  abstract = {},
  howpublished = {Invited minisymposium talk at the 20th CAD-FEM
                   User's Meeting, Friedrichshafen, Germany},
  note = {Presented by A. M. Bruaset}
}

@Misc{SC.8.Nielsen.2002,
  author = {Nielsen, B F},
  title = {Some recent projects within the Scientific Computing Department at the Simula Research Laboratory},
  year = {2002},
  abstract = {},
  howpublished = {Presented at the University of Crete, Heraklio,
                   Greece}
}

@Book{SC.1.Gesztesy.2003,
  author = {Gesztesy, F and Holden, H},
  title = {Soliton Equations and Their Algebro-Geometric Solutions},
  year = {2003},
  abstract = {},
  publisher = {Cambridge University Press},
  address = {Cambridge}
}

@Book{SC.1.Langtangen.2003,
  author = {Langtangen, H P},
  title = {Computational Partial Differential Equations - Numerical Methods and Diffpack Programming},
  year = {2003},
  abstract = {},
  publisher = {Springer-Verlag},
  note = {2nd edition, 855 pages}
}

@Book{SC.2.Langtangen.2003,
  editor = {Langtangen, Hans Petter and Tveito, Aslak},
  title = {Advanced Topics in Computational Partial Differential Equations {\textperiodcentered} Numerical Methods and Diffpack Programming},
  year = {2003},
  abstract = {},
  publisher = {Springer-Verlag},
  volume = {33},
  note = {658 pages}
}

@Phdthesis{SC.3.Mardal.2003,
  author = {Mardal, K -A},
  title = {Software and Numerical Methods for the Incompressible Navier-Stokes equations},
  year = {2003},
  abstract = {},
  school = {Faculty of Mathematics and Natural Sciences,
                   University of Oslo}
}

@Article{SC.4.Benny.2003,
  author = {Benny, Y C and Chen, W},
  title = {Boundary knot method for 2D and 3D Helmholtz and convection-diffusion problems with complicated geometry},
  year = {2003},
  abstract = {},
  journal = {Int. J. Numer. Methd. Engng.},
  volume = {56},
  number = {13},
  pages = {1931-1948}
}

@Article{SC.4.Benth.2003,
  author = {Benth, F E and Ekland, L and Hauge, R and Nielsen, B F},
  title = {On arbitrage-free pricing of forward contracts in energy markets},
  year = {2003},
  abstract = {},
  journal = {Applied Mathematical Finance},
  volume = {10},
  number = {4},
  pages = {325-336}
}

@Article{SC.4.Chen.2003.a,
  author = {Chen, W and Benny, Y C},
  title = {Numerical convergence of boundary knot method in the analysis of Helmholtz, modified Helmholtz, and convection-diffusion problems},
  year = {2003},
  abstract = {},
  journal = {Computer Methods Appl. Mech. Engng.},
  volume = {192},
  pages = {1859-1875}
}

@Article{SC.4.Chen.2003.b,
  author = {Chen, W and Holm, S},
  title = {Modified Szabo's wave equation models for lossy media obeying frequency power law},
  year = {2003},
  abstract = {},
  journal = {J. Acoust. Soc. Amer.},
  volume = {114},
  number = {5},
  pages = {2570-2584}
}

@Article{SC.4.Holden.2003,
  author = {Holden, L and Mostad, P and Nielsen, B F and Gjerde, J and Townsend, C and Ottesen, S},
  title = {Stochastic structural modelling},
  year = {2003},
  abstract = {},
  journal = {Mathematical Geology},
  volume = {35},
  number = {8},
  pages = {899-914}
}

@Article{SC.4.Ingebrigtsen.2003,
  author = {Ingebrigtsen, L and Nilssen, T K and Langtangen, H P and Tveito, A},
  title = {On the accuracy of operator splitting for a fluid-structure interaction problem},
  year = {2003},
  abstract = {},
  journal = {International Journal of Nonlinear Sciences
                   and Numerical Simulation},
  volume = {4},
  pages = {209-218}
}

@Article{SC.4.Kjeldstad.2003,
  author = {Kjeldstad, A and Skogseid, J and Langtangen, H P and Bj{\o}rlykke, K},
  title = {Differential loading by prograding sedimentary wedges on continental margin: an arch forming mechanism},
  year = {2003},
  abstract = {},
  journal = {Journal of Geophysical Research},
  volume = {108},
  number = {B1},
  pages = {2036}
}

@Article{SC.4.Lie.2003.a,
  author = {Lie, K -A and Noelle, S},
  title = {An improved quadrature rule for the flux-computation in staggered central difference schemes in multidimensions},
  year = {2003},
  abstract = {},
  journal = {J. Sci. Comp},
  volume = {18},
  number = {1},
  pages = {69-81}
}

@Article{SC.4.Lie.2003.b,
  author = {Lie, K -A and Noelle, S},
  title = {On the artificial compression method for second-order nonoscillatory central difference schemes for systems of conservation laws},
  year = {2003},
  abstract = {},
  journal = {SIAM J. Sci. Comp.},
  volume = {24},
  number = {4},
  pages = {1157-1174}
}

@Article{SC.4.Lines.2003,
  author = {Lines, G and Gr{\o}ttum, P and Tveito, A},
  title = {Modeling the Electrical Activity of the Heart --A Bidomain Model of the Ventricles Embedded in a Torso},
  year = {2003},
  abstract = {In this paper  a model for the electrical activity of the heart will be presented and an approach to solve the resulting numerical problem will be suggested. The Bidomain Model is used to compute the spatial distribution of the electrical potential. The partial differential equations are discretized
with the finite element method and the multigrid method is used to solve the corresponding linear equations. Adaptivity is applied to resolve the steep gradients in the solution.},
  journal = {Computing and Visualization in Science},
  volume = {5},
  number = {4},
  pages = {195-213}
}

@Article{SC.4.Lines.2003.b,
  author = {Lines, G and Buist, M L and Gr{\o}ttum, P and Pullan, A J and Sundnes, J and Tveito, A},
  title = {Mathematical Models and Numerical Methods for the Forward Problem in Cardiac Electrophysiology},
  year = {2003},
  abstract = {},
  journal = {Computing and Visualization in Science},
  volume = {5},
  number = {4},
  pages = {215-239}
}

@Article{SC.4.Lines.2003.c,
  author = {Lines, G and Sande, J B and Gr{\o}ttum, P and Str{\o}mme, T A and Sejersted, O},
  title = {Co-Localization of Sodium Transporters in Isolated Cardiac Cell Membrane; a Model Study},
  year = {2003},
  abstract = {},
  journal = {International Journal of Bioelectromagnetism},
  volume = {5},
  number = {1},
  pages = {179-180}
}

@Article{SC.4.Moe.2003,
  author = {Moe, H and Gjevik, B and Ommundsen, A},
  title = {A high resolution tidal model for the coast of M{\o}re and Tr{\o}ndelag, Mid-Norway},
  year = {2003},
  abstract = {},
  journal = {Norwegian Journal of Geography},
  volume = {57},
  pages = {65-82},
  note = {ISSN 0029-1951}
}

@Article{SC.4.Nilssen.2003,
  author = {Nilssen, T K and Mannseth, T and Tai, X -C},
  title = {Permeability estimation with the augmented Lagrangian method for a nonlinear diffusion equation},
  year = {2003},
  abstract = {},
  journal = {Computational Geosciences},
  volume = {7},
  pages = {27-47}
}

@Inproceedings{SC.5.Bounaim.2003.a,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Simulation of the breast imaging CARI technique by a FETD approximation of ultrasound wave propagation},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of World Congress on Ultrasonics},
  address = {Paris, France}
}

@Inproceedings{SC.5.Bounaim.2003.b,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A} and Tveito, A and Thomenius, K},
  title = {FETD Simulation of Wave Propagation Modeling the CARI Breast Sonography},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of ICCSA2003, International
                   Conference on Computational Science and its
                   Applications},
  pages = {705-714},
  address = {Montreal, Canada},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SC.5.Bounaim.2003.c,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Sensitivity of the Ultrasonic CARI Technique for Breast Tumor Detection Using a FETD Scheme},
  year = {2003},
  abstract = {},
  booktitle = {Ultrasonics International},
  address = {Granada, Spain}
}

@Inbook{SC.5.Cai.2003.a,
  author = {Cai, X and Acklam, E and Langtangen, H P and Tveito, A},
  title = {Parallel Computing},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations -- Numerical Methods and
                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering},
  pages = {1-55}
}

@Inbook{SC.5.Cai.2003.b,
  author = {Cai, X},
  title = {Overlapping Domain Decomposition Methods},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations -- Numerical Methods and
                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  pages = {57-95}
}

@Inproceedings{SC.5.Chen.2003,
  author = {Chen, W and Holm, S},
  title = {Fractional calculus equation models and L{\a\'e}vy stable distribution for lossy media obeying a frequency power law},
  year = {2003},
  abstract = {},
  booktitle = {Seminar on Modelling, Analysis and Numerical Solution
                   of Problems with Memory and After-Effect},
  address = {Chester, UK}
}

@Inproceedings{SC.5.Chen.2003.a,
  author = {Chen, W and Holm, S},
  title = {Positive fractional time derivative modeling of frequency dependent acoustic dissipation},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of International Carpathian
                   Control Conference},
  pages = {861-864},
  address = {Slovakia}
}

@Inproceedings{SC.5.Chen.2003.b,
  author = {Chen, W and Holm, S},
  title = {Fractional derivative mathematical and numerical modelling of acoustic attenuations obeying arbitrary frequency power law},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the Sixth International
                   Conference on Theoretical \& Computational
                   Acoustics},
  address = {Honolulu, USA}
}

@Inbook{SC.5.Heimsund.2003,
  author = {Heimsund, B -O and Chan, T F and Nilssen, T K and Tai, X -C},
  title = {Level set methods for a parameter identification problem},
  year = {2003},
  abstract = {},
  booktitle = {Analysis and optimization of differential systems},
  editor = {V.Barbu and I.Lasiecka and D.Tiba and
                   C.Varsan},
  publisher = {Kluwer Academic Publishers,
                   Boston/Dordrecht/London},
  pages = {189-200}
}

@Inbook{SC.5.Kjeldstad.2003,
  author = {Kjeldstad, A and Langtangen, H P and Skogseid, J and Bj{\o}rlykke, K},
  title = {Simulation of Sedimentary Basins},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations - Numerical Methods and
                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering}
}

@Inproceedings{SC.5.Langtangen.2003,
  author = {Langtangen, H P and Osnes, H},
  title = {Stochastic Partial Differential Equations},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial Differential
                   Equations -- Numerical Methods and Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  pages = {257-320},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and Engineering}
}

@Inbook{SC.5.Langtangen.2003.a,
  author = {Langtangen, H P and Mardal, K -A},
  title = {Using Diffpack from Python Scripts},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations - Numerical Methods and
                   Diffpack Programming},
  editor = {Langtangen, H.P. and Tveito, A.},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering},
  pages = {321-360}
}

@Inbook{SC.5.Mardal.2003.a,
  author = {Mardal, K -A and Zumbusch, G and Langtangen, H P},
  title = {Multigrid Methods in Diffpack},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations - Numerical Methods and
                   Diffpack Programming},
  editor = {Langtangen, H.P. and Tveito, A.},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering},
  pages = {97-152}
}

@Inbook{SC.5.Mardal.2003.b,
  author = {Mardal, K -A and Sundnes, J and Langtangen, H P and Tveito, A},
  title = {Block preconditioning and Systems of PDEs},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations - Numerical Methods and
                   Diffpack Programming},
  editor = {Langtangen, H.P. and Tveito, A.},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering},
  pages = {199-236}
}

@Inbook{SC.5.Mardal.2003.c,
  author = {Mardal, K -A and Langtangen, H P},
  title = {Mixed elements in Diffpack},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations - Numerical Methods and
                   Diffpack Programming},
  editor = {Langtangen, H.P. and Tveito, A.},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering},
  pages = {153-198}
}

@Inproceedings{SC.5.Oedegaard.2003,
  author = {{\O}deg{\r a}rd, {\r A} and Langtangen, H P and Tveito, A},
  title = {Fully Implicit Methods for Systems of PDEs},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations - Numerical Methods and
                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  pages = {237-256},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and Engineering}
}

@Inbook{SC.5.Skavhaug.2003.a,
  author = {Skavhaug, O and Nielsen, B F and Tveito, A},
  title = {Mathematical Models of Financial Derivatives},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations -Numerical Methods and
                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  pages = {451-482}
}

@Inbook{SC.5.Skavhaug.2003.b,
  author = {Skavhaug, O and Nielsen, B F and Tveito, A},
  title = {Numerical Methods for Financial Derivatives},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial
                   Differential Equations -Numerical Methods and
                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  pages = {483-506}
}

@Inbook{SC.5.Staff.2003,
  author = {Staff, G and R{\o}nquist, E M},
  title = {Stability of the Parareal Algorithm},
  year = {2003},
  abstract = {},
  booktitle = {Domain Decomposition Methods in Science and
                   Engineering},
  editor = {R. Kornhuber and R.H.W. Hoppe and D.E.
                   Keyes and J. P{\a\'e}riaux and O. Pironneau and
                   J. Xu},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and
                   Engineering}
}

@Inproceedings{SC.5.Sulebak.2003,
  author = {Sulebak, Jan Rasmus and Hjelle, {\O}yvind},
  title = {Multiresolution Spline Models and their Applications in Geomorphology},
  year = {2003},
  abstract = {},
  booktitle = {Multiresolution Spline Models and their Applications in Geomorphology, Concepts and                   Modelling in Geomorphology: International Perspectives},
  editor = {I. S. Evans and R. Dikau and E. Tokunaga and H. Ohmori and M. Hirano},
  pages = {221-237},
  publisher = { },
  address = {Tokyo, Japan}
}

@Inbook{SC.5.Sundnes.2003,
  author = {Sundnes, J and Lines, G and Gr{\o}ttum, P and Tveito, A},
  title = {Electrical Activity in the Human Heart},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial                   Differential Equations -- Numerical Methods and                   Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and                   Engineering}
}

@Inbook{SC.5.Thorvaldsen.2003.a,
  author = {Thorvaldsen, T and Langtangen, H P and Osnes, H},
  title = {Finite Element Modeling of Elastic Structures},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial Differential
                   Equations -- Numerical Methods and Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and Engineering},
  pages = {507-576}
}

@Misc{SC.6.Chen.2003,
  author = {Chen, W},
  title = {Solving Possion equations by Boundary Knot Method},
  year = {2003},
  abstract = {},
  howpublished = {Extended Abstract of International Workshop on
                   Meshfree Methods, Lisbon, Portugal}
}

@Inproceedings{SC.6.Huseby.2003,
  author = {Huseby, M and Langtangen, H P},
  title = {Modeling Propagation of Noise over Three-Dimensional Terrains},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the Second National Conference on
                   Computational Mechanics MekIT'03}
}

@Inproceedings{SC.6.Jeberg.2003,
  author = {Jeberg, P and Cai, X and Langtangen, H P and Holm, H},
  title = {A flexible architecture for welding simulators used in weld planning},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of International Conference on
                   Productive Welding in Industrial Applications},
  address = {Lappenranta, Finland}
}

@Inproceedings{SC.6.Kristoffersen.2003,
  author = {Kristoffersen, A and Mardal, K -A},
  title = {Solving Heat Distribution in room using mixed Finite Element for coupling the Navier-Stokes equations and the Heat equation},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the Second National Conference on Computational Mechanics (MekIT'03)},
  pages = {211-225},
  publisher = {Tapir Academic Press}
}

@Inproceedings{SC.6.Mardal.2003,
  author = {Mardal, K -A and Langtangen, H P and Winter, R},
  title = {Error Estimates for the Linear Navier-Stokes Equations},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the Second National Conference on
                   Computational Mechanics (MekIT'03)},
  pages = {225-237},
  publisher = {Tapir Academic Press}
}

@Inproceedings{SC.6.Sundnes.2003,
  author = {Sundnes, J and Lines, G},
  title = {Modeling the electro-mechanical behavior of an infarcted heart},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings from MekIT'03, Second national
                   conference on Computational Mechanics},
  address = {Trondheim}
}

@Techreport{SC.7.Bounaim.2003,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Focusing of ultrasonic waves: Description and FE simulations for a breast imaging technique},
  year = {2003},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report}
}

@Misc{SC.7.Chen.2003,
  author = {Chen, W and Bounaim, A and Cai, X and Holm, S and Tveito, A and {\O}deg{\r a}rd, {\r A}},
  title = {Mathematical and numerical modeling of medical ultrasound wave propagation},
  year = {2003},
  abstract = {},
  howpublished = {Invited talk to MACSI-Workshop for
                   Numerical Simulations for Ultrasound Imaging
                   and Inversion, St. Georgen, Austria, pages
                   8-13}
}

@Misc{SC.7.Daehlen.2003.a,
  author = {D{\ae}hlen, M and Tasken, K and Thorleifson, T and Espelund, M and Rognved, P and Myhre, K},
  title = {Etablering av et "technology transfer office" ved Universitetet i Oslo},
  year = {2003},
  abstract = {},
  howpublished = {Innstilling, 26. september}
}

@Misc{SC.7.Daehlen.2003.b,
  author = {D{\ae}hlen, M and Myhre, K},
  title = {Forskningsbasert nyskaping ved Universitetet i Oslo, Organisering, implementering og drift},
  year = {2003},
  abstract = {},
  howpublished = {Misc}
}

@Misc{SC.7.Daehlen.2003.c,
  author = {D{\ae}hlen, M and Myhre, K and Katle, J and Martmann-Moe, E},
  title = {Fra Forsknings til Forretning, Organisering av forskningsbasert nyskaping ved Universitetet i Oslo},
  year = {2003},
  abstract = {},
  howpublished = {Misc}
}

@Misc{SC.8.Akbay.2003,
  author = {Akbay, T and Bruaset, A M and Langtangen, H P},
  title = {A Multi-Physics Solid Oxide Fuel Cell Simulator as a Diffpack Application},
  year = {2003},
  abstract = {},
  howpublished = {Invited minisymposium talk at the 21st CAD-FEM
                   User's Meeting, Potsdam, Germany},
  note = {Presented by A. M. Bruaset}
}

@Misc{SC.8.Bruaset.2003.b,
  author = {Bruaset, A M and Langtangen, H P},
  title = {Future Development of Diffpack at Simula},
  year = {2003},
  abstract = {},
  howpublished = {Invited minisymposium talk at the 21st CAD-FEM
                   User's Meeting, Potsdam, Germany},
  note = {Presented by A. M. Bruaset}
}

@Misc{SC.8.Daehlen.2003.a,
  author = {D{\ae}hlen, M},
  title = {Tverrfaglighet og v{\ae}rvarsling},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 25. Juni}
}

@Misc{SC.8.Daehlen.2003.b,
  author = {D{\ae}hlen, M and Lysne, O},
  title = {Internett og trafikkork},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 2. Juli}
}

@Misc{SC.8.Daehlen.2003.c,
  author = {D{\ae}hlen, M and Tasken, K},
  title = {Forskningsbasert nyskaping og HIV-forskning},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 9. Juli}
}

@Misc{SC.8.Daehlen.2003.d,
  author = {D{\ae}hlen, M and Tasken, K},
  title = {Et forskningsprosjekt -- en hjertesimulator},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 16. Juli}
}

@Misc{SC.8.Daehlen.2003.e,
  author = {D{\ae}hlen, M and Hanneborg, A},
  title = {Nanoteknologi},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 23. Juli}
}

@Misc{SC.8.Daehlen.2003.f,
  author = {D{\ae}hlen, M and Gabrielsen, R and L{\o}mo, L and Martinsen, O},
  title = {Hvorfor er det olje i Nordsj{\o}en?},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 30. Juli}
}

@Misc{SC.8.Daehlen.2003.g,
  author = {D{\ae}hlen, M and Grimsg{\r a}rd, J and L{\o}mo, L},
  title = {Fra tall til bilder},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 6. August}
}

@Misc{SC.8.Daehlen.2003.h,
  author = {D{\ae}hlen, M and Solbr{\ae}kke, K Nyheim},
  title = {Kvinners posisjon(er) i overgangen fra industri- til informasjonssamfunnet},
  year = {2003},
  abstract = {},
  howpublished = {Sommer{\r a}pent, NRK, 13. August}
}

@Misc{SC.8.Daehlen.2003.i,
  author = {D{\ae}hlen, M},
  title = {Six degrees of separation},
  year = {2003},
  abstract = {},
  howpublished = {Breakfast TV (NRK), 10. November}
}

@Misc{SC.8.Daehlen.2003.j,
  author = {D{\ae}hlen, M and Maartmann-Moe, E},
  title = {Norges ukjente gullgruve},
  year = {2003},
  abstract = {},
  howpublished = {Dagens n{\ae}ringsliv, kronikk, 13. Feb.}
}

@Misc{SC.8.Daehlen.2004.a,
  author = {D{\ae}hlen, M and Sevaldrud, T},
  title = {Multi-level triangulation for fast rendering of huge data sets},
  year = {2003},
  abstract = {},
  howpublished = {Preprint}
}

@Misc{SC.8.Daehlen.2004.b,
  author = {D{\ae}hlen, M and Fimland, M},
  title = {Resolving inconsistencies in multi-level terrain models with water networks},
  year = {2003},
  abstract = {},
  howpublished = {Preprint}
}

@Misc{SC.8.Langtangen.2003,
  author = {Langtangen, H P},
  title = {Stochastic Ordinary Differential Equations and Deterministic Partial Differential Equations -- On the Relation Between Quantum Mechanics and Other Fields of Science},
  year = {2003},
  abstract = {},
  howpublished = {Lecture at the workshop Numerical Methods for
                   Quantum Mechanics at the Center of Mathematics for
                   Applications (CMA), Oslo, Norway}
}

@Misc{SC.8.Langtangen.2003.a,
  author = {Langtangen, H P and Bruaset, A M},
  title = {The History and Philosophy of Diffpack},
  year = {2003},
  abstract = {},
  howpublished = {Invited minisymposium talk at the 21st CAD-FEM
                   User's Meeting, Potsdam, Germany},
  note = {Presented by H. P. Langtangen}
}

@Misc{SC.8.Lines.2003,
  author = {Lines, G T and Nielsen, B F and Tveito, A and Sundnes, J and Gr{\o}ttum, P and Cai, X and Mardal, K A},
  title = {Computing the electrical activity in the human heart},
  year = {2003},
  abstract = {},
  howpublished = {Presented at the Centre of Mathematics for
                   Applications, Oslo}
}

@Misc{SC.8.Nielsen.2003,
  author = {Nielsen, B F and Lines, G T and Tveito, A},
  title = {Determining the size and location of a myocardial infarction; a scientific computing framework},
  year = {2003},
  abstract = {},
  howpublished = {Presented at the European Conference on
                   Numerical Mathematics and Advanced Applications,
                   Prague, Czech Republic}
}

@Misc{SC.8.Nilssen.2003,
  author = {Nilssen, T K and Ingebrigtsen, L},
  title = {Grid Size Requirements for Simulation of Electrical Activity in the Heart},
  year = {2003},
  abstract = {},
  howpublished = {Talk at the Conference on Computational Science
                   and Engineering, San Diego, CA, USA}
}

@Misc{SC.8.Skavhaug.2003,
  author = {Skavhaug, O},
  title = {Fully Automatic Method of Manufactured Solutions},
  year = {2003},
  abstract = {},
  howpublished = {Talk at the Advanced Environments and
                   Tools for High Performance Computing,
                   EuroConference on Problem Solving
                   Environments and the Information Society}
}

@Misc{SC.8.Tveito.2003,
  author = {Tveito, A and Lines, G T and Sundnes, J and Nielsen, B F and Gr{\o}ttum, P and Cai, X and Mardal, K A},
  title = {Computing the electrical activity in the human heart},
  year = {2003},
  abstract = {},
  howpublished = {Presented at the European Conference on
                   Numerical Mathematics and Advanced Applications,
                   Prague, Czech Republic}
}

@Misc{SC.8.Tveito.2003.b,
  author = {Tveito, A and Lines, G T and Nielsen, B F and Sundnes, J and Gr{\o}ttum, P and Cai, X and Mardal, K A},
  title = {Computing the heart},
  year = {2003},
  abstract = {},
  howpublished = {Presented at the 21st CAD-FEM users' meeting
                   2003 - International congress on FEM technology,
                   Potsdam, Germany}
}

@Misc{SC.8.Vazquez.2003,
  author = {V\&aacute;zquez, A A and Nielsen, B F},
  title = {On the convergence of multigrid methods for PDEs in mathematical finance},
  year = {2003},
  abstract = {},
  howpublished = {Presented at the 11th GAMM-Workshop on Multigrid
                   and Hierarchic Solution Techniques, Leipzig, Germany}
}

@Book{SC.1.Langtangen.2004,
  author = {Langtangen, H P},
  title = {Python Scripting for Computational Science},
  year = {2004},
  abstract = {},
  publisher = {Springer-Verlag},
  volume = {3},
  note = {725 pages}
}

@Phdthesis{SC.3.Skavhaug.2004,
  author = {Skavhaug, O},
  title = {Numerical Methods and Software with Applications in Computational Finance},
  year = {2004},
  abstract = {},
  school = {Department of Informatics, University of Oslo}
}

@Article{SC.4s.Glimsdal.2004.a,
  author = {Glimsdal, S and Pedersen, G and Langtangen, H P},
  title = {An Investigation of Domain Decomposition Methods for One-Dimensional Dispersive Long Wave Equations},
  year = {2004},
  abstract = {},
  journal = {Advances in Water Resources},
  volume = {27},
  number = {11},
  pages = {1111--1133}
}

@Article{SC.4.Bounaim.2003.b,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Quantification of the CARI breast imaging sensitivity by 2D/3D numerical time-domain ultrasound wave propagation},
  year = {2004},
  abstract = {},
  journal = {J. Mathematics and Computers in Simulation},
  volume = {65},
  number = {4-5},
  pages = {521--534}
}

@Article{SC.4.Chen.2004,
  author = {Chen, W and Holm, S},
  title = {Fractional Laplacian time-space models for linear and nonlinear lossy media exhibiting arbitrary frequency dependency},
  year = {2004},
  abstract = {},
  journal = {J. Acoust. Soc. Amer.},
  volume = {114},
  number = {5},
  pages = {1424--1430}
}

@Misc{SC.8.Cai.2004,
  author = {Cai, X},
  title = {Using Linux Clusters for Full-Scale Simulation of Cardiac Electrophysiology},
  year = {2004},
  abstract = {},
  howpublished = {Invited talk at the fifth annual workshop on Linux Clusters for Super Computing, October 18-21, 2004, Link{\"o}ping, Sweden}
}

@Article{SC.4.Bounaim.2004.a,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Sensitivity of the ultrasonic CARI technique for breast tumor detection using a FETD scheme},
  year = {2004},
  abstract = {},
  journal = {Journal Ultrasonics},
  volume = {42},
  pages = {919--925}
}

@Article{SC.4.Cascarval.2004,
  author = {Cascarval, R C and Gesztesy, F and Holden, H and Latushkin, Y},
  title = {Spectral analysis of Darboux transformations for the focusing NLS hierarchy},
  year = {2004},
  abstract = {},
  journal = {Journal d'Analyse Mathematique},
  volume = {93},
  pages = {139--198}
}

@Article{SC.4.Sosonkina.2004,
  author = {Sosonkina, M and Saad, Y and Cai, X},
  title = {Using the Parallel Algebraic Recursive Multilevel Solver in Modern Physical Applications},
  year = {2004},
  abstract = {},
  journal = {Future Generation of Computer Systems},
  volume = {20},
  number = {3},
  pages = {489--500},
  note = {An earlier version appeared as Technical Report
                      2002-106 at the Minnesota Supercomputing
                      Institute}
}

@Article{SC.4.Mardal.2004,
  author = {Mardal, K -A and Winther, R},
  title = {Uniform Preconditioners for the Time Dependent Stokes Problem},
  year = {2004},
  abstract = {},
  journal = {Numerische Mathematik},
  volume = {98},
  number = {2},
  pages = {305--327}
}

@Inbook{SC.5.Cai.2004.a,
  author = {Cai, X and Sosonkina, M},
  title = {A Numerical Study of Some Parallel Algebraic Preconditioners},
  year = {2004},
  abstract = {},
  booktitle = {Parallel and Distributed Scientific and
                      Engineering Computing: Practice and Experience},
  editor = {Y. Pan and L. T. Yang},
  publisher = {Nova Science Publishers, Inc.},
  pages = {9--21},
  note = {An eariler version is included in Proceedings of
                      the IPDPS 2003 Conference, Nice, France,
                      April 2003, IEEE Computer Society}
}

@Inbook{SC.5.Cai.2004.b,
  author = {Cai, X and Lines, G and Tveito, A},
  title = {Parallel Solution of the Bidomain Equations with High Resolutions},
  year = {2004},
  abstract = {},
  booktitle = {Parallel Computing: Software Technology, 
                      											Algorithms, Architectures \& Applications},
  editor = {G. R. Joubert and W. E. Nagel and F. J. Peters and 
                      											W. V. Walter},
  publisher = {Elsevier Science},
  pages = {837--844}
}

@Inproceedings{SC.6.Huseby.2004,
  author = {Huseby, M and Langtangen, H P and Reksten, D E},
  title = {A Three-Dimensional Model for Noise Propagation over Realistic Terrain},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the 27th Scandinavian Symposium on Physical Acoustics},
  address = {Ustaoset, Norway}
}

@Inproceedings{SC.6.Sundnes.2004,
  author = {Sundnes, J},
  title = {Modeling the electrophysiology of ischemic ventricular cells},
  year = {2004},
  abstract = {},
  booktitle = {Proceedings of the WSEAS Conference on Mathematical Biology
                      and Ecology}
}

@Techreport{SC.7.Lie.2004,
  author = {Lie, K -A and Tveito, A},
  title = {Development of a staggered central difference scheme that does not use a spatial reconstruction},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report}
}

@Techreport{SC.7.Thorvaldsen.2004,
  author = {Thorvaldsen, T and Sundnes, J and Osnes, H},
  title = {Numerical Modeling of Isotropic, Hyperelastic Materials Undergoing Large Deformations},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-06}
}

@Techreport{SC.7.Bounaim.2004.b,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Modified fractional derivative model for the attenuation in human soft tissue: First numerical experiments},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report}
}

@Misc{SC.8.Bounaim.2004.a,
  author = {Bounaim, A and Holm, S and Chen, W and {\O}deg{\r a}rd, {\r A}},
  title = {Mathematics in Medicine: Mathematical and Numerical Modeling of a Clinical Technique for Breast Cancer Detection},
  year = {2004},
  abstract = {},
  howpublished = {Invited talk, International Conference on
                      Mathematics and its Applications, Department of
                      Mathematics and Computer Science}
}

@Misc{SC.8.Daehlen.2004,
  author = {D{\ae}hlen, M and Holden, H},
  title = {eVITenskap og Anvendelser (eVITA) -- forskning i en ny epoke},
  year = {2004},
  abstract = {},
  howpublished = {Misc}
}

@Techreport{SC.7.Nilssen.2004,
  author = {Nilssen, T K and Ingebrigtsen, L and Hanslien, M and Lines, G T and Tveito, A},
  title = {Resolution requirements for a 1D monodomain model for simulating},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-07}
}

@Techreport{SC.7.MacLachlan.2004,
  author = {MacLachlan, Mary C and Nielsen, B F and Tveito, A},
  title = {On the use of ST shifts to identify ischemic heart disease: Towards a computational approach},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-10}
}

@Misc{SC.8.Daehlen.2004.d,
  author = {D{\ae}hlen, M},
  title = {Det n{\o}dvendige gyldne trekl{\o}ver for nyskaping},
  year = {2004},
  abstract = {},
  howpublished = {Aftenposten, kronikk, 28. mars}
}

@Misc{SC.8.Geus.2004,
  author = {Geus, R and Skavhaug, O and Langtangen, H P},
  title = {Python Wrapper Tools; a Performance Study},
  year = {2004},
  abstract = {},
  howpublished = {Talk at the EuroPython 2004 Conference,
                      Gothenburg, Sweden},
  note = {Presented by R. Geus}
}

@Misc{SC.8.Daehlen.2004.c,
  author = {D{\ae}hlen, M},
  title = {Indeksteoremet},
  year = {2004},
  abstract = {},
  howpublished = {Schr{\o}dingers Katt}
}

@Misc{SC.8.Mardal.2004,
  author = {Mardal, K -A and Tveito, A},
  title = {Block Preconditioners for PDEs Arising from the Bi-domain Model of the Electrical Activity in the Heart},
  year = {2004},
  abstract = {},
  howpublished = {Talk at the Copper Mountain Conference,
                      Colorado, USA},
  note = {Presented by K.-A. Mardal}
}

@Misc{SC.8.Huseby.2004,
  author = {Huseby, M and Langtangen, H P and Reksten, D E},
  title = {A Three-Dimensional Model for Noise Propagation over Realistic Terrain},
  year = {2004},
  abstract = {},
  howpublished = {Talk at the 27th Scandinavian Symposium on
                      Physical Acoustics, Ustaoset, Norway},
  note = {Presented by M. Huseby}
}

@Misc{SC.8.Westile.2004,
  author = {Westlie, M and Mardal, K -A and Langtangen, H P},
  title = {Making a Python Interface to the C++ Library Diffpack},
  year = {2004},
  abstract = {},
  howpublished = {Talk at the EuroPython 2004 Conference,
                      Gothenburg, Sweden},
  note = {Presented by M. Westlie}
}

@Misc{SC.8.Nilssen.2004,
  author = {Nilssen, T K and Mardal, K -A},
  title = {Reuse of Standard Preconditioners for Higher--Order Time Discretizations of Parabolic PDEs},
  year = {2004},
  abstract = {},
  howpublished = {Talk at the Eigth Copper Mountain Conference on
                      Iterative Methods, Copper Mountain, CO, USA}
}

@Misc{SC.8.Nielsen.2004,
  author = {Nielsen, B F and Lysaker, M and Lines, G T and Tveito, A},
  title = {Parameter Identification Problems for Infarction Modelling},
  year = {2004},
  abstract = {},
  howpublished = {Presented at the 2004 SIAM Conference on the
                      Life Sciences, Portland, Oregon, USA}
}

@Misc{SC.8.Oedegaard.2004.a,
  author = {{\O}deg{\r a}rd, {\r A}},
  title = {FDM Solvers in Python - a Framework},
  year = {2004},
  abstract = {},
  howpublished = {Talk at the Europython 2004 Conference,
                      Gothenburg, Sweden}
}

@Misc{SC.8.Oedegaard.2004.b,
  author = {{\O}deg{\r a}rd, {\r A} and Mardal, K -A and Miller, P and Moe, Halvard and Skavhaug, O and Bruaset, A -M and Langtangen, H P},
  title = {High Performance Computing in Python},
  year = {2004},
  abstract = {},
  howpublished = {Tutorial, SC2004}
}

@Misc{SC.8.Sundnes.2004,
  author = {Sundnes, J and Lines, G T and Tveito, A},
  title = {Challenges in mathematical models of the heart},
  year = {2004},
  abstract = {},
  howpublished = {Presented at the SIAM Annual meeting, Portland, Oregon}
}

@Misc{SC.8.Lysaker.2004,
  author = {Lysaker, M and Nielsen, B F},
  title = {A level set framework for infarction modeling; an inverse problem},
  year = {2004},
  abstract = {},
  howpublished = {Presented at the 2004 SIAM Conference on the Life Sciences, Portland, Oregon, USA}
}

@Techreport{SC.7.Boehm.2004,
  author = {B{\"o}hm, Peter and Bruaset, Are Magnus and Langtangen, Hans Petter Petter and Mardal, Kent-Andre and Wachutka, Gerhard},
  title = {Diffpack Implementation of Edge-Based Non-Standard Finite Element Methods},
  year = {2004},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-11}
}

@Article{SC.4.Ingebrigtsen.2004,
  author = {Ingebrigtsen, T and Morgan, M K and Faulder, K and Ingebrigtsen, L and Sparr, T and Schirmer, H},
  title = {Bifurcation geometry and the presence of cerebral artery aneurysms},
  year = {2004},
  abstract = {},
  journal = {Journal of neurosergery},
  volume = {101},
  number = {1},
  pages = {108--13}
}

@Article{Maclachlan.2005.1,
  author = {MacLachlan, Mary C and Sundnes, Joakim and Lines, Glenn Terje},
  title = {Simulation of ST Segment Changes During Subendocardial Ischemia Using a Realistic 3-D Cardiac Geometry},
  year = {2005},
  abstract = {},
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {52},
  number = {5},
  pages = {799--807},
  pmid = {15887529}
}

@Inbook{SC.5.Cai.2003.c,
  author = {Cai, X and Bruaset, A M and Langtangen, H P and Lines, G T and Samuelsson, K and Shen, W and Tveito, A and Zumbusch, G},
  title = {Performance Modeling of PDE Solvers},
  year = {2003},
  abstract = {},
  booktitle = {Advanced Topics in Computational Partial Differential
                   Equations -- Numerical Methods and Diffpack Programming},
  editor = {H. P. Langtangen and A. Tveito},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computational Science and Engineering},
  pages = {361-399}
}

@Article{Thorvaldsen.2005.1,
  author = {Thorvaldsen, Tom and Osnes, Harald and Sundnes, Joakim},
  title = {A Mixed Finite Element Formulation for a Nonlinear, Transversely Isotropic Material Model for the Cardiac Tissue},
  year = {2005},
  abstract = {In this paper we present a mixed finite element method for modeling the passive properties of the myocardium. The passive properties are described by a non-linear, transversely isotropic, hyperelastic material model, and the myocardium is assumed to be almost incompressible.
Single-field, pure displacement-based formulations are known to cause numerical difficulties when applied to incompressible or slightly compressible material cases. This paper presents an alternative approach in the form of a mixed formulation, where a separately interpolated pressure field is introduced as a primary unknown in addition to the displacement field. Moreover, a constraint term is included in the formulation to enforce (almost) incompressibility. Numerical results presented in the paper demonstrate the difficulties related to employing a pure displacement-based method, applying a set of physically relevant material parameter values for the cardiac tissue. The same problems are not experienced for the proposed mixed method. We show that the mixed formulation provides reasonable numerical results for compressible as well as nearly incompressible cases, also in situations of large fiber stretches. There is good agreement between the numerical results and the underlying analytical models.   

},
  journal = {Computer Methods in Biomechanics and Biomedical Engineering},
  volume = {8},
  number = {6},
  pages = {369-379}
}

@Inproceedings{Thorvaldsen.2005.2,
  author = {Thorvaldsen, Tom and Sundnes, Joakim and Osnes, Harald},
  title = {A Mixed Formulation for Modeling the Mechanical Bahavior of the Heart},
  year = {2005},
  abstract = {In this paper we present a mixed finite element formulation for modeling the mechanical behavior of the cardiac tissue. The modeling may be divided into a passive and an active part. The passive properties are in this work described by a transversely isotropic, hyperelastic material model. Moreover, the myocardium is assumed to be almost incompressible. In advanced models the active contraction is strongly connected to electro-physiological processes in the muscle cells. In this paper we simplify the active stress contribution by modeling it as a linear function in time. Due to the fact that the heart muscle undergoes large deformations during contraction, both material and geometrical non-linearities must be taken into account in the analysis. 

Numerical results are presented for different activation patterns applied to a box-shaped test specimen, which represents a piece of the myocardial wall. The proposed mixed method offers reasonable results in the modeling of the mechanical behavior of the test specimen. 
},
  booktitle = {MekIt05 - Third national conference om Computational Mechanics, Trondheim May 11-12},
  editor = {Bj{\o}rn Skallerud and Helge I. Andersson},
  pages = {327--341},
  publisher = {Tapir Academic Press}
}

@Misc{Thorvaldsen.2005.3,
  author = {Thorvaldsen, Tom and Osnes, Harald and Sundnes, Joakim},
  title = {A Mixed FEM for Modeling the Passive Mechanical Properties of the Myocardium},
  year = {2005},
  abstract = {},
  howpublished = {Talk at the SIAM Conference on Computational Science \& Engineering}
}

@Article{Cai.2005.1,
  author = {Cai, Xing and Jeberg, Peter and Langtangen, Hans Petter},
  title = {A numerical method for computing the profile of weld pool surfaces},
  year = {2005},
  abstract = {},
  journal = {International Journal for Computational Methods in Engineering Science and Mechanics},
  volume = {6},
  number = {2},
  pages = {115-125}
}

@Article{Cai.2005.2,
  author = {Cai, Xing and Pedersen, Geir Kleivstul and Langtangen, Hans Petter},
  title = {A parallel multi-subdomain strategy for solving Boussinesq water wave equations},
  year = {2005},
  abstract = {},
  journal = {Advances in Water Resources},
  volume = {28},
  number = {3},
  pages = {215-233}
}

@Article{Cai.2005.3,
  author = {Cai, Xing and Langtangen, Hans Petter and Moe, Halvard},
  title = {On the performance of the Python programming language for serial and parallel scientific computations},
  year = {2005},
  abstract = {},
  journal = {Scientific Programming},
  volume = {13},
  number = {1},
  pages = {31-56}
}

@Inproceedings{Staff.2005.1,
  author = {Staff, Gunnar Andreas and Mardal, Kent-Andre},
  authorURLs = {http://www.simula.no/portal\_memberdata/gunnaran and /people/kent-and},
  title = {The accuracy of the velocity approximation with respect to the pressure approximation for incompressible fluid flow},
  year = {2005},
  abstract = {We consider the accuracy of the velocity approximation with respect to the pressure approximation using various standard finite element methods for incompressible fluid flow. Of particular interest is the case where the hydrostatic part of the pressure is dominating, which implies that the pressure may be large relative to the velocity. Some standard mixed elements like the Mini element and the Taylor-Hood element handle this case well, while others like the $P\_2-P\_0$ element and the Crouzeix-Raviart element do not. 
This seems to be related to the fact that the Taylor-Hood and the Mini elements yield a stable approximation of the mixed Poisson problem. },
  booktitle = {Third National Conference  on Computational Mechanics (MekIT 05)},
  editor = {Bj{\o}rn Skallerud and Helge I. Andersson},
  pages = {297-311},
  publisher = {Tapir Academic Press},
  address = {Trondheim, May 11-12},
  edition = {first}
}

@Article{Hjelle.2005.1,
  author = {Hjelle, {\O}yvind and D{\ae}hlen, Morten},
  title = {Multilevel Least Squares Approximation of Scattered Data over Binary Triangulations},
  year = {2005},
  abstract = {An adaptive method for approximating huge scattered data sets is presented. The approximation scheme generates multilevel triangulations obtained using a subdivision scheme known as longest edge bisection. Nested function spaces are defined over the multilevel triangulations. The approximation problem is solved by successive refinement of the triangulation while iterative methods
are used for solving a system of linear equations at intermediate levels of the multi-level scheme. Regularization terms are coupled with a standard least squares formulation to guarantee uniqueness and control smoothness of the solution.},
  journal = {Computing and Visualization in Science},
  volume = {8},
  number = {2},
  pages = {81-92}
}

@Misc{Cai.2005.7,
  author = {Cai, Xing and Pedersen, Geir Kleivstul and Langtangen, Hans Petter},
  title = {Solving Boussinesq water wave equations on parallel computers},
  year = {2005},
  abstract = {},
  howpublished = {Talk at the International Workshop on Numerical Ocean Modeling, Oslo, Norway}
}

@Misc{Lysaker.2005.1,
  author = {Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik},
  title = {A level set framework for infarction modeling; an inverse problem.},
  year = {2005},
  abstract = {},
  howpublished = {Invited speaker at the Fifth Winter School in Computational Mathematics, Geilo}
}

@Article{Sundnes.2005.2,
  author = {Sundnes, Joakim and Lines, Glenn Terje and Tveito, Aslak},
  title = {An operator splitting method for solving the bidomain equations coupled to a volume conductor model for the torso},
  year = {2005},
  abstract = {In this paper we present a numerical method for the bidomain model, which describes the electrical activity in the heart. The model consists of two partial differential equations (PDEs), which are coupled to systems of ordinary differential equations (ODEs) describing electrochemical reactions in the cardiac cells. Many applications require coupling these equations to a third PDE, describing the electrical fields in the torso surrounding the heart. The resulting system is challenging to solve numerically, because of its complexity and very strict resolution requirements in time and space. We propose a method based on operator splitting and a fully coupled discretization of the three PDEs. Numerical experiments show that for simple simulation cases and fine discretizations, the algorithm is second-order accurate in space and time.
},
  journal = {Mathematical biosciences},
  volume = {194},
  number = {2},
  pages = {233--248},
  pmid = {15854678}
}

@Misc{Zhang.2004.1,
  author = {Zhang, Yongjie and Bajaj, Chandrajit and Hughes, Thomas and Liu, Wing Kam and Chen, Grace and Wang, Xiaodong and Lysaker, Ola Marius and Tarrou, Christian},
  title = {Finite Element Meshing for Cardiac Analysis},
  year = {2004},
  abstract = {},
  howpublished = {Poster Presentation, 13th International Meshing Roundtable, Williamsburg, Virginia, September19-22}
}

@Misc{Nielsen.2005.2,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius},
  title = {The classical inverse ECG problem},
  year = {2005},
  abstract = {},
  howpublished = {Invited speaker at the Fifth Winter School in Computational Mathematics, Geilo}
}

@Misc{Odegard.2005.1,
  author = {{\O}deg{\r a}rd, {\r A}smund},
  title = {FDM Solvers in Python - a Framework},
  year = {2005},
  abstract = {Solving Partial Differential Equations with Python and Finite Differences.

In this talk we present a framework for solving Partial Differential Equations
(PDEs) with the Finite Differences Method (FDM). We implement a flexible
framework in Python for rapid prototyping of solvers for PDEs both for scalar
and parallel computers. The framework provide an abstraction for FDM stencils
realized as a class, and the problem is specified in terms of a set of stencils
defined over some FDM grid. The framework provides both explicit and implicit
solvers.  In case of a parallel computer, the communication-patterns described
by the stencils are used to optimize the distribution of unknowns on the
available processors.

This talk focus on the parallel aspects of the framework, as well as how fairly
advanced solvers like e.g. multigrid solvers can be implemented using the set of
stencils defining a problem as a linear operator.},
  howpublished = {Conference Talk, Siam CSE'05, Orlando, USA}
}

@Misc{Maclachlan.2004.1,
  author = {MacLachlan, Mary C and Sundnes, Joakim and Lines, Glenn Terje},
  title = {Modelling ST segment changes during myocardial ischemia},
  year = {2004},
  abstract = {},
  howpublished = {Presented at the SIAM Conference on the Life Sciences:  Portland, Oregon}
}

@Misc{Lines.2003.1,
  author = {Lines, Glenn Terje and Cai, Xing and Sundnes, Joakim},
  title = {Parallel algorithms for simulating the electrical activity of the heart},
  year = {2003},
  abstract = {},
  howpublished = {Presented at the Dagstuhl seminar "Challenges in computational science and engineering"},
  note = {Presented by Joakim Sundnes, March 2003.}
}

@Misc{Sundnes.2005.4,
  author = {Sundnes, Joakim and Hanslien, Monica and Tveito, Aslak},
  title = {An unconditionally stable numerical method for the Luo-Rudy I model used in defibrillation},
  year = {2005},
  abstract = {},
  howpublished = {Presented at Tulane University, New Orleans},
  note = {Computational science seminar, Tulane University, New Orleans, February 2005.}
}

@Misc{Sundnes.2005.5,
  author = {Sundnes, Joakim},
  title = {Computing the electrical activity in the heart},
  year = {2005},
  abstract = {},
  howpublished = {Presented at the Department of Mathematics, University of Oslo},
  note = {Popmat seminar, April 2005.}
}

@Misc{Sundnes.2001.1,
  author = {Sundnes, Joakim and Lines, Glenn Terje and Mardal, Kent-Andre and Tveito, Aslak},
  title = {On numerical techniques for the bidomain model},
  year = {2001},
  abstract = {},
  howpublished = {Presented at the SIAM Annual meeting, San Diego, California}
}

@Article{Linge.2005.1,
  author = {Linge, Svein and Lines, Glenn Terje and Sundnes, Joakim},
  title = {Solving the heart mechanics equations with Newton and quasi Newton methods - a comparison},
  year = {2005},
  abstract = {The non-linear elasticity equations of heart mechancis are solved while emulating the effects of a propagating activation wave. The dynamics of a 1 cm^3 slab of active cardiac tissue was simulated as the electrical wave traversed the muscular heart wall transmurally. The regular Newton (Newton-Raphson) method was compared to two modified Newton approaches, and also to a third approach that delayed update only of some selected Jacobian elements. In addition, the impact of changing the time step (0.01 ms, 0.1 ms and 1 ms) and the relative nonlinear convergence tolerance (10^-4, 10^-3 and 10^-2) was investigated. Updating the Jacobian only when slow convergence occured was by far the most efficient approach, giving time savings of 83-96\%. For each of the four methods, CPU times were reduced by 48-90\% when the time step was increased by a factor 10. Increasing the convergence tolerance by the same factor gave time savings of 3-71\%. Different combinations of activation wave speed, stress rate and bulk modulus revealed that the fastest method became relatively even faster as stress rate and bulk modulus were decreased, while the activation speed had negligible influence in this respect.},
  journal = {Computer Methods in Biomechanics and Biomedical Engineering},
  volume = {8},
  number = {1},
  pages = {1--8}
}

@Inproceedings{Linge.2005.2,
  author = {Linge, Svein},
  title = {Biology and modelling of cardiovascular tissue dynamics},
  year = {2005},
  abstract = {It is well known that shape and function of heart and blood vessels are influenced by blood flow and pressure. The heart muscle grows to cope with the demands of a sportsman, the inner diameter of vessels increases with increased shear stress from blood flow, and vessel walls themselves get thicker with elevated blood pressure. Such effects are observed in healthy people with varying activity levels, and also in patients with different malfunctions. Modelling and simulating the function and the adaptation of heart and blood vessels to mechanical load is needed to fully understand the complex relations involved. A thorough understanding of these mechanisms is fundamental for good patient diagnosis and treatment planning. Also, simulations will reduce the need for time consuming and stressful invasive measurements. In this brief review, we present some of the relevant biology, and address important modelling approaches of cardiovascular functioning and adaptation.},
  booktitle = {Third National Conference on Computational Mechanics (MekIT 05)},
  editor = {Bj{\o}rn Skallerud and Helge I. Andersson},
  pages = {211-225},
  publisher = {Tapir Academic Press},
  address = {Trondheim, Norway May 11-12}
}

@Inproceedings{refereedinproceedingsreference.2005-06-03.9082538668,
  author = {No names specified},
  title = {},
  year = {},
  abstract = {}
}

@Inbook{inbookreference.2005-06-03.1880411951,
  author = {No names specified},
  title = {},
  year = {},
  abstract = {}
}

@Book{Tveito.2005.1,
  author = {Tveito, Aslak and Langtangen, Hans Petter and Nielsen, Bj{\o}rn Fredrik and Cai, Xing},
  title = {Introduction to Models and Methods in Computational Science},
  year = {2005},
  abstract = {},
  publisher = {Submitted to publisher}
}

@Misc{Mardal.2003.1,
  author = {Mardal, Kent-Andre and Huerta, Audrey and Langtangen, Hans Petter and Harry, Dennis},
  title = {Numerical Modeling of Crust and Mantle Movement},
  year = {2003},
  abstract = {},
  howpublished = {Talk presented at SIAM Conference on Mathematical and Computational Issues in the Geosciences (GS0
3),  March 17-20, 2003, Austin, Texas.}
}

@Article{Hanslien.2004.2,
  author = {Hanslien, M and Karlsen, K H and Tveito, A},
  title = {A maximum principle for an explicit finite difference scheme approximating the Hodgkin-Huxley model},
  year = {2005},
  abstract = {},
  journal = {BIT},
  volume = {45},
  number = {4},
  pages = {725 - 741}
}

@Article{Maclachlan.2005.3,
  author = {MacLachlan, Mary C and Sundnes, Joakim and Spiteri, Raymond J},
  title = {A comparison of non-standard solvers for ODEs describing cellular reactions in the heart},
  year = {2007},
  abstract = {},
  journal = {Computer Methods in Biomechanics and Biomedical Engineering},
  volume = {10},
  number = {5},
  pages = {317--326}
}

@Inproceedings{Staff.2005.2,
  author = {Staff, Gunnar Andreas and Mardal, Kent-Andre and Nilssen, Trygve Kastberg},
  title = {Preconditioning of fully implicit Runge-Kutta schemes for parabolic PDEs},
  year = {2005},
  abstract = {Recently, the authors  introduced a preconditioner
  for the linear systems that arise from fully implicit Runge-Kutta time
  stepping schemes applied to parabolic PDEs \cite{mns}.
  The preconditioner was a block Jacobi preconditioner, where each of the blocks
 were based on
  standard preconditioners for low-order time discretizations like
  implicit Euler or Crank-Nicolson.
  It was proven that the preconditioner is optimal with respect to the timestep
and the discretization parameter in space.

  In this paper we will improve the convergence by considering other preconditio
ners like the upper and the lower block
  Gauss-Seidel preconditioners, both in a left and right preconditioning setting
. Finally, we improve the condition number by using a generalized Gauss-Seidel p
reconditioner.
},
  booktitle = {SIMS05. 46th Conference on Simulation and Modeling, Trondheim, October 13-14},
  editor = {J{\o}rn Amundsen, Helge I. Andersson, Elena Celledoni, Tommy Gravdahl, Finn A. Michelsen, Henrik R. Nagel, Thorvald Natvig},
  pages = {315-324},
  publisher = {Tapir Academic Press}
}

@Misc{Hjelle.2005.2,
  author = {Hjelle, {\O}yvind and D{\ae}hlen, Morten},
  title = {Triangulations and Applications},
  year = {2005},
  abstract = {},
  howpublished = {Lecture Notes, University of Oslo},
  note = {Lecture notes, University of Oslo, 227 pages}
}

@Article{Nielsen.2004.1,
  author = {Nielsen, Bj{\o}rn Fredrik and Tveito, Aslak and Hackbusch, Wolfgang},
  title = {Preconditioning by inverting the Laplacian; an analysis of the eigenvalues},
  year = {2008},
  abstract = {},
  journal = {IMA Journal of Numerical Analysis},
  number = {doi: 10.1093/imanum/drm018}
}

@Misc{Mardal.2005.5,
  author = {Mardal, Kent-Andre and Nielsen, Bj{\o}rn Fredrik and Tveito, Aslak},
  title = {Preconditioners for Elliptic Problems with Variable Coefficients},
  year = {2005},
  abstract = {},
  howpublished = {Presented at the SIAM Conference on Mathematical and Computational Issues in the Geosciences, Avignon, France}
}

@Techreport{Nilssen.2005.1,
  author = {Nilssen, Trygve Kastberg},
  title = {Weakly positve definite matrices},
  year = {2005},
  abstract = {A weakly positive definite matrix is defined to be a matrix
which can be written as a product of two positive definite matrices. This
paper proves that a matrix is weakly positive definite if and only if the
real eigenvalues are positive. The main components of the proof are
Schur decomposition and mathematical induction.},
  institution = {Simula Research Laboratory},
  type = {Preprint},
  number = {2005-07}
}

@Article{Lysaker.2006.1,
  author = {Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik},
  title = {Towards a level set framework for infarction modeling: An inverse problem},
  year = {2006},
  abstract = {The purpose of this paper is to introduce a level set framework suitable for identifying heart infarctions. We do this by introducing a modified Monodomain model, and an output least squares formulation comparing simu- lation results with observation data. In this way we obtain a flexible method- ology allowing us to approximately determine the characteristics of infarctions with rather complex geometrical properties. Our approach involves a CPU demanding minimization problem. This problem is solved by applying an ad- joint equation enabling efficient differentiation of the involved cost-functional. Finally, our theoretical findings are illuminated by a series of numerical exper- iments.},
  journal = {International Journal of Numerical Analysis and Modeling},
  volume = {3},
  number = {4},
  pages = {377-394}
}

@Inproceedings{Hanslien.2005.2,
  author = {Hanslien, Monica and Sundnes, Joakim and Lines, Glenn Terje T},
  title = {Numerical simulations of cardiac arrhythmias and defibrillation},
  year = {2005},
  abstract = {},
  booktitle = {MekIT'05 - Third national conference on Computational Mechanics, Trondheim, Norway May 11-12},
  editor = {Bj{\o}rn Skallerud and Helge I. Andersson},
  pages = {135-144},
  publisher = {Tapir Academic Press},
  address = {Trondheim}
}

@Inproceedings{Nielsen.2005.3,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tarrou, Christian and Abildgaard, Andreas and MacLachlan, Mary and Tveito, Aslak},
  title = {On the Use of ST-Segment Shifts and Mathematical Models for Identifying Ischemic Heart Disease},
  year = {2005},
  abstract = {},
  booktitle = {Computers in Cardiology 2005, Lyon, France, September 25-28},
  editor = {A. Murray},
  pages = {1005--1008},
  publisher = {IEEE}
}

@Misc{Nielsen.2005.4,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and MacLachlan, Mary and Tveito, Aslak},
  title = {On the use of level set techniques for identifying heart infarctions},
  year = {2005},
  abstract = {},
  howpublished = {Presented at the Level Set Methods for Direct and Inverse Problems workshop in Linz, Austria}
}

@Misc{Nielsen.2005.5,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tarrou, Christian and MacLachlan, Mary and Abildgaard, Andreas and Tveito, Aslak},
  title = {On the Use of ST-Segment Shifts and Mathematical Models for Identifying Ischemic Heart Disease},
  year = {2005},
  abstract = {},
  howpublished = {Presented at the Computers in Cardiology conference in Lyon, France}
}

@Misc{Odegard.2005.2,
  author = {{\O}deg{\r a}rd, {\r A}smund},
  title = {PySE - Python Stencil Environment},
  year = {2005},
  abstract = {},
  howpublished = {Conference Talk, FEniCS'05, Chicago},
  note = {Talk on PySE given at the FEniCS'05 meeting.}
}

@Misc{Nielsen.2005.6,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tarrou, Christian and MacLachlan, Mary and Abildgaard, Andreas and Tveito, Aslak},
  title = {Kan vi identifisere hjerteinfarkt ved hjelp av datamaskiner?},
  year = {2005},
  abstract = {},
  howpublished = {Presented at the BeMatA seminar (Norwegian Research Council), Hurdal}
}

@Misc{Lysaker.2005.2,
  author = {Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik and Tveito, Aslak and Lines, Glenn Terje and Sundnes, Joakim and Tarrou, Christian},
  title = {Kan man simulere et hjerteslag?},
  year = {2005},
  abstract = {},
  howpublished = {LAMIS - SOMMERKURS "Matematikk {\textendash}
med r{\o}tter og vinger
", Asker}
}

@Article{Hinsen.2006.1,
  author = {Hinsen, K and Langtangen, H P and Skavhaug, O and {\O}deg{\r a}rd, {\r A}},
  title = {Using BSP and Python to Simplify Parallel Programming},
  year = {2006},
  abstract = {Scientific computing is usually associated with compiled languages for maximum efficiency. However, in a typical application program, only a small part of the code is time-critical and requires the efficiency of a compiled language. It is often advantageous to use interpreted high-level languages for the remaining tasks, adopting a mixed-language approach. This will be demonstrated for Python, an interpreted object-oriented high-level language that is well suited for scientific computing. Particular attention is paid to high-level parallel programming using Python and the BSP model. We explain the basics of BSP and how it differs from other parallel programming tools like MPI. Thereafter we present an application of Python and BSP for solving a partial differential equation from computational science, utilizing high-level design of libraries and mixed-language (Python{\textendash}C or Python{\textendash}Fortran) programming.},
  journal = {Future Generation Computer Systems},
  volume = {22},
  number = {1-2},
  pages = {123--157}
}

@Misc{Skavhaug.2005.1,
  author = {Skavhaug, Ola and Langtangen, Hans Petter},
  title = {A basic introduction to Python},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at Uppsala University}
}

@Misc{Skavhaug.2005.2,
  author = {Skavhaug, Ola},
  title = {Visions of Python in Scientific Computing},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at Uppsala University}
}

@Misc{Skavhaug.2005.3,
  author = {Skavhaug, Ola and Certic, Ondrej},
  title = {Swiginac - Extending Python with Symbolic Mathematics},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at FEniCS05 in Chicago}
}

@Article{Glimsdal.2006.1,
  author = {Glimsdal, Sylfest and Pedersen, Geir Kleivstul and Atakan, Kuvvet and Harbitz, Carl B and Langtangen, Hans Petter and L{\o}vholt, Finn},
  title = {Propagation of the Dec. 26, 2004 Indian Ocean Tsunami: effects of dispersion and source characteristics},
  year = {2006},
  abstract = { This work presents  numerical
  simulations of the tsunami generated by the Dec. 26, 2004
  Sumatra-Andaman earthquake.  The numerical models employed include
  the linear shallow water equations, a weakly nonlinear and
  dispersive model (Boussinesq equations), and ray theory for linear
  hydrostatic waves. Four different tsunami sources, constructed from
  inversion models based on seismological recordings, are studied.

  We have investigated the sensitivity to the choice of mathematical
  model, grid resolution, source parameters, and delay of tsunami
  generation at the northern part of the source area.  The results are
  compared to surface elevation recordings. Numerical simulations show
  that the effect of dispersion may modify the waves (slightly) during
  long propagation times only, and dispersion is not observed in the
  tsunami generation phase. In some shallow regions, on the other
  hand, nonlinear steepening of the wave front may enhance dispersion,
  and undular bores may be produced, which cannot be modeled by the
  standard shallow water equations commonly used for tsunami
  simulation.  The sensitivity analysis results provide important
  insights to the source complexity of the Dec. 26, 2004 earthquake.
},
  journal = {The Int. J. of Fluid Mech. Res.},
  volume = {33},
  number = {1},
  pages = {15--43}
}

@Misc{Glimsdal.2005.1,
  author = {Glimsdal, Sylfest and Pedersen, Geir Kleivstul and Shuvalov, Valery and Dypvik, Henning and Langtangen, Hans Petter and Kristiansen, {\O}yvind},
  title = {Tsunami generated by the Mj{\o}lnir impact},
  year = {2005},
  abstract = {},
  howpublished = {Poster on Lunar and Planetary Science Conference XXXVI}
}

@Phdthesis{Ingebrigtsen.2005.1,
  author = {Ingebrigtsen, Linda Irene},
  title = {On the local error in operator splitting schemes applied to diffusion-convection-reaction equations},
  year = {2005},
  abstract = {},
  school = {University of Oslo}
}

@Techreport{Skavhaug.2005.4,
  author = {Skavhaug, Ola and Mardal, Kent-Andre and Langtangen, Hans Petter},
  title = {
A Python Framework for Verifying Codes for Numerical Solutions of Partial Differential Equations},
  year = {2005},
  abstract = {},
  institution = {Simula Research Laboratory},
  type = {Simula Preprint},
  number = {2005-18}
}

@Article{Mardal.2006.1,
  author = {Mardal, Kent-Andre and Winther, Ragnar},
  title = {An observation on Korn's inequality for nonconforming finite element methods},
  year = {2006},
  abstract = {},
  journal = {Math. Comp. },
  volume = {75},
  number = {253},
  pages = {1-6}
}

@Misc{Alnes.2005.2,
  author = {Aln{\ae}s, Martin Sandve},
  title = {Scientific Computing in Python},
  year = {2005},
  abstract = {},
  howpublished = {Tutorial, ASIM 2005}
}

@Article{Hanslien.2006.1,
  author = {Hanslien, M and Karlsen, K H and Tveito, A},
  title = {On a finite difference scheme for a Beeler-Reuter based model of cardiac electrical activity},
  year = {2006},
  abstract = {},
  journal = {Int J of Num Anal and Mod},
  volume = {3},
  number = {4},
  pages = {395-412}
}

@Misc{Bruaset.2005.2,
  author = {Bruaset, Are Magnus and Aln{\ae}s, Martin Sandve and Langtangen, Hans Petter and Mardal, Kent-Andre and Moe, Halvard and Skavhaug, Ola and {\O}deg{\r a}rd, {\r A}smund},
  title = {Scientific Computing in Python},
  year = {2005},
  abstract = {},
  howpublished = {Tutorial ASIM 2005}
}

@Book{Langtangen.2005.1,
  author = {Langtangen, Hans Petter},
  title = {Python Scripting for Computational Science},
  year = {2005},
  abstract = {},
  publisher = {Springer},
  edition = {second}
}

@Inbook{Douglas.2005.3,
  author = {Douglas, Craig C and Langtangen, Hans Petter},
  title = {Hardware-assisted algorithms},
  year = {2005},
  abstract = {},
  booktitle = {Accuracy and Reliability in Scientific Computing},
  editor = {B. Einarsson},
  publisher = {SIAM},
  chapter = {11},
  pages = {241-250}
}

@Inproceedings{Holmas.2005.1,
  author = {Holm{\r a}s, K and Nossen, J and Mortensen, D and Schulkes, R and Langtangen, Hans Petter},
  title = {Simulation of wavy stratified two-phase flow using computational fluid dynamics (CFD)},
  year = {2005},
  abstract = {},
  booktitle = {12th International Conference on Multiphase Production Technology '05},
  editor = {A. Hunt},
  publisher = {BHR Group Ltd., Barcelona, Spain, May 25-27 }
}

@Misc{Langtangen.2005.2,
  author = {Langtangen, Hans Petter},
  title = {Tsunamis generated by earth-asteroide impacts},
  year = {2005},
  abstract = {},
  howpublished = {Talk},
  note = {Invited talk at the Launch of the EU-project CENS-CMA, Center of Mathematics for Applications, Oslo, Norway}
}

@Inbook{Douglas.2005.1,
  author = {Douglas, C C and Langtangen, Hans Petter},
  title = {General methods for implementing reliable and correct software: C, C++, and Python},
  year = {2005},
  abstract = {},
  booktitle = {Accuracy and Reliability in Scientific Computing},
  editor = {B. Einarsson},
  publisher = {SIAM},
  chapter = {8.2, 8.3 and 8.6},
  pages = {136-142, 142-149 and 162-170 }
}

@Techreport{Thorvaldsen.2005.4,
  author = {Thorvaldsen, Tom and Sundnes, Joakim and Osnes, Harald},
  title = {Non-Linear Elasticity with Applications to Heart Mechanics},
  year = {2005},
  abstract = {This report gives an introduction to mathematical and numerical modeling of large-deformation elasticity problems. Basic non-linear continuum mechanics is presented along with a set of hyperelastic material models. Two alternative finite element formulations are described for solving such problems, for compressible and almost incompressible materials. Simulation results are presented for both methods. 


The elasticity solver is employed in simulating the mechanical behavior of the heart. Mathematical models are presented for describing the passive behavior of cardiac tissue and the active force development. Moreover, we show mathematical models and numerical algorithms for simulating the large deformations of the left ventricle during a heart beat. 

},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2005-16}
}

@Inbook{Cai.2006.2,
  author = {Cai, Xing and Langtangen, Hans Petter},
  title = {Parallelizing PDE solvers using the Python programming language},
  year = {2006},
  abstract = {},
  booktitle = {Numerical Solution of Partial Differential Equations on Parallel Computers},
  editor = {A. M. Bruaset and A. Tveito},
  publisher = {Springer},
  series = {Lecture Notes in Computational Science and Engineering},
  pages = {295-325}
}

@Inbook{Cai.2006.3,
  author = {Cai, Xing and Lines, Glenn Terje},
  title = {Full-scale simulation of cardiac electrophysiology on parallel computers},
  year = {2006},
  abstract = {},
  booktitle = {Numerical Solution of Partial Differential Equations on Parallel Computers},
  editor = {A. M. Bruaset and A. Tveito},
  publisher = {Springer},
  series = {Lecture Notes in Computational Science and Engineering},
  pages = {385--411}
}

@Misc{Cai.2005.4,
  author = {Cai, Xing and Pedersen, Geir Kleivstul and Langtangen, Hans Petter and Glimsdal, Sylfest},
  title = {Parallel simulation of tsunamis using a hybrid software approach},
  year = {2005},
  abstract = {},
  howpublished = {Talk at ParCo 2005 Conference, 13 - 16 September, Malaga, Spain}
}

@Misc{Cai.2005.5,
  author = {Cai, Xing and Langtangen, Hans Petter},
  title = {Parallelization of PDE codes},
  year = {2005},
  abstract = {},
  howpublished = {Talk at the CMA Workshop on High-Performance Computing in Physics, November 4, Oslo, Norway}
}

@Misc{Cai.2003.1,
  author = {Cai, Xing and Lines, Glenn Terje and Tveito, Aslak},
  title = {Toward extremely high-resolution simulation of human heart},
  year = {2003},
  abstract = {},
  howpublished = {Talk at the ParCo 2003 Conference, 2 - 5 September 2003, Dresden, Germany}
}

@Misc{Cai.2003.2,
  author = {Cai, Xing and Sosonkina, Masha},
  title = {A numerical study of some parallel algebraic preconditioners},
  year = {2003},
  abstract = {},
  howpublished = {Talk at the IPDPS 2003 Conference, April 22-26, 2003, Nice, France}
}

@Article{Schroll.2005.2,
  author = {Schroll, Achim and Lines, Glenn Terje and Tveito, Aslak},
  title = {On the Accuracy of Operator Splitting as Applied to Discretized Reaction Diffusion Systems},
  year = {2007},
  abstract = {},
  journal = {International Journal of Computer Mathematics},
  volume = {84},
  number = {6},
  pages = {871 - 885}
}

@Inbook{Sulebak.2003.1,
  author = {Sulebak, Jan Rasmus and Hjelle, {\O}yvind},
  title = {Multiresolution Spline Models and their Applications in Geomorphology},
  year = {2003},
  abstract = {},
  booktitle = {Concepts and Modelling in Geomorphology: International Perspectives},
  editor = {I. S. Evans, R. Dikau, E. Tokunaga, H. Ohmori and M. Hirano},
  publisher = {TERRABUB},
  address = {Tokyo},
  pages = {221--237}
}

@Article{Staff.2006.1,
  author = {Staff, Gunnar Andreas and Mardal, Kent-Andre and Nilssen, Trygve Kastberg},
  title = {Preconditioning of fully implicit Runge-Kutta schemes for parabolic PDEs},
  year = {2006},
  abstract = {Recently, the authors  introduced a preconditioner
  for the linear systems that arise from fully implicit Runge-Kutta time
  stepping schemes applied to parabolic PDEs \cite{mns}.
  The preconditioner was a block Jacobi preconditioner, where each of the blocks were based on
  standard preconditioners for low-order time discretizations like
  implicit Euler or Crank-Nicolson.
  It was proven that the preconditioner is optimal with respect to the timestep
and the discretization parameter in space.

  In this paper we will improve the convergence by considering other preconditioners like the upper and the lower block
  Gauss-Seidel preconditioners, both in a left and right preconditioning setting. Finally, we improve the condition number by using a generalized Gauss-Seidel preconditioner.
},
  journal = {Modeling, Identification and Control},
  volume = {27},
  number = {2},
  pages = {109-123},
  note = {An earlier version of this paper var publisehd in SIMS05. 46th Conference on Simulation and Modeling, Trondheim, October 13-14, edited by J{\o}rn Amundsen, Helge I. Andersson, Elena Celledoni, Tommy Gravdahl, Finn A. Michelsen, Henrik R. Nagel, Thorvald Natvig. Tapir Academic Press, pages 315-324, 2005.}
}

@Article{Sundnes.2006.1,
  author = {Sundnes, Joakim and Nielsen, Bj{\o}rn Fredrik and Mardal, Kent-Andre and Cai, Xing and Lines, Glenn Terje and Tveito, Aslak},
  title = {On the computational complexity of the bidomain and the monodomain models of electrophysiology},
  year = {2006},
  abstract = {},
  journal = {Annals of biomedical engineering},
  volume = {34},
  number = {7},
  pages = {1088-1097}
}

@Misc{Cai.2006.5,
  author = {Cai, Xing},
  title = {A Hybrid Software Framework for Parallel Tsunami Simulations},
  year = {2006},
  abstract = {},
  howpublished = {Talk at SIAM PP06 Conference, February 22-24, 2006, San Francisco}
}

@Misc{Cai.2006.6,
  author = {Cai, Xing},
  title = {Parallelizing serial PDE software using a generic approach},
  year = {2006},
  abstract = {},
  howpublished = {Seminar at the University of Arizona, February 27}
}

@Article{Nielsen.2006.1,
  author = {Nielsen, Bj{\o}rn Fredrik and Arntzen, Ole Jakob},
  title = {Effective coefficients for elliptic boundary value problems},
  year = {2006},
  abstract = {},
  journal = {journal for publication}
}

@Article{Vazquez.2004.1,
  author = {V{\a\'a}zquez, Ariel A and Nielsen, Bj{\o}rn Fredrik},
  title = {The multigrid algorithm applied to a degenerate equation; a convergence analysis},
  year = {2008},
  abstract = {},
  journal = {Journal of Computational and Applied Mathematics},
  number = {doi: 10.1016/j.cam.2008.07.026}
}

@Book{Bruaset.2006.1,
  editor = {Bruaset, Are Magnus and Tveito, Aslak},
  title = {Numerical Solution of Partial Differential Equations on Parallel Computers},
  year = {2006},
  abstract = {},
  publisher = {Springer},
  volume = {LNCSE 51},
  isbn = {3-540-29076-1}
}

@Article{Bounaim.2006.1,
  author = {Bounaim, Aicha and Holm, Sverre and Chen, Wen and {\O}deg{\r a}rd, {\r A}smund},
  title = {Detectability of Breast Lesions with the CARI Ultrasonography Using a Bioacoustic Computational Approach },
  year = {2006},
  abstract = {},
  journal = {Computers \& Mathematics with Applications},
  volume = {54},
  number = {1}
}

@Phdthesis{Odegard.2006.1,
  author = {{\O}deg{\r a}rd, {\r A}smund},
  title = {Applications of high level software for parallel solution of Partial Differential Equations},
  year = {2006},
  abstract = {},
  school = {Informatics department, University of Oslo}
}

@Inproceedings{Lysaker.2006.3,
  author = {Lysaker, Ola Marius and Tarrou, Christian and Abildgaard, Andreas and Nielsen, Bj{\o}rn Fredrik},
  title = {On the Use of Computers to Simulate Cardiac Functions},
  year = {2006},
  abstract = {In this paper we will present a framework for simulating and visualizing the electrical activity in the human heart. Based on MR-scans we are able to build heart-in-torso grids that include lungs and muscle fibre orientations. Using such grids, along with advanced mathematical models and high speed computers, we simulate and analyse cardiac functions. Computer simulations can be a powerful tool for gaining insight into the myocardium. With this approach the researcher controls all the parameters in the cardiac model, and can effectively evaluate the effect changes in these parameters have on the simulation result.},
  booktitle = {The 24th International EuroPACS Conference},
  chapter = {Clinical Experiences},
  pages = {1-4},
  publisher = {EuroPACS}
}

@Article{Mardal.2006.2,
  author = {Mardal, Kent-Andre and Nilssen, Trygve Kastberg},
  title = {Reuse of Standard Preconditioners for Higher--Order Time Discretizations of Parabolic PDEs},
  year = {2006},
  abstract = {},
  journal = {Journal of Numerical Mathematics},
  volume = {14},
  number = {2},
  pages = {103-122}
}

@Book{Sundnes.2006.2,
  author = {Sundnes, Joakim and Lines, Glenn Terje and Cai, Xing and Nielsen, Bj{\o}rn Fredrik and Mardal, Kent-Andre and Tveito, Aslak},
  title = {Computing the electrical activity in the heart},
  year = {2006},
  abstract = {This book describes mathematical models and numerical techniques for simulating the electrical activity in the heart. The book gives an introduction to the most important models of the field, followed by a detailed description of numerical techniques for the models. Particular focus is on efficient numerical methods for large scale simulations on both scalar and parallel computers. The results presented in the book will be of particular interest to researchers in bioengineering and computational biology, who face the challenge of solving these complex mathematical models efficiently. The book will also serve as a valuable introduction to a new and exciting field for computational scientists and applied mathematicians.},
  publisher = {Springer-Verlag},
  isbn = {3-540-33432-7 }
}

@Misc{Nielsen.2006.3,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Cai, Xing and Mardal, Kent-Andre and Tarrou, Christian and MacLachlan, Mary and Sundnes, Joakim and Lines, Glenn Terje and Tveito, Aslak},
  title = {Computational issues in  heart modeling},
  year = {2006},
  abstract = {},
  howpublished = {Presented at the Johann Radon Institute for Computational and Applied Mathematics, Linz, Austria}
}

@Article{Lines.2006.1,
  author = {Lines, Glenn Terje and Sande, J{\o}rn Bodvar and Louch, William E and M{\o}rk, Halvor K and Gr{\o}ttum, Per and Sejersted, Ole M},
  title = {Contribution of the Na/Ca Exchanger to Rapid Ca Release in Cardiomyocytes},
  year = {2006},
  abstract = {Trigger Ca is considered to be the Ca current through the L-type Ca channel (LTCC) that causes release of Ca from the sarcoplasmic reticulum (SR). However, cell contraction also occurs in the absence of the LTCC current (ICa). In this paper we investigate the contribution of the Na/Ca -exchanger (NCX) to the trigger Ca. Experimental data from rat cardiomyocytes using confocal microscopy indicating that inhibition of reverse mode Na/Ca -exchange delays the Ca transient by 3-4 ms, served as a basis for the mathematical model. A detailed computational model of the dyadic cleft (fuzzy space) is presented where the diffusion of both Na and Ca is taken into account. Ionic channels are included at discrete locations, making it possible to study the effect of channel position and colocalization. The simulations indicate that if a Na channel is present in the fuzzy space, the NCX is able to bring enough Ca into the cell to affect the timing of release. However, this critically depends on channel placement and local diffusion properties. With fuzzy space diffusion in the order of 1\% compared to bulk diffusion, triggering through LTCC alone was 3-4 ms slower than with the presence of a Na channel and NCX.},
  journal = {Biophysical Journal},
  volume = {91},
  number = {3},
  pages = {779--792}
}

@Misc{Nielsen.2006.5,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius},
  title = {Identifying ischemic heart disease in terms of ECG recordings and an inverse problem for the bidomain equations; modeling and experiments},
  year = {2006},
  abstract = {},
  howpublished = {Presented at the Third International Conference "Inverse Problems: Modeling and Simulation", Fethiye, Turkey}
}

@Inproceedings{Nielsen.2006.6,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Cai, Xing and Mardal, Kent-Andre and Tarrou, Christian and Ruud, Tomas Syrstad and MacLachlan, Mary and Tveito, Aslak},
  title = {Identifying ischemic heart disease in terms of ECG recordings and an inverse problem for the bidomain equations; modeling and experiments},
  year = {2006},
  abstract = {},
  booktitle = {The Third International Conference "Inverse Problems: Modeling and Simulation"},
  editor = {H. T. Banks, A. Hasanov, S. I. Kabanikhin, K. Kunisch},
  pages = {138--140},
  publisher = {Literat{\"u}r Yayincilik Ltd.},
  isbn = {975-04-0381-9}
}

@Article{Maclachlan.2006.1,
  author = {MacLachlan, Mary C and Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tveito, Aslak},
  title = {Computing the size and location of myocardial ischemia using measurements of ST-segment shift},
  year = {2006},
  abstract = {},
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {53},
  number = {6},
  pages = {1024--1031}
}

@Article{Cai.2006.7,
  author = {Cai, Xing and Bouhmala, Noureddine},
  title = {A unified framework of multi-objective cost functions for partitioning unstructured finite element meshes},
  year = {2007},
  abstract = {},
  journal = {Applied Mathematical Modelling},
  volume = {31},
  number = {9},
  pages = {1711--1728}
}

@Misc{Cai.2006.8,
  author = {Cai, Xing},
  title = {Simulating Tsunamis on Parallel Computers},
  year = {2006},
  abstract = {},
  howpublished = {Invited talk at Notur 2006 Conference, May 11-12, Bergen, Norway}
}

@Misc{Lysaker.2006.4,
  author = {Lysaker, Ola Marius and Tarrou, Christian and Abildgaard, Andreas and Nielsen, Bj{\o}rn Fredrik},
  title = {On the Use of Computers to Simulate Cardiac Functions},
  year = {2006},
  abstract = {},
  howpublished = {Presented at the 24th International EuroPACS Conference, Trondheim}
}

@Misc{Alnes.2006.1,
  author = {Aln{\ae}s, Martin Sandve and Bruaset, Are Magnus and Cai, Xing and Langtangen, Hans Petter and Mardal, Kent-Andre and Moe, Halvard and Skavhaug, Ola and {\O}deg{\r a}rd, {\r A}smund},
  title = {Python in High Performance Computing},
  year = {2006},
  abstract = {},
  howpublished = {Tutorial presented at the Para06 Workshop}
}

@Misc{miscreference.2006-06-26.0915248923,
  author = {No names specified},
  title = {},
  year = {},
  abstract = {},
  howpublished = {Misc}
}

@Misc{Mardal.2006.3,
  author = {Mardal, Kent-Andre},
  title = {SyFi - An Element Matrix Factory, with Emphasis on the Incompressible Navier-Stokes Equations},
  year = {2006},
  abstract = {},
  howpublished = {Presentation held at the Para06 workshop},
  note = {Presentation held at the Para06 workshop}
}

@Article{Nielsen.2006.7,
  author = {Nielsen, Bj{\o}rn Fredrik and Ruud, Tomas Syrstad and Lines, Glenn Terje and Tveito, Aslak},
  title = {Optimal Monodomain approximations of the Bidomain equations},
  year = {2007},
  abstract = {},
  journal = {Applied Mathematics and Computation},
  volume = {184},
  number = {2},
  pages = {276--290}
}

@Misc{Nielsen.2006.8,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Larson, Mats},
  title = {Can inverse problems tell you the condition of your heart?},
  year = {2007},
  abstract = {},
  howpublished = {Zurich Intelligencer; International Congress on Industrial and Applied Mathematics (ICIAM), Springer-Verlag}
}

@Article{Mardal.2006.4,
  author = {Mardal, Kent-Andre and Nielsen, Bj{\o}rn Fredrik and Cai, Xing and Tveito, Aslak},
  title = {An order optimal solver for the discretized Bidomain equations},
  year = {2007},
  abstract = {},
  journal = {Numerical Linear Algebra with Applications},
  volume = {14},
  number = {2},
  pages = {83--98}
}

@Book{Hjelle.2006.1,
  author = {Hjelle, {\O}yvind and D{\ae}hlen, Morten},
  title = {Triangulations and Applications},
  year = {2006},
  abstract = {},
  publisher = {Springer-Verlag},
  address = {Berlin},
  isbn = { ISBN: 3-540-33260-X}
}

@Inproceedings{Nielsen.2006.9,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tarrou, Christian and Sundnes, Joakim and Cai, Xing and Mardal, Kent-Andre},
  title = {On the use of the bidomain equations for computing the transmembrane potential throughout the heart wall: An inverse problem},
  year = {2006},
  abstract = {},
  booktitle = {Computers in Cardiology 2006},
  editor = {A. Murray},
  pages = {797-800},
  publisher = {Computers in Cardiology},
  isbn = {0276-6547},
  note = {ISSN 0276-6547}
}

@Phdthesis{Hanslien.2006.2,
  author = {Hanslien, Monica},
  title = {Analysis of numerical methods for mathematical models of cardiac electrical activity},
  year = {2006},
  abstract = {},
  school = {Dep. of Informatics, University of Oslo}
}

@Article{Maclachlan.2006.2,
  author = {MacLachlan, Mary C and Sundnes, Joakim and Skavhaug, Ola and Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik and Tveito, Aslak},
  title = {A linear system of partial differential equations modeling the resting potential of a heart with regional ischemia},
  year = {2007},
  abstract = {},
  journal = {Mathematical Biosciences},
  volume = {210},
  number = {1},
  pages = {238--252}
}

@Misc{Nielsen.2006.10,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tarrou, Christian and Sundnes, Joakim and Cai, Xing and Mardal, Kent-Andre},
  title = {On the use of the bidomain equations for computing the transmembrane potential throughout the heart wall: An inverse problem},
  year = {2006},
  abstract = {},
  howpublished = {Presented at the Computers in Cardiology conference in Valencia, Spain}
}

@Inproceedings{Cai.2006.1,
  author = {Cai, Xing and Langtangen, Hans Petter},
  title = {Making hybrid tsunami simulators in a parallel software framework},
  year = {2007},
  abstract = {},
  booktitle = {Proceedings of the PARA'06 Workshop},
  editor = {B. K{\r a}gstr{\"o}m, E. Elmroth, J. Dongarra, J. Wasniewski},
  pages = {686-693},
  publisher = {Springer Verlag},
  address = {Berlin Heidelberg},
  series = {Lecture Notes in Computer Science, volume 4699},
  isbn = {978-3-540-75754-2}
}

@Inproceedings{Langtangen.2006.1,
  author = {Langtangen, Hans Petter and Cai, Xing},
  title = {On the efficiency of Python for high-performance computing: a case study
 involving stencil updates for partial differential equations},
  year = {2008},
  abstract = {},
  booktitle = {Proceedings of the HPSC2006 Conference},
  editor = {H. G. Bock et al},
  publisher = {Springer Verlag},
  series = {LNCSE}
}

@Inbook{Yeh.2006.1,
  author = {Yeh, Jim T.-C and Cai, Xing and Langtangen, Hans Petter and Zhu, Junfeng and Ni, Chuen-Fa},
  title = {Parallel computing engines for subsurface imaging technologies},
  year = {2008},
  abstract = {},
  booktitle = {Advanced Computational Infrastructures for Parallel/Distributed Adaptive Applications},
  editor = {M. Parashar, X. Li, S. Chandra},
  publisher = {Wiley Press}
}

@Article{Maday.2006.1,
  author = {Maday, Yvon and R{\o}nquist, Einar M and Staff, Gunnar Andreas},
  title = {The Parareal-in-Time Algorithm: basics, stability analysis and more},
  year = {2006},
  abstract = {The parareal-in-time algorithm allows to take benefit of a parallel architecture of
large scale computing resources in order to decrease the restitution time for the
numerical simulation of time dependent problems. The method can be presented
as a predictor corrector scheme where the predictor is based on a coarse grain
and inexpensive simulation, solved in a serial manner and the expensive corrector
can be spread on different processors. Like for domain decomposition techniques,
the algorithm is based on the decomposition of the global simulation time interval
into slabs, each slab being dedicated to a processor. After reminding the basics
of the approximation, we discuss the stability of the algorithm for a system of
autonomous differential equations. The stability function for the algorithm is derived
and analyzed, based on various choices of schemes in time for the coarse and the
fine propagator. We then present some complementary analysis that provides the
frame for the definition of the cheap coarse simulation. Finally, numerical results
for the viscous Burger{\textquoteright}s equation are presented.},
  journal = {journal}
}

@Article{Mardal.2006.5,
  author = {Mardal, Kent-Andre and Nilssen, Trygve Kastberg and Staff, Gunnar Andreas},
  title = {Order optimal preconditioners for implicit Runge-Kutta schemes applied to parabolic PDE's},
  year = {2007},
  abstract = {In this paper we show that standard preconditioners for parabolic PDEs discretized by implicit Euler or Crank--Nicolson schemes can be reused for higher--order
fully implicit Runge--Kutta time discretization schemes.
We prove that the suggested block diagonal preconditioners are order--optimal for A--stable and irreducible Runge--Kutta schemes with invertible coefficient matrices.
The theoretical investigations are confirmed by numerical experiments.
},
  journal = {SIAM Journal of Scientific Computing},
  volume = {29},
  number = {1},
  pages = {361-375},
  note = {Accepted for publication at SIAM Journal of Scientific Computing}
}

@Inproceedings{Staff.2006.2,
  author = {Staff, Gunnar Andreas and Mardal, Kent-Andre and Nilssen, Trygve Kastberg},
  title = {Block preconditioners for fully implicit Runge-Kutta schemes applied to the Bidomain equations},
  year = {2006},
  abstract = {Recently, the authors presented different block preconditioners for implicit Runge-Kutta discretization of the heat equation. The preconditioners were block Jacobi and block Gauss-Seidel preconditoners where the blocks reused existing preconditioners for the implicit Euler discretization of the same equation. In this paper we will introduce  similar block preconditioners for the implicit Runge-Kutta discretization of the Bidomain equation. We will, by numerical experiments, show the properties of the preconditoners, and that higher-order Runge-Kutta discretization of the Bidomain equation may be superior to lower-order in
some cases.},
  booktitle = {ECCOMAS CFD 06},
  editor = {P. Wesseling, E. O{\\~n}ate, J. P{\a\'e}riaux},
  pages = {1-15},
  publisher = {European Community on Computational Methods in Applied Sciences},
  isbn = {90-9020970-0},
  note = {The proceeding is published electronically}
}

@Article{Hanslien.2006.3,
  author = {Hanslien, Monica and Sundnes, Joakim and Tveito, Aslak},
  title = {An unconditionally stable numerical method for the Luo-Rudy 1 model used in simulations of defibrillation},
  year = {2007},
  abstract = {},
  journal = {Mathematical Biosciences},
  volume = {208},
  number = {2},
  pages = {375--392}
}

@Inproceedings{Mardal.2006.6,
  author = {Mardal, Kent-Andre},
  title = {SyFi - An Element Matrix Factory, with Emphasis on the Incompressible Navier-Stokes Equations },
  year = {2006},
  abstract = {},
  booktitle = {Para'06 }
}

@Article{Nielsen.2006.11,
  author = {Nielsen, Bj{\o}rn Fredrik and Cai, Xing and Lysaker, Ola Marius},
  title = {On the possibility for computing the transmembrane potential in the heart with a one shot method; an inverse problem},
  year = {2007},
  abstract = {},
  journal = {Mathematical Biosciences},
  volume = {210},
  number = {2},
  pages = {523--553}
}

@Phdthesis{Maclachlan.2006.3,
  author = {MacLachlan, Mary C},
  title = {Computer simulations of myocardial ischemia},
  year = {2006},
  abstract = {},
  school = {Department of Informatics, University of Oslo}
}

@Inproceedings{Cai.2006.4,
  author = {Cai, Xing and Pedersen, Geir Kleivstul and Langtangen, Hans Petter and Glimsdal, Sylfest},
  title = {Parallel Simulation of Tsunamis Using a Hybrid Software Approach},
  year = {2006},
  abstract = {},
  booktitle = {Proceedings of the International Conference ParCo 2005, September 13-16, Malaga, Spain},
  editor = {G. R. Joubert et al},
  pages = {383--390},
  publisher = {John von Neumann Institute for Computing},
  series = {Volume 33 in NIC series},
  isbn = {3-00-017352-8}
}

@Inproceedings{Cai.2006.9,
  author = {Cai, Xing},
  title = {Improving the performance of large-scale unstructured PDE applications},
  year = {2006},
  abstract = {},
  booktitle = {Proceedings of the PARA'04 Workshop, June 20-23, 2004, Lyngby, Denmark},
  editor = {J. Dongarra, K. Madsen and J. Wasniewski},
  pages = {699--708},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science, volume 3732},
  isbn = {3-540-29067-2}
}

@Article{Cai.2006.10,
  author = {Cai, Xing and Nielsen, Bj{\o}rn Fredrik and Tveito, Aslak},
  title = {A note on the efficiency of the Conjugate Gradient method for a class of time-dependent problems},
  year = {2007},
  abstract = {},
  journal = {Numerical Linear Algebra with Applications},
  volume = {14},
  number = {5},
  pages = {459--467}
}

@Article{Mardal.2006.7,
  author = {Mardal, Kent-Andre and Skavhaug, Ola and Lines, Glenn Terje and Staff, Gunnar Andreas and {\O}degard, {\r A}smund},
  title = {Using Python to Solve Partial Differential Equations},
  year = {2007},
  abstract = {},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {48-51}
}

@Inproceedings{Lovgren.2006.4,
  author = {L{\o}vgren, Alf Emil and Maday, Yvon and R{\o}nquist, Einar M},
  title = {A reduced basis element method for complex flow systems},
  year = {2006},
  abstract = {The reduced basis element method is a new approach for approximating the solution of problems described by partial differential equations within domains belonging to a certain class. The method takes its roots in domain decomposition methods and reduced basis discretizations. The basic idea is to first decompose the computational domain into smaller blocks that are topologically similar to a few reference shapes (or generic computational parts). Associated with each reference shape are precomputed solutions corresponding to the same governing partial differential equation, and similar boundary conditions, but solved for different choices of some underlying parameter. In this work, the parameters are representing the geometric shape associated with a computational part. The approximation corresponding to the computational domain is then taken to be a linear combination of the precomputed solutions, mapped from the reference shapes for the different blocks to the actual domain. The variation of the geometry induces non-affine parameter dependence, and we apply the empirical interpolation technique to achieve an offline/online decoupling of the reduced basis procedure. Some results for incompressible flow systems have already been presented, and the focus here will be to further improve the offline/online decoupling of problems with non-affine parameter dependence. To this end we use the empirical interpolation method to approximate the parameter depen- dent operators. We also present a generalized transfinite interpolation method intended to produce global C1 mappings from the reference shapes to each corresponding block of the computational domain.},
  booktitle = {ECCOMAS CFD 2006, European Conference on Computational Fluid Dynamics},
  editor = {P. Wesseling, E. O{\\~n}ate, J. P{\a\'e}riaux},
  publisher = {TU Delft},
  isbn = {90-9020970-0}
}

@Article{Thorvaldsen.2006.1,
  author = {Thorvaldsen, Tom and Osnes, Harald and Sundnes, Joakim and McCulloch, Andrew},
  title = {An operator splitting technique for integrating cardiac electro-mechanics},
  year = {2006},
  abstract = {},
  journal = {journal for publication},
  note = {Submitted for publication, 2006.}
}

@Article{Logg.2006.5,
  author = {Logg, Anders and Kirby, Robert C},
  title = {Efficient Compilation of a Class of Variational Forms},
  year = {2007},
  abstract = {},
  journal = {ACM Transactions on Mathematical Software},
  volume = {33},
  number = {3}
}

@Inbook{Logg.2007.1,
  author = {Logg, Anders},
  title = {Att l{\"o}sa en differentialekvation},
  year = {2008},
  abstract = {},
  booktitle = {Matematikens Rikedomar},
  editor = {O. Helenius and K. Wallby},
  publisher = {Nationellt centrum f{\"o}r matematikutbildning, NCM},
  isbn = {978--91--85143--08--5}
}

@Misc{Logg.2006.6,
  author = {Logg, Anders},
  title = {Automating the Finite Element Method},
  year = {2006},
  abstract = {},
  howpublished = {Lecture notes for the Sixth Winter School in Computational Mathematics}
}

@Manual{Logg.2006.7,
  author = {Logg, Anders and Hoffman, Johan and Jansson, Johan and Wells, Garth N},
  title = {DOLFIN User Manual},
  year = {2007},
  abstract = {},
  organization = {The FEniCS Project}
}

@Manual{Logg.2006.8,
  author = {Logg, Anders},
  title = {FFC User Manual},
  year = {2007},
  abstract = {},
  organization = {The FEniCS Project}
}

@Techreport{Logg.2006.9,
  author = {Logg, Anders},
  title = {Automating the Finite Element Method},
  year = {2006},
  abstract = {},
  institution = {Finite Element Center},
  type = {Preprint},
  number = {2006-01}
}

@Techreport{Logg.2006.10,
  author = {Logg, Anders and Jansson, Johan},
  title = {Algorithms and Data Structures for Multi-Adaptive Time-Stepping},
  year = {2006},
  abstract = {},
  institution = {Finite Element Center},
  type = {Preprint},
  number = {2006-06}
}

@Techreport{Logg.2006.11,
  author = {Logg, Anders and Kirby, Robert C},
  title = {Efficient Compilation of a Class of Variational Forms},
  year = {2006},
  abstract = {},
  institution = {Finite Element Center},
  type = {Preprint},
  number = {2006-07}
}

@Phdthesis{Thorvaldsen.2006.2,
  author = {Thorvaldsen, Tom},
  title = {An Electro-Mechanics Solver for the Heart},
  year = {2006},
  abstract = {},
  school = {Department of Informatics, University of Oslo}
}

@Article{Rahman.2006.1,
  author = {Rahman, S and Langtangen, Hans Petter and Barnes, C. H. W},
  title = {A finite element method for modelling electromechanical wave propagation in anisotropic piezoelectric media},
  year = {2006},
  abstract = {We describe and evaluate a numerical solution strategy for simulating surface acoustic waves (SAWs) through semiconductor devices with complex geometries. This multi-physics problem is of particular relevance to the design of SAW-based quantum electronic devices. The mathematical model consists of two coupled partial differential equations for the elastic wave propagation and the electric field, respectively, in anisotropic piezoelectric media. These equations are discretized by the finite element method in space and by a finite difference method in time. The latter method yields a convenient numerical decoupling of the governing equations. We describe how a computer implementation can utilize the decoupling and, via object-oriented programming techniques reuse independent codes for the Poisson equation and the linear time-dependent elasticity equation. First we apply the simulator to a simplified model problem for verifying the implementation, and thereafter we show that the methodology is capable of simulating a real-world case from nanotechnology, involving SAWs in a geometrically non-trivial device made of Gallium Arsenide.},
  journal = {Communications in Computational Physics},
  volume = {2},
  number = {2},
  pages = {271-292}
}

@Article{Rahman.2006.2,
  author = {Rahman, S and Goreman, J and Barnes, C. H. W and Williams, D.A and Langtangen, Hans Petter},
  title = {Finite element analysis of a silicon-based double quantum dot structure.},
  year = {2006},
  abstract = {},
  journal = {Physical Review B},
  volume = {75},
  number = {233307}
}

@Article{Rahman.2006.3,
  author = {Rahman, S and Goreman, J and Barnes, C. H. W and Williams, D.A and Langtangen, Hans Petter},
  title = {Numerical investigation of a piezoelectric surface acoustic wave interaction with a one-dimensional channel},
  year = {2006},
  abstract = {},
  journal = {Physical Review B},
  volume = {75},
  number = {233307}
}

@Article{Holmas.2006.1,
  author = {Holm{\r a}s, K and Langtangen, Hans Petter},
  title = {A sharp interface finite element method for elliptic interface problems; formulation and investigation in one space dimension},
  year = {2007},
  abstract = {},
  journal = {International Journal of Pure and Applied Mathematics},
  volume = {34},
  number = {3},
  pages = {287-312}
}

@Misc{Langtangen.2006.2,
  author = {Langtangen, Hans Petter},
  title = {Tsunami simulations and their need for computational power},
  year = {2006},
  abstract = {},
  howpublished = {Invited lecture at Dagstuhl conference Architectures and Algorithms for Petascale Computing},
  note = {Dagstuhl, Germany.}
}

@Misc{Langtangen.2006.3,
  author = {Langtangen, Hans Petter},
  title = {Simulation of tsunamis generated by earth-asteroid collisions},
  year = {2006},
  abstract = {},
  howpublished = {Invited keynote lecture},
  note = {Conference High-Performance Scientific Computing - Modeling,Simulation and Optimization of Complex Processes, Hanoi, Vietnam.}
}

@Misc{Langtangen.2006.4,
  author = {Langtangen, Hans Petter and Mardal, Kent-Andre},
  title = {Mathematical modeling of blood flow},
  year = {2006},
  abstract = {},
  howpublished = {Lecture at Svalbard summer school},
  note = {European Mathematical Society Summer School Mathematical Modeling of the Heart.
Longyearbyen, Svalbard, Norway,}
}

@Misc{Langtangen.2006.5,
  author = {Langtangen, Hans Petter and Sundnes, Joakim},
  title = {Introduction to heart muscle mechanics-basic concepts of muscle contraction and soft tissue mechanics.},
  year = {2006},
  abstract = {},
  howpublished = {Lecture at Svalbard summer school},
  note = {European Mathematical Society Summer School Mathematical Modeling of the Heart,
Longyearbyen, Svalbard, Norway. 
Presented by J. Sundnes.}
}

@Misc{Glimsdal.2006.2,
  author = {Glimsdal, Sylfest and Pedersen, Geir Kleivstul and Langtangen, Hans Petter and Shuvalov, V and Dypvik, H},
  title = {Tsunami generation and propagation from the Mj{\o}lnir asteroid impact},
  year = {2006},
  abstract = {},
  howpublished = {Poster at Impact craters as indicators for planetary environmental evolution and astrobiology},
  note = {Ostersund, Sweden.
Presented by H. Dypvik.}
}

@Misc{Langtangen.2006.6,
  author = {Langtangen, Hans Petter},
  title = {Mixed-language programming for HPC applications},
  year = {2006},
  abstract = {},
  howpublished = {Invited keynote lecture at PARA{\textquoteright}06},
  note = {State-of-the-art in Scientific and Parallel Computing, Ume{\r a}, Sweden.}
}

@Misc{Cai.2006.11,
  author = {Cai, Xing and Langtangen, Hans Petter},
  title = {Making hybrid tsunami simulators in a parallel software framework},
  year = {2006},
  abstract = {},
  howpublished = {Minisyposium Talk at PARA06},
  note = {Conference PARA06: State-of-the-art in Scientific and Parallel Computing,Ume{\r a}, sweden.
Talk at the minisymposium Software Tools for Parallel CFD Applications organized by X. Cai and H. P. Langtangen.
Presented by X. Cai}
}

@Misc{Langtangen.2006.7,
  author = {Langtangen, Hans Petter},
  title = {Building programmable problem solving environments for porous media flow},
  year = {2006},
  abstract = {},
  howpublished = {Invited keynote lecture},
  note = {Invited keynote lecture at the international conference on Computational Methods in Water Resources, CMWR {\textendash} XVI, Copenhagen, Denmark.}
}

@Misc{Langtangen.2006.8,
  author = {Langtangen, Hans Petter},
  title = {An overview of mathematical modeling and numerical simulation},
  year = {2006},
  abstract = {},
  howpublished = {Invited talk at the Oslo School of Architecture and Design}
}

@Misc{Langtangen.2006.9,
  author = {Langtangen, Hans Petter},
  title = {Why simulate?},
  year = {2006},
  abstract = {},
  howpublished = {Invited plenary talk at the Norwegian Defence Research Establishment (FFI)}
}

@Misc{Langtangen.2006.10,
  author = {Langtangen, Hans Petter},
  title = {Using Python to build programmable problem solving environments},
  year = {2006},
  abstract = {},
  howpublished = {Invited talk at the Norwegian Defence Research Establishment (FFI)}
}

@Misc{Langtangen.2006.11,
  author = {Langtangen, Hans Petter},
  title = {Modeling tsunamis originating from earth-asteroid collisions},
  year = {2006},
  abstract = {},
  howpublished = {Invited talk at the Center of Excellence for Nonlinear Science},
  note = {Tallin, Estonia.}
}

@Misc{Langtangen.2005.3,
  author = {Langtangen, Hans Petter},
  title = {Tools for multi-physics simulation},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at the FEniCS{\textquoteright}05 workshop},
  note = {Toyota Technological Institute, Chicago, USA.}
}

@Misc{Langtangen.2005.4,
  author = {Langtangen, Hans Petter},
  title = {Tsunamis generated by earth-asteroid impacts},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at the BeMatA (Computational Mathematics for Applications) meeting},
  note = {Hurdalsj{\o}en, Norway}
}

@Misc{Langtangen.2005.5,
  author = {Langtangen, Hans Petter},
  title = {Solution of partial differential equations on parallel computers},
  year = {2005},
  abstract = {},
  howpublished = {Lecture at the workshop High-Performance Computing in Physics},
  note = {Lecture at the Center of Excellence Mathematics for Applications (CMA).
Oslo, Norway,}
}

@Phdthesis{Staff.2006.3,
  author = {Staff, Gunnar Andreas},
  title = {Numerical Computation of Initial Value Problems originating from Partial Differential Equations},
  year = {2006},
  abstract = {},
  school = {University of Oslo}
}

@Phdthesis{Vazquez.2004.2,
  author = {Vazquez, Ariel Almendral},
  title = {Financial Derivatives Under Generalized Black-Scholes models; the PDE approach},
  year = {2004},
  abstract = {},
  school = {University of Oslo}
}

@Misc{Zhu.2006.1,
  author = {Zhu, Junfeng and Yeh, Jim and Cai, Xing},
  title = { Fusion of Hydraulic and Tracer Tomography for DNAPL Detection},
  year = {2006},
  abstract = {},
  howpublished = {Poster presented at AGU Fall Meeting 2006, Dec. 11-15, San Francisco}
}

@Misc{Cai.2006.12,
  author = {Cai, Xing and Zhu, Junfeng and Ni, Chuen-Fa and Yeh, Jim},
  title = { Parallel Computational Methodology for Hydraulic Tomography},
  year = {2006},
  abstract = {},
  howpublished = {Poster presented at AGU Fall Meeting 2006, San Francisco, Dec. 11-15}
}

@Misc{Cai.2006.13,
  author = {Cai, Xing and Zhu, Junfeng and Zhang, Yeliang and Yeh, Jim},
  title = {Hybrid Parallelization of a 3D Transient Hydraulic Tomography Code},
  year = {2006},
  abstract = {},
  howpublished = {Poster presented at Western Pacific Geophysics Meeting 2006, Beijng, July 24-27}
}

@Misc{Cai.2006.14,
  author = {Cai, Xing and Zhu, Junfeng and Yeh, Jim},
  title = {Parallel Programming and Computing for Large-Scale Hydraulic Tomography},
  year = {2006},
  abstract = {},
  howpublished = {Poster presented at Workshop on Hydraulic Tomography, Boise, June 8-9}
}

@Article{Nielsen.2007.1,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Ola Marius and Tveito, Aslak},
  title = {On the use of the resting potential and level set methods for identifying ischemic heart disease; an inverse problem},
  year = {2007},
  abstract = {},
  journal = {Journal of Computational Physics},
  volume = {220},
  number = {2},
  pages = {772--790}
}

@Misc{Odegard.2006.2,
  author = {{\O}deg{\r a}rd, {\r A}smund},
  title = {Configure and Build},
  year = {2006},
  abstract = {A survey of some tools for configuring and building software. },
  howpublished = {Talk at FEniCS'06, Nov. 8-9, Delft, Netherlands.},
  note = {Talk at the FEniCS'06 workshop, Delft, Netherlands. }
}

@Manual{Simula.SC.1,
  author = {Aln{\ae}s, Martin Sandve and Mardal, Kent-Andre},
  title = {SyFi User Manual},
  year = {2006},
  abstract = {}
}

@Book{SE.2.Kirby.2001,
  editor = {Kirby, G NC and Dearle, A and Sj{\o}berg, Dag I. K},
  title = {9th International Workshop, POS-9},
  year = {2001},
  abstract = {},
  publisher = {Springer-Verlag},
  address = {Lillehammer, Norway},
  number = {2135},
  isbn = {3-540-42735-X}
}

@Phdthesis{SE.3.Arisholm.2001,
  author = {Arisholm, Erik},
  title = {Empirical Assessment of Changeability in Object-Oriented Software},
  year = {2001},
  abstract = {},
  school = {University of Oslo},
  note = {ISSN 1501-7710, No. 143}
}

@Phdthesis{SE.3.Bratthall.2001,
  author = {Bratthall, L},
  title = {Empirical Studies of the Impact of Architectural Understanding of Software Evolution},
  year = {2001},
  abstract = {},
  school = {University of Oslo},
  note = {ISSN 1501-7710, No. 141}
}

@Article{SE.4.Arisholm.2001,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and J{\o}rgensen, M},
  title = {Assessing the Changeability of two Object-Oriented Design Alternatives - a Controlled Experiment},
  year = {2001},
  abstract = {},
  journal = {Empirical Software Engineering},
  volume = {6},
  number = {3},
  pages = {231-277}
}

@Article{SE.4.Joergensen.2001.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Impact of Effort Estimates on Software Project Work},
  year = {2001},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {43},
  number = {15},
  pages = {939-948}
}

@Article{SE.4.Joergensen.2001.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Software Process Improvement and Human Judgement Heuristics},
  year = {2001},
  abstract = {},
  journal = {Scandinavian Journal of Information Systems},
  volume = {13},
  number = {1},
  pages = {99-122}
}

@Inproceedings{SE.5.Anda.2001.a,
  author = {Anda, Bente Cecilie Dahlum and Dreiem, Hege and Sj{\o}berg, Dag Ingar Kondrup and J{\o}rgensen, Magne},
  title = {Estimating Software Development Effort Based on Use Cases - Experiences from Industry},
  year = {2001},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Several methods for estimating software development effort are based on attributes of a use case model. This paper reports the results of three industrial case studies on the application of a method for effort estimation based on use case points. The aim of this paper is to provide guidance for other organizations that want to improve their estimation process applying use cases. Our results support existing claims that use cases can be used successfully in estimating software development effort. The results indicate that the guidance provided by the use case points method can support expert knowledge in the estimation process. Our experience is also that the design of the use case models has a strong impact on the estimates. 
},
  booktitle = {4th International Conference on the Unified                   Modeling Language (UML2001)},
  editor = {Gogolla, M. and Kobryn, C.},
  pages = {487-502},
  publisher = {Springer-Verlag},
  address = {Toronto, Canada},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-42667-1}
}

@Inproceedings{SE.5.Anda.2001.b,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag Ingar Kondrup and J{\o}rgensen, Magne},
  title = {Quality and Understandability of Use Case Models},
  year = {2001},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Use case models are also used in communication between stakeholders in development projects. It is therefore important that the use case models are constructed in such a way that they support the development process and promote a good understanding of the requirements among the stakeholders. Despite this, there are few guidelines on how to construct use case models. This paper describes an explorative study where three different sets of guidelines were used for constructing and documenting use case models. An experiment with 139 undergraduate students divided into 31 groups was conducted. Each group used one out of the three sets of guidelines when constructing a use case model from an informal requirements specification. After completing the use case model, each student answered a questionnaire. The results of the experiment indicate that guidelines based on templates support the construction of use case models that are easier to understand for the readers, than guidelines without specific details on how to document each use case. The guidelines based on templates were also considered as the most useful when constructing use cases. In addition to better understandability, our experiment indicates that the guidelines based on templates result in better use case models regarding also other quality attributes. Our results further indicate that it may be beneficial to combine the template guidelines with another set of guidelines that focus on the documentation of the flow of events of each use case. Keywords. Object-oriented analysis, Requirements specification, Use Cases, UML, Understandability, Experiment},
  booktitle = {ECOOP 2001 - Object-Oriented Programming, 15th                   European Conference},
  editor = {J. Lindskov Knudsen},
  pages = {402-428},
  publisher = {Springer-Verlag},
  address = {Budapest, Hungary},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-42206-4}
}

@Inproceedings{SE.5.Bratthall.2001.a,
  author = {Bratthall, L and Arisholm, Erik and J{\o}rgensen, M},
  title = {Program Understanding Behaviour During Estimation of Enhancement on Small Java Programs},
  year = {2001},
  abstract = {},
  booktitle = {3rd International Conference on Product                   Focused Software Process Improvement (PROFES                   2001)},
  publisher = {-},
  address = {Kaiserslautern, Germany}
}

@Inproceedings{SE.5.Bratthall.2001.b,
  author = {Bratthall, L and J{\o}rgensen, M},
  title = {Can you Trust a Single Data-source Exploratory Software Engineering Case Study},
  year = {2001},
  abstract = {},
  booktitle = {EASE 2001},
  publisher = {-},
  address = {Keele, England}
}

@Inproceedings{SE.5.Gallis.2001,
  author = {Gallis, H and Kasbo, J P and Herstad, J},
  title = {The Multidevice Paradigm in Knowmobile - Does one size fit all?},
  year = {2001},
  abstract = {This paper describes preliminary results and topics from the Knowmobile 
project. The Knowmobile project is concerned with mobile opportunities for medical students at the University of Oslo. The focus is on the students learning context at a local hospital and at a number of general practitioners offices. The paper reports theoretical data on the multidevice paradigm. The multidevice paradigm refers to interaction with many different devices in contrast to interaction with just one device. The multidevice paradigm leads to problems connected to the use, design, harmonization, standardization and synchronization of various devices, and this paper states that introducing artefacts in a multidevice environment, must concider all these problem areas. The paper concludes that there is no one size fit all and, that the use situation for the medical students points towards the multidevice paradigm.},
  booktitle = {Proceedings of the 24th Information System                   Research in Scandinavia (IRIS24)},
  editor = {Bj{\o}rnestad, S. and Moe, R.E. and M{\o}rch,                   A.I. and Opdahl, A.L.},
  pages = {491-504},
  publisher = {-}
}

@Inproceedings{SE.5.Joergensen.2001.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, D I. K},
  title = {Anchoring Effects: An Important Cause of Too Optimistic Effort Estimates in Software Development Projects},
  year = {2001},
  abstract = {},
  booktitle = {24nd IRIS Conference (Information Systems                   Research Seminar In Scandinavia)},
  editor = {-},
  publisher = {-},
  address = {Ulvik, Norway},
  isbn = {0}
}

@Inproceedings{SE.5.Joergensen.2001.b,
  author = {J{\o}rgensen, M and Indahl, U and Sj{\o}berg, D I. K},
  title = {Software Effort Estimation by Analogy and Regression Toward the Mean},
  year = {2001},
  abstract = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon {\textquotedblleft}regression toward the mean{\textquotedblright} (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  booktitle = {The Thirteenth International Conference on                   Software Engineering \& Knowledge Engineering                   (SEKE01)},
  editor = {-},
  pages = {253 -- 262 },
  publisher = {Elsevier Science Inc},
  address = {Buenos Aires, Argentina},
  isbn = {0}
}

@Inproceedings{SE.5.Johansson.2001,
  author = {Johansson, E and Bratthall, L and H{\o}st, M and Wesslen, A},
  title = {The Importance of Quality Requirements in Software Platform Development - a Survey},
  year = {2001},
  abstract = {},
  booktitle = {HICS-34, Hawaii International Conference on                   System Science},
  publisher = {-},
  address = {Maui}
}

@Inproceedings{SE.5.Karahasanovic.2001.a,
  author = {Karahasanovic, A and Sj{\o}berg, D I. K},
  title = {Visualizing Impacts of Database Schema Changes -- A Controlled Experiment},
  year = {2001},
  abstract = {Research in schema evolution has been driven by the need for more effective software development and maintenance. Finding impacts of schema changes on the applications and presenting them in an appropriate way are particularly challenging. We have developed a tool that finds impacts of schema changes on applications in object-oriented systems. This tool displays components (packages, classes, interfaces, methods and fields) of a database application system as a graph. Components potentially affected by a change are indicated by changing the shape of the boxes representing those components. Two versions of the tool are available. One version identifies affected parts of applications at the granularity of packages, classes, and interfaces, whereas the other version identifies affected parts at the finer granularity of fields and methods. This paper presents the design and results of a controlled student experiment testing these two granularity levels with respect to productivity and user satisfaction. There are indications that identifying impacts at the finer granularity can reduce the time needed to conduct schema changes and reduce the number of errors. Our results also show that the subjects of the experiment appreciated the idea of visualizing the impacts of schema changes.},
  booktitle = {2001 IEEE Symposium on Visual/Multimedia                   Approaches to Programming and Software                   Engineering},
  editor = {-},
  pages = {358-365},
  publisher = {IEEE Computer Society},
  address = {Stresa, Italy},
  isbn = {0-7695-0474-4}
}

@Inproceedings{SE.5.Karahasanovic.2001.b,
  author = {Karahasanovic, A},
  title = {Identifying Impacts of Database Schema Changes on Application},
  year = {2001},
  abstract = {},
  booktitle = {Proceedings of the 8th Doctoral Consortium at                   the CAiSE*01},
  editor = {A.Hinze and O.Herden},
  pages = {93-104},
  publisher = {Freie Universit{\"a}t Berlin},
  address = {Interlaken, Switzerland},
  note = {Technical Report No B 01-04, Institute of
                   Computer Science}
}

@Inproceedings{SE.5.Karahasanovic.2001.c,
  author = {Karahasanovic, A and Sj{\o}berg, D I. K and J{\o}rgensen, M},
  title = {Data Collection in Software Engineering Experiments},
  year = {2001},
  abstract = {This paper presents ongoing work on developing a mechanism for automatic data collection during software engineering experiments. The importance of experiments in software engineering can hardly be overemphasised. Conducting such experiments raises the challenge of efficient and reliable data collection and analysis. During the experiment subjects (students, experienced programmers, etc.) perform tasks on the software engineering artefacts (e.g., design documents and code) using the technology under study in the given context (e.g. operating system). We are typically interested in data concerning subjects, their interaction with the technology and context, and changes of the software engineering artefacts. To conduct a series of experiments on software engineering technology (methods and tools), we envisage an experimental environment for data collection and analysis. As a first step, we have developed a logging mechanism that collects data about subjects of the experiment and their usage of the technology under study. We believe that our logging mechanism will alleviate data collection and increase the validity of experiments.},
  booktitle = {Managing Information Technology in a Global                   Economy, Information Resources Management                   Association International Conference IRMA 2001,                   Software Engineering Track},
  editor = {-},
  pages = {1027-1028},
  publisher = {Idea Group Publishing},
  address = {Toronto, Ontario Canada},
  isbn = {0}
}

@Inproceedings{SE.5.Kasbo.2001,
  author = {Kasbo, J P and Gallis, H and Herstad, J},
  title = {Designing for a Multidevice Environment - A Descriptive Case},
  year = {2001},
  abstract = {In this paper we are focusing on designing for a multidevice environment. The paper presents preliminary results and describes topics from the Knowmobile project. The Knowmobile project is concerned with mobile opportunities for medical students at the University of Oslo. The multidevice paradigm in Knowmobile is enlighted, and the paper reports theoretical and empirical data on cross publishing. Cross publishing is a term that refers to one publication that is presented on various devices such as a book, a PDA, on the Internet and WAP and so on. The paper report Cross publishing on multiple devices is an approach to design for a multidevice environment.},
  booktitle = {Proceedings of the 24th Information System                   Research Seminar in Scandinavia (IRIS24)},
  editor = {Bj{\o}rnestad, S. and Moe, R.E. and M{\o}rch,                   A.I. and Opdahl, A.L.},
  pages = {533-546},
  publisher = {-}
}

@Inproceedings{SE.5.Lien.2001.a,
  author = {Lien, A C and Arisholm, Erik},
  title = {Evolutionary Development of Web-applications - Lessons learned},
  year = {2001},
  abstract = {On evolutionary web development in three different companies.},
  booktitle = {European Software Process Improvement                   Conference (EuroSPI'2001)},
  publisher = {-},
  address = {Limerick Institute of Technology, Ireland}
}

@Inproceedings{SE.5.Olssson.2001,
  author = {Olssson, T and Bauer, N and Runeson, P and Bratthall, L},
  title = {An Experiment on Lead-Time Impact in Testing of Distributed Real-Time Systems},
  year = {2001},
  abstract = {},
  booktitle = {METRICS 2001},
  publisher = {-},
  address = {London, England}
}

@Inproceedings{SE.5.Sjoeberg.2001,
  author = {Sj{\o}berg, D I. K and Arisholm, Erik and J{\o}rgensen, M},
  title = {Conducting Experiments on Software Evolution},
  year = {2001},
  abstract = {},
  booktitle = {4th International Workshop on Principles of                   Software Evolution (IWPSE 2001)},
  editor = {Tetsuo Tamai and Mikio Aoyama and Keith                   Bennet},
  pages = {142-145},
  publisher = {-},
  address = {Vienna, Austria},
  isbn = {1-58113-508-4}
}

@Phdthesis{SE.3.Karahasanovic.2002,
  author = {Karahasanovic, A},
  title = {Supporting Application Consistency in Evolving Object-Oriented Systems by Impact Analysis and Visualisation},
  year = {2002},
  abstract = {There is a growing number of object-oriented systems within areas such as CAD/CAM, software engineering, office automation and multimedia management where changes are rather a rule than an exception. Application systems in these areas are supposed to be easy to change due to encapsulation and inheritance. On the other hand, the transitivity of inheritance and aggregation structures makes it difficult to detect dependencies between classes, methods and fields of a system, which in turn makes it more difficult to change the system. 
The research presented in this thesis demonstrates that identifying and visualising impacts of changes in evolving object-oriented systems is a step towards improving the process of maintaining application consistency in such systems. A technology that identifies and visualises such impacts has been developed and evaluated. This technology helps discover dependencies and thus support maintaining application consistency. It is based on a component-based model of object-oriented systems and an improved version of a transitive closure algorithm. This technology identifies impacts of complex changes, i.e., changes involving several classes like merge and split. Furthermore, a visual language allows displaying relatively large amount of objects, both at a coarse level of granularity (packages, classes and interfaces) and at a fine level of granularity (fields and methods). Several empirical studies focusing on the usability of the proposed technology have been conducted. The results of these studies show that visualising impacts of changes improves the effectiveness of developers when conducting changes. Furthermore, the higher precision of impact analysis achieved by identifying impacts at the finer level of granularity (fields and methods) reduces the number of errors and the time needed to conduct changes. Identifying impacts of complex changes (merge, split) improves effectiveness. 
Empirical studies are a prerequisite for usability evaluation of any technology, including those supporting application consistency. Such studies, in turn, require efficient and reliable data collection. A tool for automatic data collection during software engineering experiments has been developed. This tool automatically collects data about the subjects and their interaction with the software technology under study. A {\textquoteleft}think-aloud{\textquoteright} screen was proposed as a means for collecting subjective information. High-level subjective data sources (the think-aloud screen and evaluation screen) are combined with a low-level objective data sources (user command logs). It has been demonstrated that this combination may increase the validity of the conducted studies.},
  school = {University of Oslo},
  note = {ISSN 1501-7710, Nr. 234, Unipub AS}
}

@Article{SE.4.Arisholm.2002,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and Carelius, G J and Lindsj{\o}rn, Y},
  title = {A Web-based Support Environment for Software Engineering Experiments},
  year = {2002},
  abstract = {The software engineering communities frequently propose new software engineering technologies, such as new development techniques, programming languages and tools, without rigorous scientific evaluation. One way to evaluate software engineering technologies is through controlled experiments where the effects of the technology can be isolated from confounding factors, i.e., establishing cause-effect relationships. For practical and financial reasons, however, such experiments are often quite unrealistic, typically involving students in a class-room environment solving small pen-and-paper tasks. A common criticism of the results of the experiments is their lack of external validity, i.e., that the results are not valid outside the experimental conditions. To increase the external validity of the experimental results, the experiments need to be more realistic. The realism can be increased using professional developers as subjects who conduct larger experimental tasks in their normal work environment. However, the logistics involved in running such experiments are tremendous. More specifically, the experimental materials (e.g., questionnaires, task descriptions, code and tools) must be distributed to each programmer, the progress of the experiment needs to be controlled and monitored, and the results of the experiment need to be collected and analyzed. To support this logistics for large-scale, controlled experiments, we have developed a web-based experiment support environment called SESE. This paper describes SESE, its development and the experiences from using it to conduct a large controlled experiment in industry.},
  journal = {Nordic Journal of Computing},
  volume = {9},
  number = {4},
  pages = {231-247}
}

@Article{SE.4.Bratthall.2002,
  author = {Bratthall, L and J{\o}rgensen, M},
  title = {Can you trust a single data-source exploratory software engineering case-study},
  year = {2002},
  abstract = {},
  journal = {Journal of Empirical Software Engineering},
  volume = {7},
  pages = {9-26}
}

@Article{SE.4.Joergensen.2002.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Impact of Experience on Maintenance Skills},
  year = {2002},
  abstract = {This study reports results from an empirical study of 54 software maintainers in the software maintenance department of a Norwegian company. The study addresses the relationship between amount of experience and maintenance skills. The findings were, amongst others: (1) While there may have been a reduction in the frequency of major unexpected problems from tasks solved by very inexperienced to medium experienced maintainers, additional years of general software maintenance experience did not lead to further reduction. More application specific experience, however, further reduced the frequency of major unexpected problems. (2) The most experienced maintainers did not predict maintenance problems better than maintainers with little or medium experience. (3) A simple one-variable model outperformed the maintainers{\textquoteright} predictions of maintenance problems, i.e., the average prediction performance of the maintainers seems poor. An important reason for the weak correlation between length of experience and ability to predict maintenance problems may be the lack of meaningful feedback on the predictions.},
  journal = {Software Maintenance: Research and Practice},
  volume = {14},
  number = {2},
  pages = {123-146}
}

@Article{SE.4.Joergensen.2002.b,
  author = {J{\o}rgensen, M},
  title = {Comments on: A simulation tool for efficient analogy based cost estimation},
  year = {2002},
  abstract = {},
  journal = {Journal of Empirical Software Engineering},
  volume = {7},
  pages = {375-376}
}

@Inproceedings{SE.5.Anda.2002.a,
  author = {Anda, B},
  title = {Comparing Effort Estimates Based on Use Case Points with Expert Estimates},
  year = {2002},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Attributes of a use case model may therefore serve as measures of the size and complexity of the functionality of a system. Many organizations use a system's use case model in the estimation process. 
This paper reports the results from a study conducted to evaluate a method for estimating software development effort based on use cases, the use case points method, by comparing it with expert estimates. A system was described by a brief problem statement and a detailed use case model. The use case points method gave an estimate that was closer to the actual effort spent on implementing the system than most estimates made by 37 experienced professional software developers divided into 11 groups (MRE of 0.21 versus MMRE of 0.37). 
The results support existing claims that the use case points method may be used successfully in estimating software development effort. They also show that the combination of expert estimates and method based estimates may be particularly beneficial when the estimators lack specific experience with the application domain and the technology to be used.},
  booktitle = {Empirical Assessment in Software Engineering                   (EASE)},
  editor = {Anda},
  publisher = {-},
  address = {Keele, UK},
  isbn = {0}
}

@Inproceedings{SE.5.Anda.2002.b,
  author = {Anda, B and Sj{\o}berg, Dag I. K},
  title = {Towards an Inspection Technique for Use Case Models},
  year = {2002},
  abstract = {A use case model describes the functional requirements of a software system and is used as input to several activities in a software development project. The quality of the use case model therefore has an important impact on the quality of the resulting software product. Software inspection is regarded as one of the most efficient methods for verifying software documents. There are inspection techniques for most documents produced in a software development project, but no comprehensive inspection technique exists for use case models. This paper gives an overview of typical defects in use case models and proposes a checklist-based inspection technique for detecting such defects. This inspection technique was evaluated in two studies with undergraduate students as subjects. The results indicate that inspections are useful for detecting defects in use case models. However, more work is needed to establish appropriate inspection techniques.},
  booktitle = {Fourteenth IEEE Conference on Software                   Engineering and Knowledge Engineering (SEKE'02)},
  editor = {B Anda},
  pages = {127-134},
  publisher = {-},
  isbn = {1-58113-556-4}
}

@Inproceedings{SE.5.Anda.2002.c,
  author = {Anda, Bente Cecilie Dahlum and Angelvik, E and Ribu, K},
  title = {Improving Estimation Practices by Applying Use Case Models},
  year = {2002},
  abstract = {An estimation method based on use cases, the use case points method, has given promising results. However, more knowledge is needed about the contexts in which the method can be applied and how it should be adapted to local environments to improve the estimation process. We applied the use case points method to several projects in a Scandinavian software development company as the first activity in a software process improvement project on improving estimation. The second activity of the improvement project was to conduct interviews with project managers and senior developers about how to obtain continued and more widespread use of the method in the company. Based on the interviews, we propose a tailored, potentially improved version of the method and suggest how estimation practices can be improved by applying it. We believe that these experiences may be of interest to other companies that consider applying use case models as part of their estimation practices.},
  booktitle = {4th International Conference on Product                   Focused Software Process Improvement},
  editor = {B Anda},
  pages = {383-397},
  publisher = {Springer-Verlag},
  address = {Rovaniemi, Finland},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-00234-0}
}

@Inproceedings{SE.5.Arisholm.2002.a,
  author = {Arisholm, Erik},
  title = {Dynamic Coupling Measures for Object-Oriented Software},
  year = {2002},
  abstract = {The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and the fault-proneness of the classes. A common way to quantify the coupling is through static code analysis. However, the resulting static coupling measures only capture certain underlying dimensions of coupling. Other dependencies regarding the dynamic behavior of software can only be inferred from run-time information. For example, due to inheritance and polymorphism, it is not always possible to determine the actual receiver and sender classes (i.e., the objects) from static code analysis. 
This paper describes how several dimensions of dynamic coupling can be calculated by tracing the flow of messages between objects at run-time. As a first evaluation of the proposed dynamic coupling measures, fairly accurate prediction models of the change proneness of classes have been developed using change data from nine maintenance releases of a large SmallTalk system. Preliminary results suggest that dynamic coupling may also be useful for developing prediction models and tools supporting change impact analysis. At present, work on developing a dynamic coupling tracer and ripple-effect prediction models for Java programs is underway.},
  booktitle = {8th International Symposium on Software                   Metrics},
  pages = {33-42},
  publisher = {IEEE Computer Society},
  address = {Ottawa, Ont., Canada}
}

@Inbook{SE.5.Arisholm.2002.b,
  author = {Arisholm, Erik and Skagestein, G},
  title = {Kapittel 9: Fra skallet og inn (kravspesifikasjon)},
  year = {2002},
  abstract = {},
  booktitle = {Systemutvikling - Fra kjernen og ut, fra skallet                   og inn},
  editor = {G. Skagestein},
  publisher = {H{\o}yskoleforlaget}
}

@Inproceedings{SE.5.Arisholm.2002.c,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and Carelius, G J and Lindsj{\o}rn, Y},
  title = {SESE -- an Experiment Support Environment for Evaluating Software Engineering Technologies},
  year = {2002},
  abstract = {},
  booktitle = {NWPER'2002 (Tenth Nordic Workshop on                   Programming and Software Development Tools and                   Techniques)},
  editor = {-},
  pages = {81-98},
  publisher = {-},
  address = {Copenhagen, Denmark},
  isbn = {0000-0000}
}

@Inbook{SE.5.Arisholm.2002.d,
  author = {Arisholm, Erik and Skagestein, G},
  title = {Kapittel 10: Objektdesign},
  year = {2002},
  abstract = {},
  booktitle = {Systemutvikling - Fra kjernen og ut, fra skallet                   og inn},
  editor = {G. Skagestein},
  publisher = {H{\o}yskoleforlaget}
}

@Inproceedings{SE.5.Gallis.2002,
  author = {Gallis, H and Arisholm, Erik and Dyb{\r a}, T},
  title = {A Transition from Partner Programming to Pair Programming - an Industrial Case Study},
  year = {2002},
  abstract = {Pair programming is believed to provide many benefits compared with individual programming. However, implementing pair programming in a development project may not be trivial. At present, we are studying a small, industrial development project with two distributed pairs. Data has been collected through daily diaries, task questionnaires, feedback meetings and observation of the pairs. The preliminary results show that the two pairs are struggling to convert from their usual {\textquotedblleft}partner programming{\textquotedblright} habits to pair programming. We believe the main reason is that they mostly are used to working on their own individual tasks, but in close cooperation with another programmer in the same physical room (i.e., {\textquotedblleft}partner programming{\textquotedblright}). They are not convinced that a more structured collaboration (i.e., pair programming) will provide additional benefits. There is also a great difference between the pairs in the degree of resistance to pair programming. It seems that the pair with more experience and more knowledge about the system as a whole is more resistant to pair programming than the inexperienced pair. We believe that future empirical studies need to compare pair programming with other kinds of programmer collaboration, such as partner programming and team collocation. Research on pair programming should address when and how the various types of programmer collaboration are beneficial.},
  booktitle = {Workshop "Pair Programming Installed" in 17th                   Annual ACM Conference on Object-Oriented                   Programming, Systems, Languages, and                   Applications (OOPSLA), Position paper},
  publisher = {-},
  address = {Seattle USA}
}

@Inproceedings{SE.5.Hanssen.2002,
  author = {Hanssen, G K and Dyb{\r a}, T and St{\r a}lhane, T and Dings{\o}yr, T and Westerheim, H},
  title = {SPI - Easy in Theory, Hard in Practice},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of European Software Process Improvement
                   Conference (EuroSPI'2002)},
  address = {N{\"u}rnberg, Germany}
}

@Inproceedings{SE.5.Joergensen.2002.a,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Combination of software development effort prediction intervals: Why, when and how?},
  year = {2002},
  abstract = {The uncertainty of a software development effort estimate may be
described through a prediction interval, e.g., that the most likely
use of effort is 1.500 work-hours and that it is 90 \% probable
(90\% confidence level) that the actual use of effort will be
between 1.000 (minimum) and 2.000 (maximum) work-hours.
Previous studies suggest that software development effort
prediction intervals are, on average, much too narrow to reflect
high confidence levels, i.e., the uncertainty is under-estimated.
This paper analyses when and how a combination of several
individual prediction intervals of the same task improves the
correspondence between hit rate and confidence level of effort
prediction intervals. We analyse three combination strategies: (1)
Average of the individual minimum and maximum values, (2)
Maximum and minimum of the individual maximum and
minimum values, and (3) Group process (discussion) based
prediction intervals. Based on an empirical study with software
professionals we found that strategy (1) did not lead to much
correspondence improvement compared with the individual
prediction intervals, mainly because of a, as expected, strong
individual bias towards too narrow prediction intervals. Strategy
(2) and (3) both improved the correspondence. However, Strategy
(3) used the uncertainty information more efficiently, i.e., had
narrower prediction intervals for the same hit rate. Our empirical
results suggest that group discussion based combination of
prediction intervals should be used instead of {\a`\i}mechanical{\^\i}
combinations of individual prediction intervals. Clearly, there is
no best combination strategy for all prediction interval situations,
and the choice of strategy should be based on an investigation of
factors that impact the usefulness of a strategy.},
  booktitle = {Fourteenth IEEE Conference on Software                   Engineering and Knowledge Engineering (SEKE'02)},
  pages = {425-428},
  publisher = {-},
  address = {Ischia, Italy}
}

@Inproceedings{SE.5.Joergensen.2002.b,
  author = {J{\o}rgensen, M and Teigen, K H},
  title = {Uncertainty Intervals versus Interval Uncertainty: An Alternative Method for Eliciting Effort Prediction Intervals in Software Development Projects},
  year = {2002},
  abstract = {Frequently, there is a poor correspondence between the judged and the actual uncertainty of effort usage in software development projects. This may to some extent be a consequence of the uncertainty elicitation process. Traditionally, software developers are asked to provide the minimum and maximum effort of development work for a given confidence level, e.g., minimum and maximum effort that includes the actual effort usage with a 90\% probability. An alternative uncertainty elicitation process is to instruct the software developers to provide the uncertainty of a given effort interval, e.g., the probability that the actual effort is between 50\% and 200\% of the estimated most likely effort. In an empirical investigation, this alternative process led to significant improvement of prediction interval accuracy. The observed improvement using this alternative elicitation process can, we believe, be explained through a simplified interpretation of historical prediction accuracy data, less {\textquotedblleft}conflicting estimation goal{\textquotedblright}, and less influence from the {\textquotedblleft}anchoring effect{\textquotedblright}.},
  booktitle = {Proceedings of International Conference on Project                   Management (ProMAC)},
  pages = {343-352},
  publisher = {-},
  address = {Singapore}
}

@Inproceedings{SE.5.Joergensen.2002.c,
  author = {J{\o}rgensen, M and L{\o}vstad, N and Moen, L},
  title = {Combining quantitative software development cost estimation precision data with qualitative data from project experience reports at Ericsson Design Center in Norway},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Conference on Empirical                   Assesssments of Software Engineering},
  publisher = {-},
  address = {Keele, UK}
}

@Inproceedings{SE.5.Joergensen.2002.d,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {A simple effort prediction interval approach},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Conference on Achieving Quality in Information Systems},
  editor = {-},
  pages = {19-30},
  publisher = {-},
  address = {Venezia},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Karahasanovic.2002,
  author = {Karahasanovic, A and Sj{\o}berg, Dag I. K},
  title = {Visualising Impacts of Change in Evolving Object-Oriented Systems: An Explorative Study},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the International Workshop on Graph-Based Tools GraBaTs'02},
  editor = {-},
  pages = {22-31},
  publisher = {Technische Universitat Berlin},
  address = {Barcelona, Spain},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Koren.2002.a,
  author = {Koren, E and Hove, S E},
  title = {Using a Knowledge Survey to plan Software Process Improvement Activities -- a Case Study},
  year = {2002},
  abstract = {An important success factor for software process improvement is to identify the areas of the software process with the largest improvement potentials, and focus the improvement work on these areas. It is important to involve the employees in the improvement activities. Furthermore, recent research suggests that the exploitation of existing knowledge in an organisation is another important success factor for software process improvement. Consequently, it might be effective to focus improvement efforts on areas that exploit existing knowledge. This paper reports from a case study where we conducted a knowledge survey to identify the process improvement areas with high improvement potentials. A questionnaire was distributed to the developers of the organisation. The developers described their level of knowledge on general software engineering areas such as methods and programming languages. They also described how important they felt those areas would be in the future. Then, the resulting data were analyzed using simple statistical techniques. The results of the case study suggest that, when combined with other qualitative information, the knowledge survey was useful for identifying improvement areas that exploit existing knowledge. Furthermore, conducting a knowledge survey may have a positive impact on the improvement effort since the survey involved the employees.},
  booktitle = {EuroSPI 2002},
  pages = {213 - 224},
  publisher = {-},
  address = {N{\"u}rnberg, Germany}
}

@Inproceedings{SE.5.Koren.2002.b,
  author = {Koren, E and Westerheim, H},
  title = {Techniques for Gathering Information about an Organisation and its Needs for Software Process Improvement},
  year = {2002},
  abstract = {This paper reports on how the authors planned and conducted the work in the initial phase of software process improvement in a Norwegian software house. The work was motivated by the results from the projects SPIQ (Dyb{\r a}, Wedde et al. 2000) and PROFIT (http://www.geomatikk.no/profit), where a pragmatic approach to software process improvement was the main focus. The way the work was conducted was further motivated by the results of Tore Dyb{\r a} (Dyb{\r a} 2001). He claimed that for instance participation of employees and exploitation of existing knowledge has positive influence on the success of software process improvement. This led us to arrange workshops with participants from all relevant parts of the organisation to gather a picture as broad as possible of the situation. 
The paper is organised as follows: First we describe the motivation behind the choice of method. Then we describe the case study. Then we discuss the results we got from our investigations. The paper ends with a conclusion and a description of further work.},
  booktitle = {EASE 2002},
  publisher = {-},
  address = {Keele, England}
}

@Inproceedings{SE.5.Sjoeberg.2002,
  author = {Sj{\o}berg, Dag I. K and Anda, B and Arisholm, Erik and Dyb{\r a}, T and J{\o}rgensen, M and Karahasanovic, A and Koren, E and Vok{\a\'a}c, Marek},
  title = {Conducting Realistic Experiments in Software Engineering},
  year = {2002},
  abstract = {An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. 
This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes some of the experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.},
  booktitle = {ISESE'2002 (First International Symposium on                   Empirical Software Engineering)},
  editor = {not found},
  pages = {17-26},
  publisher = {IEEE Computer Society},
  isbn = {0-7695-1796-X}
}

@Inproceedings{SE.5.Teigen.2002,
  author = {Teigen, K H and J{\o}rgensen, M},
  title = {Probability Intervals and Interval Probabilities are Not the Same},
  year = {2002},
  abstract = {},
  booktitle = {Annual Meeting Society for Judgment and                   Decision Making (Poster)},
  publisher = {-},
  address = {Kansas, USA}
}

@Inproceedings{SE.5.Westerheim.2002,
  author = {Westerheim, H and Koren, E},
  title = {'Leave the programmers alone' -- A Case Study},
  year = {2002},
  abstract = {There has been much focus on the development process in the Software Process Improvement community. This paper describes software process improvement work done within a product division in a medium-sized Norwegian software company. The division has the main responsibility for the market activities, development, implementation, maintenance and support of their software products. We identified three main problem areas related to the overall process for management of the software product. None of the problem areas were directly concerned with the development process itself. The problem areas were addressed by the division's management. Despite the lack of focus on the programmers and the development process, the programmers' attitude changed into a more positive one. One reason might be that the new overall product handling process "protected" the programmers from the market activities, and also clarified responsibility for the core development and the maintenance development. This paper discusses the findings in the case-study, and concludes that maybe the best software process improvement initiative would be to simply leave the programmers alone.
},
  booktitle = {4th International Conference on Product                   Focused Software Process Improvement},
  pages = {291-299},
  publisher = {Springer-Verlag},
  address = {Rovaniemi, Finland},
  series = {Lecture Notes in Computer Science}
}

@Phdthesis{SE.3.Anda.2003,
  author = {Anda, B},
  title = {Empirical Studies of Construction and Application of Use Case Models},
  year = {2003},
  abstract = {Requirements engineering is a critical part of the development of software systems. A plethora of techniques has been proposed to elicit and document the requirements of a software system. One technique that is now widely used in industry is use case modelling. Although the technique imposes no particular constraints on the methodology used in other phases in a development project, it is most frequently used in combination with object-oriented development. In addition to serving as a requirements capture vehicle and a means for developers to communicate with end users and customers, use cases are claimed to be useful for estimating software development effort and for facilitating the transition from functional requirements to software design. 
Despite the important role of use case modelling in software development projects, and the numerous recommendations and claims that have been made about how to construct and apply use case models, there are very few systematic empirical studies in this field. 
This thesis investigates the use case modelling process and the role of use cases in software development projects by means of a number of empirical studies. In total, five experiments with 37 professional software developers and approximately 250 students as subjects, three industrial case studies and 11 interviews with project managers and senior developers were conducted. The empirical studies were conducted on the construction of use case models by the use of guidelines and inspections, and the application of use case models in (a) the estimation of software development effort and (b) the design of object-oriented systems. 
The results indicate that guidelines based on templates support the construction of use case models that are of higher quality, and that are easier to understand for the readers, than guidelines without specific details on how to document use cases. The results also indicate that quality may be further enhanced by combining the template guidelines with style guidelines for the documentation of the flow of events of each use case. 
A taxonomy of defects in use case models, and a checklist-based inspection technique to detect such defects, were proposed. The evaluation of this technique shows that it may increase the detection of serious defects in a use case model and also that there is a large difference between the defects detected by the developers of the use case model and by the end-users of the resulting system. 
The results further demonstrate that use case models can be used successfully to (a) estimate software development effort, (b) identify the prerequisites for successful use and (c) propose a refined and potentially improved version of an existing method for use case based estimation, the use case points method. 
The results also indicate that the quality of a design model is affected by the way in which a use case model is applied in an object-oriented design process, in particular, that a process which applies a use case model in validation results in class diagrams that implement more of the requirements, while a use case driven process results in class diagrams with a better structure. 
The empirical studies were exploratory because few similar studies have been conducted. Among other things, they required the development of original experimental designs. The major contribution of this thesis is, therefore, that it represents a starting point for more thorough empirical evaluation in the area of use case modelling},
  school = {University of Oslo},
  note = {ISSN 1501-7710, Nr. 268}
}

@Article{SE.4.Joergensen.2003.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K and Indahl, U},
  title = {Software Effort Estimation by Analogy and Regression Toward the Mean},
  year = {2003},
  abstract = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon {\textquotedblleft}regression toward the mean{\textquotedblright} (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  journal = {Journal of Systems and Software},
  volume = {68},
  number = {3},
  pages = {253-262}
}

@Article{SE.4.Joergensen.2003.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {An effort prediction interval approach based on the empirical distribution of previous estimation accuracy},
  year = {2003},
  abstract = {When estimating software development effort, it may be useful to describe the uncertainty of the estimate through an effort prediction interval (PI). An effort PI consists of a minimum and a maximum effort value and a confidence level. We introduce and evaluate a software development effort PI approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the effort PIs of new projects. First, we demonstrate the applicability and different variants of the approach on a data set of 145 software development tasks. Then, we experimentally compare the performance of one variant of the approach with human (software professionals{\textquoteright}) judgment and regression analysis-based effort PIs on a data set of 15 development tasks. Finally, based on the experiment and analytical considerations, we discuss when to base effort PIs on human judgment, regression analysis, or our approach.},
  journal = {Journal of Information and Software Technology},
  volume = {45},
  number = {3},
  pages = {123-136}
}

@Article{SE.4.Joergensen.2003.c,
  author = {J{\o}rgensen, M},
  title = {How much does a vacation cost? or What is a software cost estimate?},
  year = {2003},
  abstract = {What is a software cost estimate? Is it the most likely cost, the planned cost, the budget, the price, or,
something else? Through comparison with vacation cost estimation and a real-life case we illustrate that it is not
meaningful to compare and analyze cost estimates unless it is clear which interpretation is applied. Unfortunately,
the software industry, software engineering textbooks and scientific estimation studies do frequently not clarify how
they apply the term {\textquoteleft}cost estimate{\textquoteright}. We argue that this lack of clarity may lead to conflicting estimation goals,
communication problems, and, learning problems, and provide recommendations on how to deal with these
problems.},
  journal = {ACM Software Engineering Notes},
  volume = {28},
  number = {6},
  pages = {30}
}

@Inproceedings{SE.5.Anda.2003,
  author = {Anda, B and Sj{\o}berg, Dag I. K},
  title = {Applying Use Cases to Design versus Validate Class Diagrams -- A Controlled Experiment Using a Professional Modelling Tool},
  year = {2003},
  abstract = {Several processes have been proposed for the transition from functional requirements to an object-oriented design, but these processes have been subject to little empirical validation. A use case driven development process is often recommended when applying UML. Nevertheless, it has been reported that this process leads to problems, such as the developers missing some requirements and mistaking requirements for design. This paper describes a controlled experiment, with 53 students as subjects, conducted to investigate two alternative processes for applying a use case model in an object-oriented design process. One process was use case driven, while the other was a responsibility-driven process in which the use case model was applied as a means of validating the resulting class diagram. Half of the subjects used the modelling tool Tau UML Suite from Telelogic; the other half used pen and paper. The results show that the validation process led to class diagrams implementing more of the requirements. The use case driven process did, however, result in class diagrams with a better structure. The results also show that those who used the modelling tool spent more time on constructing class diagrams than did those who used pen and paper. We experienced that it requires much more effort to organize an experiment with a professional modelling tool than with only pen and paper.},
  booktitle = {ISESE'2003 (Second International Symposium on                   Empirical Software Engineering)},
  editor = {not found},
  pages = {50-60},
  publisher = {IEEE Computer Society},
  address = {Rome, Italy},
  isbn = {not found}
}

@Inproceedings{SE.5.Conradi.2003.a,
  author = {Conradi, R and Mohagheghi, P and Arif, T and Hegde, L C and Bunde, G A and Pedersen, A},
  title = {Object-Oriented Reading Techniques for Inspection of UML Models - An Industrial Experiment},
  year = {2003},
  abstract = {},
  booktitle = {European Conference on Object-Oriented                   Programming ECOOP'03},
  pages = {483-501},
  publisher = {Springer-Verlag},
  address = {Darmstadt, Germany},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Conradi.2003.b,
  author = {Conradi, R and Dyb{\r a}, T and Sj{\o}berg, Dag I. K and Ulsund, T},
  title = {Lessons Learned and Recommendations from two Large Norwegian SPI Programmes},
  year = {2003},
  abstract = {},
  booktitle = {9th European Workshop on Software Process                   Technology (EWSPT 2003)},
  editor = {Oquendo},
  pages = {32-45},
  publisher = {Springer-Verlag},
  address = {Helsinki, Finland},
  series = {Lecture Notes in Computer Science},
  isbn = {978-3-540-40764-5}
}

@Inproceedings{SE.5.Gallis.2003.a,
  author = {Gallis, H and Arisholm, Erik and Dyb{\r a}, T},
  title = {An Initial Framework for Research on Pair Programming},
  year = {2003},
  abstract = {In recent years, several claims have been put forward in favour of pair programming, as opposed to individual programming. However, results from existing studies on pair programming contain apparent contradictions. The differences in the context in which the studies were conducted may be one explanation for such results. This paper presents an initial framework for research on pair programming. The aim is to support empirical studies and meta-analysis for developing theories about pair programming. The framework is based on (1) existing studies on pair programming, (2) ongoing studies by the authors, and (3) theories from group dynamics.},
  booktitle = {ISESE'2003 (Second International Symposium on                   Empirical Software Engineering)},
  pages = {132-142},
  publisher = {IEEE Computer Society},
  address = {Rome, Italy}
}

@Inproceedings{SE.5.Gallis.2003.b,
  author = {Gallis, H},
  title = {Collaboration on Software Tasks},
  year = {2003},
  abstract = {In pair programming (PP), two developers work together on the same task using one computer and keyboard. PP involves not just coding, but many phases of the software development process such as design and testing. PP has been proposed by several authors since the 70{\textquoteright}s, but it is primarily during the last three to four years that some of the claimed benefits have been tested empirically. 

Initial results indicate many benefits in favour of PP. However, there are some studies that conflict with these benefits. One explanation of the apparently contradicting results may be differences in the context in which the studies were conducted. Existing studies on PP have so far compared PP with individual programming. However, PP is just one way in which programmers can collaborate on software development tasks. Partner programming and team collocation are examples of other levels of programmer collaboration. Furthermore, most studies have so far used students as subjects. There is a need for more realistic studies using professionals as subjects to ensure external validity.},
  booktitle = {Fourth International Conference on eXtreme                   Programming and Agile Processes in Software                   Engineering (XP 2003)},
  pages = {423-424},
  publisher = {-},
  address = {Genova, Italy},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Hannay.2003.a,
  author = {Hannay, J},
  title = {Axiomatic Criteria for Quotients and Subobjects at Higher Order},
  year = {2003},
  abstract = {Axiomatic criteria are given for the existence of higher-order maps over subobjects and quotients. These criteria are applied in showing the soundness of a method for proving specification refinement up to observational equivalence. This generalises the method to handle data types with higher-order operations. We also give a direct setoid-based model satisfying the criteria. The setting is the second-order polymorphic lambda calculus and the assumption of relational parametricity.},
  booktitle = {Automata, Languages and Programming. Proceedings of ICALP 2003, 30th International Colloquium, Eindhoven, the Netherlands, LNCS volume 2719},
  pages = {903-917},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Hannay.2003.b,
  author = {Hannay, J},
  title = {Abstraction Barrier-Observing Relational Parametricity},
  year = {2003},
  abstract = {A concept of relational parametricity is developed where the encapsulation mechanism inherent in universal types is taken into account. This is then applied in the context of data types and refinement, naturally giving rise to a notion of simulation relations that compose for data types with higher-order operations, and whose existence coincides with observational equivalence. The ideas are developed syntactically in lambda calculus with a relational logic. The new notion of relational parametricity is asserted axiomatically, and a corresponding parametric PER-semantics is devised.},
  booktitle = {Typed Lambda Calculi and Applications. Proceedings of TLCA, 6th International Conference, Valencia, Spain, LNCS volume  2701},
  pages = {135-152},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Hannay.2003.c,
  author = {Hannay, J and Katsumata, S -Y and Sannella, D T},
  title = {Semantic and Syntactic Approaches to Simulation Relations},
  year = {2003},
  abstract = {Simulation relations are tools for establishing the correctness of data refinement steps. In the simply-typed lambda calculus, logical relations are the standard choice for simulation relations, but they suffer from certain shortcomings; these are resolved by use of the weaker notion of pre-logical relations instead. Developed from a syntactic setting, abstraction barrier-observing simulation relations serve the same purpose, and also handle polymorphic operations. Meanwhile, second-order pre-logical relations directly generalise pre-logical relations to polymorphic lambda calculus (System F). We compile the main refinement-pertinent results of these various notions of simulation relation, and try to raise some issues for aiding their comparison and reconciliation.},
  booktitle = {Mathematical Foundations of Computer Science. Proceedings of MFCS, 28th International Symposium on Mathematical Foundations of Computer Science, Bratislava, Slovak Republic, LNCS volume 2747},
  pages = {68-91},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science},
  note = {Invited paper.}
}

@Misc{Simula.SC.126,
  author = {Nielsen, Bj{\o}rn Fredrik and Lysaker, Marius and Gr{\o}ttum, Per and Tveito, Aslak and Abildgaard, Andreas and Fjeld, Jan Gunnar and Hermann Haugaa, Kristina},
  title = {The inverse problem of identifying ischemic heart disease},
  year = {2008},
  abstract = {},
  howpublished = {Presented at the annual meeting of European Cardiac Simulation Group, Bologna, Italy}
}

@Techreport{Simula.SE.237,
  author = {Khosrovian, Keyvan and Pfahl, Dietmar and Garousi, Vahid},
  title = {GENSIM 2.0: A Customizable Process Simulation Model for Software Process Evaluation},
  year = {2008},
  abstract = {Software process analysis and improvement relies heavily on empirical research. Empirical research requires measurement, experimentation, and modeling. However, whatever evidence is gained via empirical research is strongly context dependent. Thus, it is hard to combine results and capitalize upon them in order to improve software development processes in evolving development environments. The process simulation model GENSIM 2.0 addresses the challenge mentioned above. Compared to existing process simulation models in the literature, the novelty of GENSIM 2.0 is twofold: (1) Model structure is customizable to organization-specific processes. This is achieved by using a limited set of generic structures (macro-patterns). (2) Model parameters can be easily calibrated to available empirical data and ex-pert knowledge. This is achieved by making the internal model structures explicit and by providing guidance on how to calibrate model parameters. This technical report explains the overall structure of GENSIM 2.0, its internal mechanisms, and its parameters.},
  institution = {University of Calgary and Simula Research Laboratory}
}

@Inproceedings{Simula.ND.221,
  author = {Vik, Knut-Helge and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Constructing Low-Latency Overlay Networks: Tree vs. Mesh Algorithms},
  year = {2008},
  abstract = {Distributed interactive applications may have stringent latency requirements and dynamic user groups. These applications may benefit  from a group communication system, and to improve the system support for such applications, we investigate graph algorithms that construct low-latency overlay networks for application-layer multicast.  In particular, we focus on reducing the diameter and the pair-wise latencies in the overlay.  The overlay construction time is also considered,  as it is often time-dependent in our dynamic target applications.  Here, we have implemented and experimentally analyzed spanning-tree heuristics and mesh construction heuristics, and compared their performance and applicability to distributed interactive applications. We found that trees are faster to construct and save considerable amounts of resources in the network. Meshes, on the other hand, yield lower pair-wise latencies and increases the fault tolerance, but at the expense of increased resource consumption.},
  booktitle = {Proceedings of IEEE Conference on Local Computer Networks (LCN)}
}

@Article{Simula.SC.114,
  author = {Sundnes, Joakim and Artebrant, Robert and Skavhaug, Ola and Tveito, Aslak},
  title = {A second order algorithm for solving dynamic cell membrane equations},
  year = {2008},
  abstract = {This paper describes an extension of the so-called Rush-Larsen scheme, which is a  widely used numerical method for solving dynamic models of cardiac cell  electrophysiology. The proposed method applies a local linearization of non-linear  terms in combination with the analytical solution of linear ordinary differential equations, to obtain a second order accurate numerical scheme. We compare the error and computational load of the second order scheme to the original Rush-Larsen  method. For a given error tolerance, the second order method is from nine to 500 times  faster than the original scheme, depending on the choice of model and tolerance.},
  journal = { journal for publication}
}

@Article{Simula.ND.127,
  author = {Wang, Yanbo and Xin, Qin and Coenen, Frans},
  title = {Hybrid Rule Ordering in Classification Association Rule Mining},
  year = {2008},
  abstract = {Classification Association Rule Mining (CARM) is an approach to classifier generation that builds an Association Rule Mining based classifier using Classification Association Rules (CARs). Regardless of which particular CARM algorithm is used, a similar set of CARs is always generated from data, and a classifier is usually presented as an ordered list of CARs, based on a selected rule ordering strategy. Hence to produce an accurate classifier, it is essential to develop a rational rule ordering mechanism. In the past decade, a number of rule ordering strategies have been introduced. Six major ones can be identified: Confidence Support \& size-of-Antecedent (CSA), size-of-Antecedent Confidence \& Support (ACS), Confidence Support size-of-Antecedent class-distribution-Frequency \& Row-ordering (CSAFR), Weighted Relative Accuracy (WRA), Laplace Accuracy (LA), and Chi-square Testing ($\Lambda^2$). Broadly speaking, these strategies can be categorized into two groups: Support-Confidence (including CSA, ACS and CSAFR) and Rule Weighting (including WRA, LA and $\Lambda^2$). In this paper, we propose a hybrid rule ordering approach (framework) by combining one strategy taken from Support-Confidence and another strategy taken from Rule Weighting, which consequently develops nine rule ordering mechanisms. The experimental results demonstrate that all developed mechanisms perform well with respect to the accuracy of classification.},
  journal = {Transaction on Machine Learning and Data Mining}
}

@Article{Simula.SE.277,
  author = {Garousi, Vahid and Briand, Lionel and Labiche, Yvan},
  title = {A UML-based Quantitative Framework for early Prediction of Resource Usage and Load in Distributed Real-Time Systems},
  year = {2008},
  abstract = {},
  journal = {Software and Systems Modeling (Springer)}
}

@Inproceedings{Simula.ND.219,
  author = {Vik, Knut-Helge and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {On the Influence of Latency Estimation on Dynamic Group Communication using Overlays},
  year = {2009},
  abstract = {Distributed interactive applications tend to have stringent latency requirements and some may have high bandwidth demands. Many of them have also very dynamic user groups for which all-to-all communication is needed. In online multiplayer games, for example, such groups are determined through region-of-interest management in the application. We have investigated a variety of group management approaches for overlay networks in earlier work and shown that several useful tree heuristics exist. However, these heuristics require full knowledge of all overlay link latencies. Since this is not scalable, we investigate the effects that latency estimation techniques have on the quality of overlay tree constructions. We do this by evaluating one example of our group management approaches in Planetlab and examining how latency estimation techniques influence their quality. Specifically, we investigate how two well-known latency estimation techniques, \textit{Vivaldi} and \textit{Netvigator}, affect the quality of tree building.},
  booktitle = {Proceedings of Multimedia Computing and Networking (MMCN'09)}
}

@Inproceedings{Simula.ND.225,
  author = {Beskow, Paul B and Vik, Knut-Helge and Halvorsen, P{\r a}l and Griwodz, Carsten},
  title = {Latency Reduction by Dynamic Core Selection and Partial Migration of Game State },
  year = {2008},
  abstract = {Massively multi-player online games (MMOGs) require low latency while supporting a large number of concurrent players, often sharing one worldwide instance of the game. As these are conflicting requirements, a common way of distributing load is by dividing the virtual environment into virtual regions. As MMOGs are world-spanning games, it is plausible to disperse these regions on geographically distributed servers. As such, we propose the use of core selection for finding an optimal server for placing a region, and support for migrating the game state to that server. The first goal relies on a set of servers and measurement of the interacting players latencies. In locating an optimal server, we anticipate a decrease in the overall latency for the majority of players. This reduction occurs by migrating the region to a server closer in proximity to the majority of players in that virtual region, thereby lowering the response time of any interaction.},
  booktitle = {Proceedings of Workshop on Network and Systems Support for Games (NetGames)}
}

@Article{Simula.SE.311,
  author = {J{\o}rgensen, Magne and Halkjelsvik, Torleif},
  title = {The Effects of Request Formats on Judgment-based Effort Estimation},
  year = {2008},
  abstract = {In this paper we study the effects of a change from the traditional request {\textquotedblleft}How much effort is required to complete X?{\textquotedblright} to the alternative {\textquotedblleft}How much can be completed in Y work-hours?{\textquotedblright}. Studies 1 and 2 report that software professionals receiving the alternative format provided much lower, and presumably more optimistic, effort estimates of the same software development work than those receiving the traditional format. Studies 3 and 4 suggest that the effect belongs to the family of anchoring effects. An implication of our results is that project managers and clients should avoid the alternative estimation request format.},
  journal = {Journal of Systems and software}
}

@Article{Simula.ND.137,
  author = {Shu, L and Zhang, Y and Zhou, Z and Hauswirth, M and Yu, Z and Hynes, G},
  title = {Transmitting and Gathering Streaming Data in Wireless Multimedia Sensor Networks within Expected Network Lifetime},
  year = {2008},
  abstract = {},
  journal = {ACM/Springer Mobile Networks and Applications (MONET)}
}

@Conference{Simula.SE.247,
  author = {Briand, Lionel},
  title = {Novel Applications of Machine Learning in Software Testing (Keynote address)},
  year = {2008},
  abstract = {},
  booktitle = {Eigth IEEE International Conference on Quality Software},
  note = {Keynote address}
}

@Techreport{Simula.SE.249,
  author = {Khosrovian, Keyvan and Pfahl, Dietmar and Garousi, Vahid},
  title = {Calibrating a Customizable System Dynamics Simulation Model of Generic Software Development Processes},
  year = {2008},
  abstract = {GENSIM 2.0 is a customizable system dynamics simulation model of generic software development processes which takes as input the specifics of software development projects (e.g. size of the code document, headcount of the developer team and their skills) and generates as output a broad range of distinct variables (e.g. software product quality, total project effort) of interest to different users of the model. This technical report is dedicated to elaborate on current calibration of GENSIM 2.0, its calibration parameters and the source from which they can be potentially calibrated.},
  institution = {University of Calgary and Simula Research Laboratory},
  number = {University of Calgary SERG 2007-08 and Simula TR 2008-02}
}

@Misc{Simula.ND.199,
  author = {Kaspar, Dominik and Hansen, Audun F},
  title = {A Survey of Network Striping},
  year = {2008},
  abstract = {Multilink transfer is the method of connecting a single host over multiple access networks to the Internet, with the potential of increasing communication quality. However, link heterogeneity causes severe challenges on end-to-end traffic. We explain the problems in more detail and how we intend to solve them using a proxy solution.},
  howpublished = {Technical Report},
  note = {This is a "living" document that will be regularly updated as the state of the Simtel project evolves.}
}

@Conference{Simula.SC.150,
  author = {Stegman, Dave and Capitano, F A and Moresi, Louis and Mueller, D and Clark, Stuart R},
  title = {4-D Subduction Models Incorporating an Upper Plate},
  year = {2007},
  abstract = {Thus far, relatively simplistic models of free subduction have been employed in which the trench and plate kinematics are emergent features completely driven by the negative buoyancy of the slab. This has allowed us to build a fundamental understanding of subduction processes such as the kinematics of subduction zones, the strength of slabs, and mantle flow-plate coupling. Additionaly, these efforts have helped to develop appreciable insight into subduction processes when considering the energetics of subduction, in particular how energy is dissipated in various parts of the system such as generating mantle flow and bending the plate. We are now in a position to build upon this knowledge and shift our focus towards the dynamic controls of deformation in the upper plate (vertical motions, extension, shortening, and dynamic topography). Here, the state of stress in the overriding plate is the product of the delicate balance of large tectonic forces in a highly-coupled system, and must therefore include all components of the system: the subducting plate, the overriding plate, and the underlying mantle flow which couples everything together. We will present some initial results of the fully dynamic 3-D models of free subduction which incorporate an overriding plate and systematically investigate how variations in the style and strength of subduction are expressed by the tectonics of the overriding plate. Deformation is driven in the overriding plate by the forces generated from the subducting plate and the type of boundary condition on the non-subducting side of the overriding plate (either fixed or free). Ultimately, these new models will help to address a range of issues: how the overriding plate influences the plate and trench kinematics; the formation and evolution of back-arc basins; the variation of tractions on the base of the overriding plate; the nature of forces which drive plates; and the dynamics controls on seismic coupling at the plate boundary.},
  booktitle = {Eos Trans., Fall Meet. Suppl.},
  volume = {88},
  number = {52},
  publisher = {American Geophysical Union}
}

@Misc{Simula.SC.162,
  author = {Clark, Stuart R},
  title = {Geodynamics: The Physics and Mathematics of Terrestrial and Extra-Terrestrial Processes},
  year = {2008},
  abstract = {},
  howpublished = {Talk at the Center for Biomedical Computing Annual Meeting }
}

@Article{Simula.ND.131,
  author = {Zhang, Y and Chen, Y and He, J and Wang, C and Vasilakos, Athanasios V},
  title = {Call Admission Control Algorithms in OFDM-Based Wireless Multiservice Networks},
  year = {2008},
  abstract = {},
  journal = {Springer Wireless Personal Communications}
}

@Article{Simula.ND.133,
  author = {Guo, H and Hu, H and Zhang, Y},
  title = {A High-Throughput Random Access Protocol for Multiuser MIMO Systems},
  year = {2008},
  abstract = {},
  journal = {EURASIP Research Letters in Communications}
}

@Article{Simula.ND.135,
  author = {Leng, S and Zhang, Y and Chen, H and Zhang, L and Liu, K},
  title = {A Novel k-hop Compound Metric based Clustering Scheme for ad hoc Wireless Networks},
  year = {2008},
  abstract = {},
  journal = {IEEE Transaction on Wireless Communications}
}

@Inproceedings{Simula.ND.141,
  author = {Hong, X and Wang, C and Thompson, J and Zhang, Y},
  title = {Demystifying Spatial White Spaces Using Stochastic Geometry},
  year = {2008},
  abstract = {},
  booktitle = { International Conference on Communications, Circuits and Systems (ICCCAS 2008)}
}

@Inproceedings{Simula.ND.143,
  author = {Zhu, X and Zhu, G and Jiang, T and Zhang, Y},
  title = {Extended Iterative Flipping Algorithm for PAPR Reduction in OFDM Systems},
  year = {2008},
  abstract = {},
  booktitle = {International Conference on Communications and Networking in China (CHINACOM 2008)}
}

@Inproceedings{Simula.ND.145,
  author = {Xie, L and Xiang, J and Zhang, Y},
  title = {Revenue-based Admission Control for Cognitive Radio Cellular Systems},
  year = {2008},
  abstract = {},
  booktitle = {2008 International Workshop on Cognitive Networks and Communications (COGCOM 2008), in conjunction with CHINACOM 2008}
}

@Article{Simula.SC.116,
  author = {Tveito, Aslak and Lines, Glenn Terje},
  title = {A note on a method for determining advantageous properties of an anti-arrhythmic drug based on a mathematical model of cardiac cells},
  year = {2008},
  abstract = {},
  journal = {journal for publication}
}

@Inproceedings{Simula.ND.147,
  author = {Pham, Hai Ngoc and Xiang, Jie and Zhang, Yan and Skeie, Tor},
  authorURLs = {http://heim.ifi.uio.no/\~hainp/ and /people/jxiang and /people/yanzhang and /people/tskeie},
  title = {QoS-Aware Channel Selection in Cognitive Radio Networks: A Game-Theoretic Approach},
  year = {2008},
  abstract = {Abstract{\textemdash}In a cognitive radio wireless network, each node can sense and opportunistically access the under-utilized spectrums in the primary system. Since the unoccupied spectrum is locationdependent and time-dependent, the available spectrums in each node are different.With this spectrum heterogeneity and different Quality-of-Service (QoS) requirement, different node may have different preference in using a particular channel to communicate with its neighboring nodes. In this paper, we formulate this channel selection problem using a cooperative game theoretical approach such that the QoS requirement of each node is guaranteed and the total throughput is maximized. To further improve the performance, a learning negotiation mechanism is introduced. The key motivation is to derive new mixed strategies of the nodes with the reference to the historical profiles of all selected strategies in the past. Simulation results are presented to show the fast convergence of the game, the high efficiency of the learning mechanism, and the effect of mobility.},
  booktitle = {IEEE Global Communications Conference (GLOBECOM 2008)}
}

@Inproceedings{Simula.ND.149,
  author = {Xie, Lang and Xiang, Jie},
  title = {A Novel Bandwidth Degradation Scheme for Admission Control in IEEE 802.16e Networks},
  year = {2008},
  abstract = {IEEE 802.16e networks can be widely applied in broadband wireless access scenarios. Admission control is an essential mechanism in IEEE 802.16e networks to guarantee the QoS of different subscribers. In this paper, we propose a novel bandwidth degradation scheme for the admission control in IEEE 802.16e networks. Simulation results are presented to demonstrate the performance of the proposed scheme in terms of service flow blocking rate and system bandwidth utilization.},
  booktitle = { the 4th IEEE International Conference on Wireless Communications, Networking and Mobile Computing (WiCOM 2008)}
}

@Inproceedings{Simula.SE.251,
  author = {Briand, Lionel and Labiche, Yvan and Bawar, Zaheer},
  title = {Using Machine Learning to Refine Black-Box Test Specifications and Test Suites},
  year = {2008},
  abstract = {},
  booktitle = {Eigth IEEE International Conference on Quality Software}
}

@Article{Simula.SE.303,
  author = {Briand, Lionel and Labiche, Yvan and He, Siyuan},
  title = {Automating Regression Test Selection Based on UML Designs},
  year = {2008},
  abstract = {},
  journal = {Information and Software Technology (Elsevier)}
}

@Inproceedings{Simula.SE.283,
  author = {Araujo, Wladimir and Briand, Lionel and Labiche, Yvan},
  title = {Concurrent Contracts for Java in JML},
  year = {2008},
  abstract = {},
  booktitle = {IEEE International Symposium on Software Reliability Engineering (ISSRE)}
}

@Misc{Simula.SC.120,
  author = {Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik and Gr{\o}ttum, Per and Abildgaard, Andreas and Fjeld, Jan and Haugaa, K},
  title = {Theoretical and practical aspects of the inverse problem of electrocardiography},
  year = {2008},
  abstract = {},
  howpublished = {Presented at the Fourth International Conference "Inverse Problems: Modeling and Simulation", Turkey}
}

@Article{Simula.ND.195,
  author = {Chen, Y and Hu, L and Yuen, C and Zhang, Y and Zhang, Z and Rapajic, P},
  title = {Intrinsic Measure of Diversity Gains in Generalized Distributed Antenna Systems with Cooperative Users},
  year = {2008},
  abstract = {},
  journal = { IET Communications (Formerly IEE Proceedings Communications)}
}

@Techreport{Simula.SE.293,
  author = {Ali, Shaukat and Briand, Lionel C and Hemmati, Hadi and Panesar-Walawege, Rajwinder Kaur},
  title = {A Systematic Review of the Application and Empirical Investigation of Evolutionary Testing},
  year = {2008},
  abstract = {Metaheuristic search techniques have been extensively used to automate the process of generating test cases and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined as {\textquotedblleft}Evolutionary Testing{\textquotedblright} (ET), has been used for a wide variety of test case generation purposes, differing in terms of test objectives, test levels, and other characteristics. Since ET techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study ET techniques have shown wide variation in the literature.  This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate ET cost-effectiveness and what empirical evidence is available in the literature regarding ET cost-effectiveness and scalability. We also provide a framework that drives the data collection process of this systematic review and can be the starting point of guidelines on how ET techniques can be empirically assessed.},
  institution = {Simula Research Laboratory}
}

@Article{Simula.SE.223,
  author = {J{\o}rgensen, Magne and Gruschke, Tanja},
  title = {The Impact of Lessons-Learned Sessions on Effort Estimation and Uncertainty Assessments},
  year = {2008},
  abstract = {Inaccurate estimates of software development effort is a frequently reported cause of IT-project failures. We report results from a study that investigated the effect of introducing lessons learned sessions on estimation accuracy and the assessment of uncertainty. Twenty software professionals were randomly allocated to a Learning group or a Control group and instructed to estimate and complete the same five development tasks. Those in the Learning group, but not those in the Control group, were instructed to spend at least 30 minutes on identifying, analyzing, and summarizing their effort estimation and uncertainty assessment experience after completing each task. We found that the estimation accuracy and the realism of the uncertainty assessment were not better in the Learning group than in the Control group. A follow-up study with 83 software professionals was completed to better understand this lack of improvement from lessons-learned sessions. The follow-up study found that receiving feedback about other software professionals{\textquoteright} estimation performance led to more realistic uncertainty assessments than receiving the same feedback of one{\textquoteright}s own estimates. Lessons-learned sessions, we argue, have to be carefully designed to avoid wasting resources on learning processes that stimulate rather than reduce learning biases related to assessment of own estimation performance.},
  journal = {IEEE Transactions of Software Engineering}
}

@Inproceedings{Simula.ND.185,
  author = {Petlund, Andreas and Evensen, Kristian and Halvorsen, P{\r a}l and Griwodz, Carsten},
  title = {Improving application layer latency for reliable thin-stream game traffic },
  year = {2008},
  abstract = {A wide range of networked games send data with very high interarrival-time between packets and with small payloads in  each packet. We call these transmission patterns ``thin streams''.  Reliability, when needed for game traffic,  is usually achieved through TCP or by using retransmission  schemes modelled on TCP. These retransmission schemes can  result in very large delays if the stream is  thin.  Viewed in the light of the time-dependent nature of game  traffic, large delays can be severely impeding  to the game experience. In order to reduce application-layer latency when packets  are lost, we have implemented modifications to TCP in the  Linux kernel. The changes are only active when thin-stream  properties are detected, thus not affecting TCP behaviour  when the stream is not thin. In this paper, we show the latency improvements from these  thin-stream modifications. As a case study, we have used  the game BZFlag to test the mechanisms, and present statistics  from these tests. The experimental results show that our modifications allow TCP  to recover earlier from packet loss. This latency reduction was then shown to improve the difference between perceived and actual player positions in the BzFlag game.},
  booktitle = {Netgames, Worcester, Ma. US, 2008}
}

@Article{Simula.SE.243,
  author = {Briand, Lionel and Labiche, Yvan and Yue, Tao},
  title = {Automated Traceability Analysis for UML Model Refinements},
  year = {2008},
  abstract = {},
  journal = {Information and Software Technology (Elsevier)}
}

@Techreport{Simula.SE.253,
  author = {Khosrovian, Keyvan and Pfahl, Dietmar and Garousi, Vahid},
  title = {Application Scenarios for a Customizable System Dynamics Simulation Model of Generic Software Development Processes},
  year = {2008},
  abstract = {GENSIM 2.0 is a customizable system dynamics simulation model of generic software development processes which takes as input specifics of software development projects (e.g. size of the code document, headcount of the developer team and their skills) and generates as output a broad range of distinct variables (e.g. software product quality, total project effort) of interest to different users of the model. This technical report is dedicated to illustrate example scenarios of the application of GENSIM 2.0 to address software development process issues. Firstly, it is shown how GENSIM 2.0 could be used to find the most suitable combination of verification and validation techniques in order to achieve defined time, quality and cost goals in a given context. Secondly, it is used to figure out the most promising investments in a development project{\textquoteright}s workforce. These scenarios represent only a small subset of the numerous situations that GENSIM 2.0 can be applied to.},
  institution = {University of Calgary and Simula Research Laboratory},
  number = {University of Calgary SERG-2007-09 and Simula TR 2008-03}
}

@Article{Simula.ND.233,
  author = {Xin, Qin and Zhang, Yan and Yang, Laurence T},
  title = {Optimal Fault-tolerant Broadcasting in Wireless Mesh Networks},
  year = {2008},
  abstract = {},
  journal = {Wiley Wireless Communications and Mobile Computing}
}

@Conference{Simula.SC.156,
  author = {Clark, Stuart R and Bruaset, Are Magnus and Loseth, Tore},
  title = {Handling Uncertainty in Numerical Models of Sedimentary Deposition: A Stochastic Approach},
  year = {2008},
  abstract = {Forward stratigraphic models simulates the process of sedimentary deposition over geological time periods and helps geoscientists and engineers to understand the location of reservoir rocks and stratigraphic traps. The most popular and commonly used models are diffusion-based. A major draw-back with such models is that diffusion coefficients, that largely control the outcome of the simulations, are highly uncertain and difficult to derive., This clearly reduces their usefulness at describing the evolution of a basin. Diffusion coefficients represent conglomerations of physical processes in sedimentation and basin morphology that depend, for instance, on slope. By considering the diffusion coefficients as randomly distributed fields, the uncertainty in the morphology of a basin can be constrained. We use a Polynomial Chaos Expansion (PCE) to approximate the model at each spatial and temporal point of interest. The Probabilistic Collocation Method (PCM) is used to calculate the coefficients of the PCE, based on a few, specifically chosen, calculations of the underlying model of sedimentation. In contrast to the Monte Carlo approach, which may require hundreds or thousands of runs of the simulator, the PCM requires only a dozen or so, depending on the number of inputs and the desired accuracy. Furthermore, the PCM is not tied to a particular implementation of a sedimentation model, as it treats the model as a blackbox, and can be placed upon any depositional simulator. This method allows for the quantification of the stochastic moments of the height and the fractional distribution of the materials. By understanding the spatial distribution of the uncertainty in basin evolution, these depositional models can begin to have real meaning in industrial applications. Our approach is not dependent on depositional modelling, but can be applied to any process-oriented geological simulation.},
  booktitle = {Proceedings of the 33rd International Geological Congress},
  publisher = {IGC (Oslo, 2008)}
}

@Article{Simula.SE.299,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment},
  year = {2008},
  abstract = {Several studies have reported that software development effort estimates can be strongly affected by effort-irrelevant and misleading information without the estimators being aware of this effect. These studies were conducted in laboratory (artificial) estimation contexts. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from Eastern European and East Asian countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: i) reduced length of requirement specification with no change of content, ii) information about the low effort spent on the development of the old system to be replaced, iii) information about the client{\textquoteright}s unrealistic expectations about low cost, and iv) a restriction of a short development period with start up a few months ahead (which should, rationally speaking, lead to an increase in effort). All manipulations led to decreased median effort estimates, but only manipulation iv) led to a large, statistically significant decrease. A comparison of the effects of similar types of irrelevant and misleading information in laboratory and field settings suggests that the effect of manipulations i), ii) and iii) where much lower in field settings than in laboratory settings, while the effect of manipulation iv) was almost at the same level. We conclude that the tendency towards a smaller  effect in field settings means that laboratory studies are frequently only useful for demonstrating the existence of a software engineering phenomenon, or for understanding it better, and that we need field studies to analyze its importance.},
  journal = {IEEE Transactions on Software Engineering}
}

@Inproceedings{Simula.ND.217,
  author = {Elmokashfi, Ahmed and Kvalbein, Amund and Dovrolis, Constantine},
  title = {On the Scalability of BGP:  the roles of topology growth and update rate-limiting},
  year = {2008},
  abstract = {The scalability of BGP routing is a major concern for the Internet community. Scalability is an issue in two different aspects: increasing routing table size, and increasing rate of BGP updates. In this paper, we focus on the latter. Our objective is to characterize the churn increase experienced by ASes in different levels of the Internet hierarchy as the network grows. We look at several ``what-if'' growth scenarios that are either plausible directions in the evolution of the Internet or educational corner cases, and investigate their scalability implications. In addition, we examine the effect of the BGP update rate-limiting timer (MRAI), considering both major variations with which it has been deployed. Our findings explain the dramatically different impact of multihoming and peering on BGP scalability, identify which topological growth scenarios will lead to faster churn increase, and emphasize the importance of not rate-limiting explicit withdrawals (despite what RFC-4271 recently required).},
  booktitle = {CoNext 2008}
}

@Article{Simula.SE.301,
  author = {Benestad, Hans Christian and Anda, Bente Cecilie Dahlum and Arisholm, Erik},
  title = {Understanding the cost of software change: A quantitative and qualitative investigation of change effort during evolution of two software systems},
  year = {2008},
  abstract = {Making changes to software systems can prove costly and it remains a challenge to understand the factors that affect the costs of software evolution. This study sought to identify such factors by investigating the effort expended by developers to perform 336 change tasks in two different software organizations. We quantitatively analyzed data from version control systems and change trackers to identify factors that correlated with change effort. In-depth interviews with the developers about a subset of the change tasks further refined the analysis. Two central quantitative results found that volatility of requirements and dispersion of changed code consistently correlated with change effort. The analysis of the qualitative interviews pointed to two important, underlying cost drivers: Difficulties in anticipating side effects of changes and difficulties in comprehending dispersed code. This study demonstrates a novel method for combining qualitative and quantitative analysis to assess cost drivers of software evolution. Given our findings, we propose improvements to design practices and development tools to reduce the costs.},
  journal = {A journal}
}

@Article{Simula.ND.161,
  author = {He, J and Zhang, Y and kaleshi, D and Munro, A and McGeehan, J},
  title = {Dynamic Spectrum Access in Heterogeneous Unlicensed Wireless Networks},
  year = {2008},
  abstract = {},
  journal = {Inderscience International Journal of Autonomous and Adaptive Communications Systems (IJAACS), invited paper}
}

@Book{Simula.SE.231,
  editor = {Wang, Qing and Pfahl, Dietmar and Raffo, David},
  title = {Making Globally Distributed Software Development a Success Story - International Conference on Software Process, ICSP 2008, Leipzig, Germany, May 10-11, 2008, Proceedings},
  year = {2008},
  abstract = {},
  publisher = {Berlin-Heidelberg: Springer-Verlag },
  isbn = {978-3-540-79587-2}
}

@Mastersthesis{Simula.SE.289,
  author = {Salicath, Arne Alexander},
  title = {Administrasjon og vurdering av programmeringsbesvarelser: En prototypeimplementasjon av et programvarerammeverk for automatisk og manuell evaluering og sk{\r a}ring av Java programmer},
  year = {2008},
  abstract = {},
  school = {University of Oslo},
  type = {Master's Theis}
}

@Techreport{Simula.SE.297,
  author = {Benestad, Hans Christian and Dahlum Anda, Bente Cecilie and Arisholm, Erik},
  title = {A systematic review of empirical software engineering studies that analyze individual changes},
  year = {2008},
  abstract = {},
  institution = {Simula Research Laboratory},
  number = {2008-05}
}

@Article{Simula.SE.309,
  author = {Hannay, Jo Erskine and Dyb{\r a}, Tore and Arisholm, Erik and Sj{\o}berg, Dag},
  title = {The Effectiveness of Pair-Programming: A Meta-Analysis},
  year = {2008},
  abstract = {The meta-analysis provided in this article shows a small significant positive overall effect of pair programming (PP) compared with solo programming on quality, a medium  significant positive overall effect on duration, and a medium  significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on PP. A more detailed examination of the evidence suggests that PP is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. However, the higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that  greater attention should be given to moderating factors on the effect of PP.},
  journal = {Information and Software Technology}
}

@Phdthesis{Simula.ND.231,
  author = {Sem-Jacobsen, Frank Olaf},
  title = {Towards a Unified Interconnect Architecture: Combining Dynamic Fault Tolerance with Quality of Service, Community Separation, and Power Saving},
  year = {2008},
  abstract = {High-performance computing has, for a decade, been synonymous with parallel computer systems.  Whereas parallel systems initially were based on shared memory processing, all current high-performance systems are based on massively parallel processing, utilising a large number of loosely coupled processing units.  Any parallel processing relies on a degree of communication, so the network interconnecting the processing units in the computer system has a significant impact on the efficiency of the system.  Interconnection networks consist of a large number of switches and links to support the possible communication demands, and so the probability of some part of the system failing has to be considered.  Steps must be taken to guarantee that all sources and destinations will be allowed to continue communication even with some failed elements, the system must be fault tolerant.  As elements fail, communication patterns in the network will change and affect the quality of service experienced by all traffic in the system.  Furthermore, in many cases, single applications need not occupy the entire computer system to perform its calculations.  To increase system efficiency, several such applications may be run in parallel on the same system, or parts of the system may be shut down when it is underutilised to save power and cut expenses. When several applications are sharing the same system, there should be a minimal degree of interaction between them.  This requires separation of communities and routing containment to guarantee that separate applications do not share network resources, and in the case where this is unavoidable, quality of service must be enforced to provide reliable guarantees to the applications.    All these fields have received attention from the academic world, however, most proposed solutions for one problem are not easilly combined with solutions for the other problems. In this thesis, we develop a number of solutions for the different problems and attempt to combine these into a unified architecture.  For fault tolerance we develop a number of algorithms based on re-routing locally around failed elements.  We considered both specific fat-tree topologies, and a topology agnostic solution. For the fat-tree topology we are able to tolerate $\frac{switch\\_ports}{2} -1$ link or switch faults dynamically with very low response times.    We then proceed to evaluate how these mechanisms affect quality of service experienced by traffic flows in the network, and propose and evaluate a number of methods to re-prioritise traffic and maintain quality-of-service guarantees.  We also develop a multipath routing scheme for fat-trees.  By carefully selecting which path is utilised, we can achieve fault tolerance, separation communities, and power saving.  Finally, we describe how a number of the proposed methods can be combined into a unified network architecture that addresses all the challenges we have stated.},
  school = {University of Oslo},
  type = {Ph.D. thesis},
  isbn = {1501-7710}
}

@Inproceedings{Simula.ND.123,
  author = {Xin, Qin and Zhang, Yan and Xiang, Jie},
  title = {Optimal Spectrum Scheduling in Cognitive Wireless Mesh Networks},
  year = {2008},
  abstract = {In a Cognitive Wireless Mesh Network (CogMesh), each Cognitive Radio (CR) enabled mesh node can sense and opportunistically access the under-utilized spectrums in the primary system. Since the unoccupied spectrum is location-dependent and time-dependent, the available spectrums in each mesh node are different. With this spectrum heterogeneity, each node shall have the knowledge of spectrum, scheduling, and routing path of other nodes such that it cacommunicate with them with minimal cost and no collision. The problem may occur in a number of the situations, e.g. the routing table re-building and the unicast real-time applications.  In this paper, we focus on the 2-hop spectrum scheduling, which will enable any pair of 2-hop neighborhood nodes know the spectrum, collision-free scheduling, and minimal-cost routing path. Two optimal strategies are proposed to address the spectrum scheduling: Optimal Deterministic Spectrum Scheduling (ODS) and Optimal Randomized Spectrum Scheduling (ORS). In the deterministic situation, with the scheme ODS, the 2-hop spectrum scheduling can be solved in polynomial time. In the randomized situation, with the scheme ORS, the 2-hop spectrum scheduling can be solved in polynomial time with high probability at least $(1-\frac{1}{n})$ where $n$ denotes the network size. The simulation experiment is carried out to show the achievable performance of the proposed algorithms.},
  booktitle = {Proceedings of International Wireless Communications and Mobile Computing Conference 2008 (IWCMC'08)}
}

@Article{Simula.SE.235,
  author = {J{\o}rgensen, Magne},
  title = {Selection of Effort Estimation Strategies},
  year = {2008},
  abstract = {We currently know little about the factors that motivate the selection and change of estimation strategy in judgment-based effort estimation context. A better understanding of these issues may lead to more accurate judgment-based effort estimates and motivates the four experiments reported in this paper. The experiments{\textquoteright} two main results are the identification of the importance of {\textquotedblleft}estimation surprises{\textquotedblright} (large estimation errors) to motivate estimation strategy change and the large individual variation in the initial choice of estimation strategy. The individual variation seems not only to be a result of differences in previous experiences, but also a result of differences in the mental {\textquotedblleft}accessibility{\textquotedblright} of the strategies. We found, for example, that the use of a strategy was increased when we instructed a developer to use the same type of strategy on unrelated tasks immediately before. The laboratory contexts of the studies means that the results should be interpreted as a first step towards more knowledge about expert estimation strategies and that there is a strong need for more studies, preferably in field situations, before recommending actions on the basis of the findings.},
  journal = {IEEE Transactions of Software Engineering}
}

@Inproceedings{Simula.SE.279,
  author = {Shousha, Marwa and Briand, Lionel and Labiche, Yvan},
  title = {A UML/SPT Model Analysis Methodology for Concurrent Systems Based on Genetic Algorithms},
  year = {2008},
  abstract = {},
  booktitle = {ACM/IEEE 11th International Conference in Model Driven Engineering Languages and Systems (MODELS 2008)}
}

@Inproceedings{Simula.ND.187,
  author = {Gran, Ernst Gunnar and Reinemo, Sven-Arne},
  title = {Dragon Kill Points: Loot Distribution in Massive Multiplayer Online Role Playing Games},
  year = {2008},
  abstract = {One of the major reasons for playing Massive Multiplayer Online Role Playing Games (MMORPGs) is the possibility to show off your abilities to other players. The more rare your equipment is, the higher is the show off value of your character. And because rare items are hard to find cooperation between several players is often required. This introduces a conflict between the players, and a way to distribute loot is necessary.   We introduce the problem of loot distribution in MMORPG, and we suggest and give a preliminary evaluation of a new and improved Dragon Kill Points system.},
  booktitle = {Proceedings of the 7th ACM SIGCOMM workshop on Network and system support for games (NetGames)}
}

@Article{Simula.SE.121,
  author = {Bj{\o}rnson, Finn Olav and Wang, Alf Inge and Arisholm, Erik},
  title = {Improving the effectiveness of root cause analysis in post mortem analysis: A controlled experiment },
  year = {2008},
  abstract = {Retrospective analysis is a way to share knowledge following the completion of a project or major milestone. However, in the busy workday of a software project, there is rarely time for such reviews and there is a need for effective methods that will yield good results quickly without the need for external consultants or experts. Building on an existing method for retrospective analysis and theories of group involvement, we propose improvements to the root cause analysis phase of a lightweight retrospective analysis method known as post mortem analysis (PMA). In particular, to facilitate brainstorming during the root cause analysis phase of the PMA, we propose certain processual changes to facilitate more active individual participation and the use of less rigidly structured diagrams. We conducted a controlled experiment to compare this new variation of the method with the existing one, and conclude that in our setting of small software teams with no access to an experienced facilitator, the new variation is more effective when it comes to identifying possible root causes of problems and successes. The modified method also produced more specific starting points for improving the software development process.},
  journal = {Information and Software Technology}
}

@Inproceedings{Simula.ND.40,
  author = {Gr{\o}nsund, P{\r a}l and Engelstad, Paal Einar and Ayoun, Moti and Skeie, Tor},
  title = {Real Life Field Trial over a pre-mobile WiMAX System with 4th Order Diversity},
  year = {2007},
  abstract = {Mobile WiMAX is a promising wireless technology approaching market deployment. Much discussion concentrate on whether mobile WiMAX will reach a tipping point and become 4G or not. As a pre-mobile WiMAX system is being delivered, we decided to set up a real life field trial and perform the most important measurements over the system setup. The system was delivered with 4th order diversity. In this paper we analyze physical system performance based on field trial measurements, especially at locations with non line of sight conditions in urban areas. We investigate the gain with 2nd and 4th order base station diversity and derive analytical expressions. The system path loss is plotted and found to approach the Cost-231 Hata model for urban areas. Throughput is also measured and analyzed. Sub-channelization in the uplink points out to be an important feature for enhanced coverage.},
  booktitle = {The 7th International Conference on Next Generation Teletraffic and Wired/Wireless Advanced Networking (NEW2AN)},
  editor = {Yevgeni Koucheryavy},
  pages = {121-132},
  publisher = {LNCS Springer},
  series = {Lecture Notes in Computer Science},
  isbn = {978-3-54074-832-8}
}

@Inproceedings{Simula.SE.123,
  author = {Anda, Bente Cecilie Dahlum},
  title = {Assessing Software System Maintainability using Structural Measures and Expert Assessments},
  year = {2007},
  abstract = {A software client usually has few means of assessing the maintainability of a software system in the acquisition process. Software maintenance is often expensive, hence, strategies for assessing the maintainability of complete software systems are important. Assessing the maintainability of complete systems is difficult due to the influence of many different factors such as the people, tasks and tools, in addition to the code. Most research on maintainability focuses on individual classes of individual systems. 
This paper describes an empirical study where the maintainability of four functionally equivalent systems was assessed using both structural measures and expert assessments. The results suggest that such a combination may be useful. Although, the assessment based on structural measures mostly corresponded with the expert assessments, there were several examples of potential maintainability problems that were not captured by the structural measures. 
},
  booktitle = {the 23rd International Conference on Software Maintenance (ICSM 2007, the paper received Best Paper Award)},
  editor = {Gerardo Canfora and Ladan Tahvildari},
  pages = {204-213},
  publisher = {IEEE},
  isbn = {1-4244-1256-0}
}

@Phdthesis{Simula.ND.53,
  author = {Reinemo, Sven-Arne},
  title = {Quality of Service in Interconnection Networks},
  year = {2007},
  abstract = {Interconnection networks were traditionally confined to multiprocessors where low latency and high bandwidth were necessary for interprocessor communication. But in the last decade interconnection networks have become crucial in other application areas such as storage area networks and high performance computing clusters. This development has led to an increased interest in supporting quality of service in interconnection networks driven by the wish to converge cluster storage, cluster communication, and cluster management in one single network. In this thesis we propose and study a set of mechanisms to achieve this in existing interconnection technologies.  We introduce two new topology agnostic routing algorithms, a service differentiation scheme for the InfiniBand Architecture, and several admission control schemes.
},
  school = {Department of Informatics, Faculty of Mathematics and Natural Sciences, University of Oslo}
}

@Inproceedings{Simula.ND.43,
  author = {Birkedal, Erlend and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Implementation and Evaluation of Late Data Choice for TCP in Linux},
  year = {2007},
  abstract = {Real-time delivery of time-dependent data over the Internet is challenging. UDP
has often been used to transport data in a timely manner, but its lack of
congestion control is often criticized. This criticism is a reason that the vast
majority of applications today use TCP. The downside of this is that TCP has
problems with the timely delivery of data. A transport protocol that adds
congestion control to an otherwise UDP-like behaviour is DCCP. For this protocol
,
{\it late data choice} (LDC)\~\cite{lai++2005} has been proposed to allow
adaptive applications control over data packets up to the actual transmission
time. We find, however, that application developers appreciate other TCP
features as well, such as its reliability. We have therefore implemented and
tested the LDC ideas for TCP. It allows the application to modify or drop
packets that have been handed to TCP until they are
actually transmitted to the network. This is achieved with a shared
packet ring and indexes to hold the current status. Our experiments show that we
can send more useful data with LDC than without in a streaming scenario. We can
therefore claim that we achieve a better utilization of the throughput, giving u
s a
higher \textit{goodput} with LDC than without.},
  booktitle = { IEEE International Symposium on Multimedia (ISM2007)},
  editor = { ?},
  pages = {221 -- 228},
  publisher = { IEEE Computer Society},
  address = {10662 Los Vaqueros Circle, P.O. Box 3014, Los Alamitos, CA 90720-1314},
  isbn = {ISBN 0-7695-3058-3}
}

@Article{Simula.SE.159,
  author = {J{\o}rgensen, Magne},
  title = {How to Avoid Selecting Providers with Bids Based on Over-Optimistic Cost Estimates},
  year = {2009},
  abstract = {It is well known that software development companies tend to produce over-optimistic cost estimates and that this over-optimism may lead to delivery problems for the clients as well as the providers. In this paper we summarize evidence suggesting that the clients can reduce the likelihood of selecting providers with bids based on over-optimistic cost estimates through their control of the bidding processes. Important means for this purpose include: avoid inviting many bidders when price is an important criterion for selection; avoid using price as an important criterion for selection when the ability to assess provider competence is low; apply bidding processes that ensure that the provider understands the complexity of the project; avoid budget or price expectation related information in the bidding material and avoid a negotiation process in which you ask for bid updates on reduced functionality. The evidence presented in this paper can also be used by software providers to identify bidding rounds where it is likely that they win only when strongly over-optimistic about the cost.},
  journal = {IEEE Software (May/June)}
}

@Inproceedings{Simula.ND.44,
  author = {Lundesgaard, Sten Amundsen and Solberg, Arnor and Oldevik, Jon and France, Robert and Aagedal, Jan {\O}yvind and Eliassen, Frank},
  title = {Construction of and Execution of Adaptable Applications using an Aspect-Oriented and Model Driven Approach},
  year = {2007},
  abstract = {Constructing and executing distributed applications that can adapt to their current operating context, in order to maintain or enhance Quality of Service (QoS) attribute levels, are complex tasks. Managing multiple, interacting QoS features is particularly difficult since these features tend to be distributed across the system and tangled with other features. The crosscutting nature of QoS features can make them difficult to evolve, and it can make it complicated to dynamically optimize with respect to provided QoS during execution. Furthermore, it complicates efficient construction of application variants that differ in their QoS characteristics to suit various execution contexts. This paper presents an aspect-oriented and model driven approach for constructing and a QoS-aware middleware for execution of QoS-sensitive applications. Aspect-oriented modeling techniques are used to separate QoS features from primary application logic, and for efficient specification of alternative application variants. Model driven engineering techniques are used to derive run-time representations of application variants from platform independent models. The developed middleware chooses the best variant according to the current operating context and the available resources. 
},
  booktitle = {Distributed Applications and Interoperable Systems (DAIS)},
  editor = {Jadwiga Indulska, Kerry Raymond},
  pages = {76--89},
  publisher = {Spinger Verlag },
  address = {Berlin Heidelberg New York},
  series = {LNCS 4531},
  isbn = {3-540-72881-3}
}

@Book{Simula.ND.47,
  editor = {Zhang, Y and Chen, H},
  title = {Mobile WiMAX: Toward Broadband Wireless Metropolitan Area Networks},
  year = {2007},
  abstract = {},
  publisher = {Auerbach Publications, Taylor\&Francis Group},
  address = {New York},
  edition = {first},
  isbn = {9780849326240}
}

@Inproceedings{Simula.SC.22,
  author = {Aln{\ae}s, Martin Sandve and Mardal, Kent-Andre and Sundnes, Joakim},
  title = {Application of symbolic finite element tools to nonlinear hyperelasticity},
  year = {2007},
  abstract = {The present paper addresses the use of high level languages, symbolic
mathematical tools and code generation in an implementation of the finite
element method, using a nonlinear hyperelasticity equation as example.
Advantages of the software development method that will be demonstrated
include closeness to the mathematics, enabling high human efficiency with
easy to use high level languages, while still keeping a high computational
efficiency by generating tailored inner loop code for the problem at hand.
The application we have in mind for the equations presented here is the
simulation of the passive elastic properties of heart and blood vessel
tissue.},
  booktitle = {Fourth national conference on Computational Mechanics (MekIT'07)},
  editor = {B. Skallerud and H.I. Andersson},
  pages = {87-101},
  publisher = {Tapir Academic Press},
  address = {NO-7005 Trondheim},
  isbn = {978-82-519-2235-7}
}

@Misc{Simula.SC.23,
  author = {Aln{\ae}s, Martin Sandve and Mardal, Kent-Andre and Sundnes, Joakim},
  title = {Application of symbolic finite element tools to nonlinear hyperelasticity},
  year = {2007},
  abstract = {},
  howpublished = {Talk at MekIT'07}
}

@Inproceedings{Simula.ND.30,
  author = {Gr{\o}nsund, P{\r a}l and Engelstad, Paal Einar and Johnsen, Torbj{\o}rn and Skeie, Tor},
  title = {The Physical Performance and Path Loss in a Fixed WiMAX Deployment},
  year = {2007},
  abstract = {Fixed WiMAX is being deployed worldwide, and the networks are increasing in size. Measurements have been performed, but the amount of measurements are few and do therefore not demonstrate performance in a real life deployment. We have performed extensive analyses of the physical performance in a fixed WiMAX deployment which has been operative for a year and where the amount of subscribers constantly increases. The analyses presented in this paper focus on received signal strength and signal to noise ratio. Based on the measured parameters, we present a Path Loss model for fixed WiMAX which will hopefully be of great reference value due to the great amount of measurements presented. Finally, our Path Loss model is compared to other well known Path Loss models and is found to approach the free space loss model.},
  booktitle = {International Wireless Communications and Mobile Computing Conference },
  editor = {Mohsen Guizani and Hsiao-Hwa Chen},
  pages = {439 - 444},
  publisher = {ACM Press},
  isbn = {978-1-59593-695-0}
}

@Inproceedings{Simula.ND.42,
  author = {G{\a\'o}mez, Antonio Robles and Berm{\a\'u}dez, Aurelio and Casado, Rafael and Quiles, Francisco Jose and Skeie, Tor},
  title = {A Distributed Approach to Handle Topological Changes in Advanced Switching},
  year = {2007},
  abstract = {Advanced Switching Interconnect (ASI) is a new high-performance serial interconnect. In order to support high availability, its specification establishes a management infrastructure, which is in charge of maintaining the network operation after the occurrence of a topological change. During the change assimilation process, a management mechanism must discover the new topology, obtain a set of paths, and finally distribute them to the fabric endpoints. Recently, an implementation for this mechanism has been proposed, in which a management entity performs all the previous tasks in a centralized way. In this paper, we propose a distributed solution for computing the new set of paths, with the aim of reducing the negative impact on the network service of the centralized proposal.},
  booktitle = {2nd ACM International Workshop on Performance Monitoring, Measurement, and Evaluation of Heterogeneous Wireless and Wired Networks},
  editor = {M. OULD-KHAOUA},
  pages = {37-44},
  publisher = {ACM Press},
  isbn = {978-1-59593-805-3}
}

@Techreport{Simula.SE.161,
  author = {Dzidek, James and Arisholm, Erik and Briand, Lionel Claude},
  title = {A Realistic Empirical Evaluation of the Costs and Benefits of UML in Software Maintenance},
  year = {2007},
  abstract = {The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54\% increase in the functional correctness of changes (p=0.03), an insignificant 7\% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56\%) - at the expense of an insignificant 14\% increase in development time caused by the overhead of updating the UML documentation (p=0.35). },
  institution = {Simula Research Laboratory},
  type = {Technical Report},
  number = {2007 - 04},
  address = {Oslo, Norway}
}

@Article{Simula.SE.162,
  author = {Dyb{\r a}, Tore and Arisholm, Erik and Sj{\o}berg, Dag and Hannay, Jo Erskine and Shull, Forrest},
  title = {Are Two Heads Better than One? On the Effectiveness of Pair Programming},
  year = {2007},
  abstract = {},
  journal = {IEEE Software},
  volume = {24},
  number = {6},
  pages = {12-15}
}

@Misc{Simula.SE.151,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {How much does feedback and performance review improve software development effort estimation? An Empirical Study (extended abstract)},
  year = {2006},
  abstract = {Over-optimistic and over-confident software development effort estimates are more the rule than the exception. Several software improvement process frameworks, e.g., the Capability Maturity Model, are based on the assumption that improved feedback and use of performance reviews leads to higher degree of realism and better project performance. This paper investigates that assumptions, i.e., whether effort estimation and uncertainty assessment skills improve with better feedback processes and mandatory estimation performance reviews.

We recruited 20 professional software developers with the necessary technological experience and skill to complete the same 5 software development tasks on an in-use web-system. They were paid standard industry wage corresponding to their level of experience and expertise. Work conditions were like those in industry. Ten of the participants received outcome feedback on their estimation performance and followed our estimation performance review instructions, the other ten acted as a control group with no performance review instructions and only on-the-job feedback. 

We found no or only minor differences in performance between the treatment groups and conclude that the feedback and performance reviews did not lead to improvement of estimation or uncertainty assessment skill. Possible reasons for this surprising finding include: a) Low motivation for learning (A high performance on effort estimation and uncertainty assessment may have been perceived as a much less important goal than being perceived as a skilled programmer through low effort estimates and high confidence.), and, b) The software developers who were instructed to learn from previous performance may have believed that they have learned more than they actually did.

In a follow-up experiment 81 software professionals assessed the uncertainty of the effort estimates provided by the developers in the original experiment. We found that the software professionals assessing the uncertainty of other developers{\textquoteright} effort estimates achieved a much higher realism and better learning from history.
},
  howpublished = {In International Symposium on Forecasting, edited by Antonio Garc{\a\'\i}a-Ferrer, Santander, 12-14 June 2006. International Institute of Forecasters, pages 103}
}

@Manual{Simula.SC.32,
  author = {Aln{\ae}s, Martin Sandve and Logg, Anders and Mardal, Kent-Andre and Skavhaug, Ola and Langtangen, Hans Petter},
  title = {UFC Specification and User Manual 1.0},
  year = {2007},
  abstract = {},
  edition = {1.0},
  note = {http://www.fenics.org/ufc/}
}

@Inproceedings{Simula.ND.39,
  author = {Elmokashfi, Ahmed and kleis, Michael and Popescu, Adrian},
  title = {NetForecast: A Delay Prediction Scheme for Provider Controlled Networks},
  year = {2007},
  abstract = {Over the last years, the Internet has evolved towards becoming the dominant platform for deploying real time and multimedia services. This evolution has had as a consequence that the selection of an appropriate server, proxy or super node with reference to some specific QoS parameter becomes of paramount importance. We consider in our paper the specific case of delay estimation. An investigation of existing approaches for the estimation and prediction of network delay is provided. Based on that, we further suggest NetForecast as a way to overcome limitations of existing prediction methods. NetForecast is an algorithm for delay prediction in provider controlled networks. The algorithm is based on a combination of landmark-based distance estimation, clustering and a triangulation principle. The paper reports on preliminary performance of NetForecast, as provided by a simulation study. Our results show the feasibility of the suggested method. },
  booktitle = { IEEE Globecom 2007},
  editor = {IEEE},
  publisher = {IEEE},
  isbn = {N/A}
}

@Article{Simula.ND.41,
  author = {Flich, Jos{\a\'e} and Skeie, Tor and Mejia, Andres and Lysne, Olav and L{\a\'o}pez, Pedro and Robles, Antonio and Duato, Jos{\a\'e} and Koibuchi, Michihiro and Rokicki, Tomas and Sancho, Jos{\a\'e}},
  title = {A Survey and Evaluation of Topology Agnostic Routing Algorithms},
  year = {2008},
  abstract = {Most standard interconnect technologies that have emerged over the last decade, mainly for clusters,
are flexible with respect to network topology. This has spawned a substantial amount of research on
topology agnostic routing algorithms, and some of the developed algorithms are widely deployed. These
algorithms make no assumption about the structure of the network, thus they provide the flexibility
needed to route on a network in the presence of faulty components.

On the other hand, the advent of massive multi-core chips challenges the state of the art in
interconnection networks. The topology of choice for point to point Networks on-chip (NoC) is the
two dimensional mesh which is well understood, but the need of high yield, out of a production line for
such chips requires that they should be functional even if some components suffer from manufacturing
defects. Such defects will turn the regular topology into a slightly irregular one, making topology
dependent routing algorithms inapplicable. Also, heterogeneous multi-core chips, where different cores
could have different sizes and shapes, naturally lead to (slightly) irregular topologies.

Therefore, topology agnostic routing algorithms proposed for clusters find a new environment to be
applied since they provide the flexibility needed to route on a NoC in the presence of faulty or missing
components. Moreover, in some cases, they have been shown to perform comparably to special purpose
algorithms for regular topologies.

Unfortunately, the existing topology agnostic routing algorithms have been developed for varying
purposes, giving them different and not always comparable properties. Furthermore, the information on
these routing algorithms is scattered on different papers where they have been evaluated under different
and not comparable conditions.

In this paper, we present a comprehensive overview of the topology agnostic routing algorithms that
have been proposed to date. The algorithms are classified with respect to their most important properties,
and their performance under equal, but varying conditions is analyzed. Thereby, we provide significant
new insight into the properties of the different routing algorithms, and give substantial support for, given
a set of requirements, finding the most suitable both for the on and off-chip environments.},
  journal = {ACM Computing Surveys}
}

@Techreport{Simula.SC.25,
  author = {L{\o}vgren, Alf Emil},
  title = {On the implementation of 'pseudo-harmonic' extension.},
  year = {2007},
  abstract = {  The {\textquoteright}pseudo-harmonic{\textquoteright} extension is an approximation to the common harmonic extension for extending a function over a domain based on its trace along
the boundary of the domain. On a circle the two extension methods produce
identical results. We present explicit formulas for the computation of distance
functions and intersection points needed in the {\textquoteright}pseudo-harmonic{\textquoteright} extension on
a circle, a square, and a pentagon. While the harmonic extension needs the
solution of a Laplace problem for each new boundary function, the {\textquoteright}pseudo-harmonic{\textquoteright} extension can reuse the distance functions and intersection points
for any piecewise continuous function defined on the boundary of the domain.
},
  institution = {The Norwegian University of Science and Technology},
  type = {Preprint Series: Numerics},
  number = {2/07},
  address = {7491 Trondheim}
}

@Inproceedings{Simula.SE.157,
  author = {Torres-Rojas, Francisco J and Fallas Yamashita, Aiko and N{\a\'u}{\\~n}ez Corrales, Santiago},
  title = {Is Action Research the Path to a Solid Research Culture in IS/SE for Costa Rica?},
  year = {2007},
  abstract = {In Latin America there is a visible evidence of lack of research initiatives and a strong disassociation between Academia and Industry in the areas of Information Systems and Software Engineering. Despite the potential of Latin America, such situations hinder the opportunity to build up the required knowledge in order to participate competitively in the international market, holding back the possibility of a technology-based economy in these countries. We propose action research as a flexible and plausible research schema for bridging the gaps between Academia and Industry and at the same time promoting a research culture in Latin America (more specifically in the Costa Rican context), considering the current needs of the Industry and the current directions that have been taken by the Academia for performing research in Information Systems and Software Engineering.},
  booktitle = {33rd Latin American Conference on Informatics},
  editor = {Marcelo Jenkins and Manuel Berm{\a\'u}dez},
  publisher = {CLEI},
  isbn = {ISBN 978-9968-9678-9-1}
}

@Inproceedings{Simula.ND.51,
  author = {Beskow, Paul and Halvorsen, P{\r a}l and Griwodz, Carsten},
  title = {Latency Reduction in Massively Multi-player Online Games by Partial Migration of Game State},
  year = {2007},
  abstract = {With the increasing popularity of massively multi-player online games
(MMOGs), developers are continually forced to deal with the
conflicting requirements of supporting a large number of concurrent
users, while simultaneously providing low latency. As a result, a
common way of distributing load is by dividing the virtual environment
into logical regions. Geographically coupled users in such regions can
be distinguished by analyzing IP addresses, RTTs or similar. This
paper proposes the architecture for a decentralized middleware capable
of utilizing such information, with the intent of decreasing the
overall latency for the majority of users in that region. The latency
reduction is accomplished by migrating a game region to a server
closer in locality to the users, thereby lowering the response time of
remote procedure calls. Due to the characteristics of MMOGs, the
middleware implements a distributed name service, made possible by
activating the system from a single node in the system.},
  booktitle = {Second International Conference on Internet Technologies and Applications},
  editor = {Vic Grout, Denise Oram and Rich Picking},
  pages = {153--163},
  publisher = {Centre for Applied Internet Research (CAIR)},
  address = {University of Wales, NEWI, Wrexham, UK.},
  isbn = {978-0-946881-54-3}
}

@Article{Simula.SE.160,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {The Impact of Irrelevant Information on Estimates of Software Development Effort},
  year = {2008},
  abstract = {Software professionals typically estimate software development effort on the basis of a requirement specification. Parts of this specification frequently contain information that is irrelevant to the estimation of the actual effort involved in the development of software. We hypothesize that effort-irrelevant information sometimes has a strong impact on effort estimates. To test this hypothesis, we conducted three controlled experiments with software professionals. In each of the experiments, the software professionals received specifications describing the same requirements. However, we gave one group of the software professionals a version of the requirement specification where we had included additional, effort-irrelevant, information. In all three experiments, we observed that the estimates of most likely effort increased when the estimates were based on requirement specifications that contained the information irrelevant to development effort. The results suggest that when estimation-irrelevant information is included as input to expert judgment-based estimation processes, the estimators find it difficult to ignore it. The results also show that it is difficult to predict the impact of estimation-irrelevant information and that software professionals seem to be unaware of the impact. One possible (and advisable) course of action, given our findings, would be to remove estimation-irrelevant information from the requirement specification prior to the use of it as input to estimation work.},
  journal = {JSS}
}

@Inproceedings{Simula.SE.141,
  author = {Anda, Bente Cecilie Dahlum},
  title = {Assessment of Software System Evolvability},
  year = {2007},
  abstract = {The evolvability, the ease of further development, of a software systems is difficult to assess, but may have large economic consequences. Many studies have investigated the relations between particular software metrics and effort on evolving individual classes, but little attention has been given to methods for assessing and measuring evolvability of complete software systems. This paper discusses such methods, and motivates that they should use a combination of structural code measures and expert assessments. This is exemplified in a case study assessing the evolvability of four functionally equivalent systems. The paper also gives with directions for future work on evolvability assessments.},
  booktitle = {International Workshop on Principles of Software Evolution (IWPSE 2007)},
  editor = {Massimiliano di Penta and Michele Lanza},
  pages = {71--74},
  publisher = {ACM},
  isbn = {978-1-59593-811-4}
}

@Article{Simula.SE.150,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and Haugen, Nils Christian and Benestad, Hans Christian},
  title = {Using Planning Poker for Combining Expert Estimates in Software Projects },
  year = {2008},
  abstract = {Combination of expert opinion is frequently used to produce estimates in software projects. However, whether, when, and how to combine expert estimates is poorly understood. In order to study the effects of a combination technique called planning poker, the technique was introduced in a software project for half of the tasks. The tasks estimated with planning poker provided: 1) group consensus estimates that were less optimistic than the statistical combination (mean) of individual estimates for the same tasks, and 2) group consensus estimates that were more accurate than the statistical combination of individual estimates for the same tasks. The set of control tasks in the same project, estimated by individual experts, achieved similar estimation accuracy as the planning poker tasks. Moreover, for both planning poker and the control group, measures of the median estimation bias indicated that both groups had unbiased estimates, because the typical estimated task was perfectly on target. A code analysis revealed that for tasks estimated with planning poker, more effort was expended due to the complexity of the changes to be made, possibly triggered by the group discussions.},
  journal = {Journal of Systems and Software }
}

@Misc{Simula.SC.30,
  author = {Lysaker, Ola Marius and Nielsen, Bj{\o}rn Fredrik},
  title = {PDE methods for identifying ischemic heart disease},
  year = {2007},
  abstract = {We explore the possibilities for using high speed laptops, mathematics and
biological knowledge to construct a new medical imaging device. The source
of the electrical potentials measured in ECG recordings at the body surface
is the electrical pulses generated by the myocardium (heart). Our goal is to
use ECG data and a geometrical model of the human body to compute an image
of the potential distribution inside the heart wall. Since many heart
conditions lead to changes in the potential pattern, such a device could be
very useful for the physician. 

 },
  howpublished = {Presented at the 6th International Congress on Industrial and Applied Mathematics}
}

@Misc{Simula.SE.152,
  author = {J{\o}rgensen, Magne},
  title = {How to get a low price on your next software development project and why you should avoid it},
  year = {2007},
  abstract = {Software providers are well known for over-optimistic forecasts of how much the development of software systems will cost. We claim that the software clients, to a far greater degree than is commonly believed, are responsible for the providers{\textquoteright} over-optimism. Our claim is based on empirical studies of how client controlled factors impact the level of optimism in providers{\textquoteright} software development cost forecasts. The studies suggest that forecasting over-optimism is related to clients who communicate unrealistic price expectations or present attractive future opportunities related to winning the bidding round systematically. The studies also suggest that there are commonly used formats of software project bidding processes that strongly increase the risk of selecting providers with prices based on over-optimistic cost forecasts. We find, for example, that bidding processes with negotiation tend to increase the level of over-optimism when the negotiation involves asking for price updates on a reduced version of the initial software requirements. Evidence suggests that the clients may not benefit from a low price based on an over-optimistic cost forecast. One reason for this, applying concepts from the principal-agency theory, is the high level of information asymmetry in most software development project. As an illustration, the clients{\textquoteright} poor ability to specify and monitor software quality attributes, such as the maintainability of the software, makes it possible for the providers to deliver software with poorer than expected quality. We observe that social factors that otherwise may be sufficient to avoid opportunistic behaviour by the clients, e.g., the software developers{\textquoteright} self-imposed code of ethics, are less important in situations with over-optimistic effort estimates, e.g., in situations where the provider tries to avoid financial losses. Based on the above findings we outline elements of how the bidding processes should be designed to reduce the probability of receiving bids based on over-optimistic forecasts.},
  howpublished = {International Symposium on Forecasting}
}

@Inbook{Simula.SC.33,
  author = {Logg, Anders and Mardal, Kent-Andre and Aln{\ae}s, Martin Sandve and Langtangen, Hans Petter and Skavhaug, Ola},
  title = {A Hybrid Approach to Efficient Finite Element Code Development},
  year = {2007},
  abstract = {},
  booktitle = {Petascale Computing -- Algorithms and Applications},
  editor = {D. A. Bader},
  publisher = {Chapman and Hall},
  series = {Computational Science},
  chapter = {19},
  isbn = {978-1584889090}
}

@Misc{Simula.SE.177,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Hvordan hyppige leveranser kan hjelpe deg i ukjente prosjektomgivelser - Erfaringer fra en doktorgrad levert under budsjett og f{\o}r deadline},
  year = {2007},
  abstract = {},
  howpublished = {Smidig 2007}
}

@Inproceedings{Simula.ND.71,
  author = {Flich, Jose and Duato, Jose and S{\o}dring, Thomas and Solheim, {\r A}shild Gr{\o}nstad and Skeie, Tor and Lysne, Olav and Rodrigo, Samuel},
  title = {On the Potential of NoC Virtualization for Multicore Chips},
  year = {2008},
  abstract = {As the  end of Moores-law is on the horizon, power becomes a limiting
factor to the continuous increases in performance gains for
single-core processors. Processor engineers have shifted to the
multicore paradigm and many-core processors are reality. Within the
context of these multi-core, three key metrics point themselves out as
being of major importance, performance, fault-tolerance
(including yield), and power consumption.  A solution that
optimizes all three of these metric is challenging. As the number of
cores increases the importance of the interconnection network-on-chip
(NoC) grows as well, and chip designers should aim to optimize these
three key metrics.

In this paper we identify and discuss the main properties that a NoC
must exhibit in order to enable such optimizations. In particular, we
propose the use of virtualization techniques at the NoC level. The
implementation of routing algorithms for NoC is a key design parameter
in order to achieve an effective virtualization of the chip that
should also support broadcast within the virtualized context.

The intention behind this paper is for it to serve a position paper on the
topic of virtualization for NoC and the challenges that should be met
at the routing layer in order to maximize performance, fault-tolerance
and power consumption.

},
  booktitle = {International Workshop on Multi-Core Computing Systems (MuCoCoS'08)},
  publisher = {IEEE}
}

@Inproceedings{Simula.ND.91,
  author = {Zhang, L and Soong, B and Zhang, Y and Ma, M and Guan, Y},
  title = {An Analysis of k-Connectivity in Shadowing and Nakagami Fading Wireless Multi-Hop Networks},
  year = {2008},
  abstract = {},
  booktitle = {the IEEE 67th Vehicular Technology Conference (VTC2008-Spring)},
  publisher = {IEEE}
}

@Article{Simula.ND.97,
  author = {Zhang, Y and Hu, H and Chen, H},
  title = {QoS Differentiation for IEEE 802.16 WiMAX Mesh Networking},
  year = {2008},
  abstract = {},
  journal = {ACM/Springer Mobile Networks and Applications (MONET)}
}

@Article{Simula.ND.99,
  author = {Cao, X and Chen, J and Zhang, Y and Sun, Y},
  title = {Development of An Integrated Wireless Sensor Network Micro-Environment Monitoring System},
  year = {2008},
  abstract = {},
  journal = {Elsevier ISA Transactions}
}

@Techreport{Simula.ND.100,
  author = {Robles-Gomez, Antonio and Berm{\a\'u}dez, Aurelio and Casado, Rafael and Solheim, {\r A}shild Gr{\o}nstad},
  title = {Deadlock-Free Dynamic Network Reconfiguration Based on Close Up*/Down* Graphs},
  year = {2008},
  abstract = {},
  institution = {University of Castilla-La Mancha, Spain}
}

@Article{Simula.SC.35,
  author = {Nilssen, Trygve Kastberg and Staff, Gunnar Andreas and Mardal, Kent-Andre},
  title = {Order--optimal preconditioners for fully implicit Runge-Kutta schemes applied to the Bidomain equations},
  year = {2007},
  abstract = {},
  journal = {journal}
}

@Misc{Simula.SE.176,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Lightning-talk: Estimation},
  year = {2007},
  abstract = {},
  howpublished = {XP-meetup}
}

@Misc{Simula.SC.107,
  author = {Hanslien, Monica and Sundnes, Joakim and Lines, Glenn Terje and Tveito, Aslak},
  title = {Simulating electrical activity in the heart},
  year = {2006},
  abstract = {},
  howpublished = {Presented at St Jude Medical, Stockholm, Sweeden}
}

@Inproceedings{Simula.SC.36,
  author = {Bredesen, Rolv Erlend and Langtangen, Hans Petter and Pedersen, Geir},
  title = {Benchmark of a Tsunami Run-Up Code},
  year = {2007},
  abstract = {},
  booktitle = {MekIT'07},
  editor = {B. Skallerud and H. I. Anderson },
  pages = {127--134},
  publisher = {Tapir Academic Press 2007},
  isbn = {978-82-519-2235-7}
}

@Article{Simula.ND.70,
  author = {G{\a\'o}mez, Antonio Robles and Berm{\a\'u}dez, Aurelio and Casado, Rafael and Quiles, Francisco Jos{\a\'e} and Skeie, Tor and Duato, Jos{\a\'e}},
  title = {A Proposal for Managing ASI Fabrics},
  year = {2007},
  abstract = {Recent years, computer performance has been significantly increased. As a con-sequence, data I/O systems have become bottlenecks within systems. To alleviate this problem, Advanced Switching was recently proposed as a new standard for future interconnects. The Advanced Switching specification establishes a fabric man-agement infrastructure, which is in charge of updating the set of fabric paths each time a topological change takes place. The use of source routing and passive switches makes unfeasible the adaptation to this new technology of many existing proposals to handle topological changes in switched interconnection networks. This paper presents a fabric management mechanism for Advanced Switching, but also suitable for other source routing interconnects. Furthermore, the work presents a detailed performance evaluation for this proposal. This evaluation allows us to identify the main drawbacks of the mechanism and to define future improvements.},
  journal = {Journal of System Architecture (Elsevier) }
}

@Inproceedings{Simula.ND.117,
  author = {Espeland, H{\r a}vard and Lunde, Carl Henrik and Stensland, H{\r a}kon Kvale and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {Transparent Protocol Translation and Load Balancing on a Network Processor in a Media Streaming Scenario},
  year = {2008},
  abstract = {Today, major newspapers and TV stations make live and
on-demand audio/video content available, video-on-demand
services are becoming common and even personal media
are frequently uploaded to streaming sites. The discussion
about the best transport protocol for streaming has been
going on for years. Currently, HTTP-streaming is usual although the transport of streaming media data over TCP is
hindered by TCP{\textquoteright}s probing behavior, which results in the
rapid reduction and slow recovery of the packet rates. On
the other hand, UDP has been criticized for being unfair
against TCP, and it is therefore often blocked by access network providers.
To exploit benefits of both TCP and UDP, we have implemented a proxy that performs 
transparent protocol translation in such a way that the video stream is delivered to
clients in a TCP-compatible and TCP-friendly way, but
with UDP-like smoothness. The translation is related to
multicast-to-unicast translation and to voice-over-IP proxies 
that translate between UDP and TCP. Furthermore, it is
also similar to the use of proxy caching that ISPs employ to
reduce bandwidth demands. The unique advantage of our
approach is that we avoid full-featured TCP handling on
the proxy server but still achieve live protocol translation at
line-speed in a TCP-compliant, TCP-friendly manner. Although 
we discard packets just like a sender of non-adaptive
video over TCP, we achieve higher user-perceived quality
because our proxy can avoid receive queue underflows in the
proxy, while also achieving the same average bandwidth as
a TCP connection between proxy and client.
In this demo, we present our prototype implemented on
an Intel IXP2400 network processor. The prototype proxy
does not buffer outgoing packets, yielding data loss in case
of a congested TCP side. Comparing HTTP-streaming from
a web-server and RTP/UDP-streaming from a video server
shows that, in case of some loss, our solution using UDP
from the server and a proxy that translates to TCP delivers
a smoother stream at playout rate while the end-to-end TCP stream oscillates heavily.},
  booktitle = {Network and Operating System Support for Digital Audio and Video (NOSSDAV 2008)},
  publisher = {ACM}
}

@Article{Simula.ND.55,
  author = {Zhang, Y and Xiao, Y and Chen, H},
  title = {Queueing Analysis for OFDM Subcarrier Allocation in Broadband Wireless Multiservice Networks},
  year = {2007},
  abstract = {},
  journal = {IEEE Transactions on Wireless Communications}
}

@Article{Simula.SE.204,
  author = {Al-Emran, Ahmed and Pfahl, Dietmar},
  title = {Performing Operational Release Planning, Replanning and Risk Analysis using a System Dynamics Simulation Model},
  year = {2008},
  abstract = {},
  journal = {Software Process: Improvement and Practice},
  volume = {13},
  number = {3},
  pages = {265-279}
}

@Inproceedings{Simula.SE.207,
  author = {Al-Emran, Ahmed and Kapur, Puneet and Pfahl, Dietmar and Ruhe, Guenther},
  title = {Simulating Worst Case Scenarios and Analyzing their Combined Effect in Operational Release Planning},
  year = {2008},
  abstract = {},
  booktitle = {International Conference on Software Process (ICSP 2008)},
  editor = {Q. Wang, D. Pfahl, D. Raffo},
  pages = {269-281},
  publisher = {Springer},
  series = {LNCS 5007},
  isbn = {978-3-540-79587-2}
}

@Inproceedings{Simula.SE.208,
  author = {Zhang, He and Kitchenham, Barbara and Pfahl, Dietmar},
  title = {Reflections on 10 Years of Software Process Simulation Modeling: A Systematic Review},
  year = {2008},
  abstract = {},
  booktitle = {International Conference on Software Process (ICSP 2008)},
  editor = {Q. Wang, D. Pfahl, D. Raffo},
  pages = {345-356},
  publisher = {Springer},
  series = {LNCS 5007},
  isbn = {978-3-540-79587-2}
}

@Inproceedings{Simula.SE.209,
  author = {Khosrovian, Keyvan and Pfahl, Dietmar and Garousi, Vahid},
  title = {GENSIM 2.0: A Customizable Process Simulation Model for Software Process Evaluation},
  year = {2008},
  abstract = {},
  booktitle = {International Conference on Software Process (ICSP 2008)},
  editor = {Q. Wang, D. Pfahl, D. Raffo},
  pages = {294-306},
  publisher = {Springer},
  series = {LNCS 5007},
  isbn = {978-3-540-79587-2}
}

@Misc{Simula.SC.102,
  author = {Bredesen, Rolv Erlend and Pedersen, Geir and Langtangen, Hans Petter},
  title = {Benchmark of a Tsunami Run-Up Code},
  year = {2007},
  abstract = {},
  howpublished = {Talk at the \emph{Fourth National Conference on Computational Mechanics (MekIT'07)}, Trondheim, Norway},
  note = {Presented by R. E. Bredesen.}
}

@Phdthesis{Simula.SC.103,
  author = {Glimsdal, Sylfest},
  title = {Numerical simulations of tsunamis},
  year = {2007},
  abstract = {},
  school = {Dep. of nformatics, University of Oslo}
}

@Article{Simula.SC.105,
  author = {Nielsen, Bj{\o}rn Fredrik and Mardal, Kent-Andre},
  title = {Efficient preconditioners for optimality systems arising in connection with inverse problems},
  year = {2008},
  abstract = {},
  journal = {journal for publication}
}

@Inproceedings{Simula.ND.67,
  author = {Zhang, Y and Xiang, J},
  title = {A New Adaptive Energy Management Scheme in IEEE 802.16e Mobile WiMAX},
  year = {2007},
  abstract = {},
  booktitle = {Norwegian Informatics Conference 2007 (NIK 2007)},
  editor = {Frode Eika Sndnes},
  pages = {111-114},
  publisher = {Tapir Akademisk Forlag},
  isbn = {978-82-519-2272-2}
}

@Inproceedings{Simula.ND.119,
  author = {Boudko, Svetlana and Leister, Wolfgang and Griwodz, Carsten and Halvorsen, P{\r a}l},
  title = {A Benchmarking System for Multipath Overlay Multimedia Straming},
  year = {2008},
  abstract = {The rapid growth of the Internet multimedia services brings new
challenges to how multimedia streams can be delivered to the users
over bandwidth-constraint networks.  Different strategies
that exploit multipath streaming
in order to provide better utilization of the Internet resources
have been
proposed by the research community.
However, there exists no metric that allows us to evaluate how close
these strategies are to the optimal resource utilization.  
This
paper proposes a static benchmarking system that models the best possible
distribution of streams along multiple paths in an overlay
network that is shared by several senders and receivers.
We have tested it with several different network topologies, and present
the test results in this paper. 

},
  booktitle = {IEEE International Conference on Multimedia \& Expo (ICME)}
}

@Article{Simula.SC.110,
  author = {Aln{\ae}s, Martin Sandve and Mardal, Kent-Andre},
  title = {Symbolic Computations and Code Generation for Finite Element Methods},
  year = {2008},
  abstract = {Efficient and easy implementation of variational forms for finite element discretization can be accomplished with meta-programming. Using a high-level language like Python and symbolic mathematics makes an abstract problem definition possible, but the use of a low-level compiled language is vital for run-time efficiency. By generating low-level C++ code based on symbolic expressions for the discrete weak form, it is possible to accomplish a high degree of abstraction in the problem definition while surpassing the run-time efficiency of traditional hand written C++ codes. We provide several examples where we demonstrate orders of magnitude in speed-up.}
}

@Inproceedings{SE.5.Joergensen.2003,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {A Preliminary Checklist for Software Cost Management},
  year = {2003},
  abstract = {This paper presents a process framework and a preliminary checklist for software cost management.
While most textbooks and research papers on cost estimation look mainly at the {\textquotedblleft}Estimation{\textquotedblright} phase, our
framework and checklist includes the phases relevant to estimation: {\textquotedblleft}Preparation{\textquotedblright}, {\textquotedblleft}Estimation{\textquotedblright},
{\textquotedblleft}Application{\textquotedblright}, and {\textquotedblleft}Learning{\textquotedblright}. We believe that cost estimation processes and checklists should support these
phases to enable high estimation accuracy. The checklist we suggest is based on checklists from a number of
sources, e.g., a handbook in forecasting and checklists present in several Norwegian software companies. It
needs, however, to be extended through feedback from other researchers and software practitioners. There is
also a need for a provision of conditions for meaningful use of the checklist issues and descriptions of the
strength and sources of evidence in favor of the checklist issues. The present version of the checklist should
therefore be seen as preliminary and we want to get feedback from the conference participants and other
readers of this paper for further improvements.},
  booktitle = {IEEE International Conference on Quality                   Software},
  pages = {134-140},
  publisher = {-},
  address = {Dallas, USA}
}

@Inproceedings{SE.5.Joergensen.2003.a,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Situational and Task Characteristics Systematically Associated With Accuracy of Software Development Effort Estimates},
  year = {2003},
  abstract = {},
  booktitle = {Information Resources Management Association                   Conference},
  pages = {824-826},
  publisher = {-},
  address = {Philadelphia, USA}
}

@Inproceedings{SE.5.Joergensen.2003.b,
  author = {J{\o}rgensen, M},
  title = {An Attempt to Model the Software Development Effort Estimation Accuracy and Bias},
  year = {2003},
  abstract = {This paper describes regression models of estimation accuracy and bias, i.e., models aiming at explaining the accuracy and bias variation of an organization{\textquoteright}s software development effort estimates. We collected information about variables we believed would impact the estimation accuracy or bias of tasks completed by the organization. In total, information about 49 software development tasks was collected. We found that the following conditions led to decrease in estimation accuracy: 1) Estimates were provided by a person in the role {\textquoteleft}software developer{\textquoteright} instead of {\textquoteleft}project leader{\textquoteright}, 2) The project had its highest priority on time-to-delivery instead of quality or cost, and, 3) The estimator did not participate in the completion of the task. The following conditions led to increased bias towards under-estimation: 1) Estimates were provided by a person with the role {\textquoteleft}software developer{\textquoteright} instead of {\textquoteleft}project leader{\textquoteright}, and, 2) The estimator assessed the accuracy of own estimates of similar, previously completed tasks to be low (more than 20\% deviation). Although all variables included in the regression models were significant (p<0.1), the explanatory and predictive power of both models were poor, i.e., most of the variance in estimation accuracy and bias was not explained or predicted by our models. An analysis of the estimators{\textquoteright} own descriptions of the reasons for achieved estimation accuracy on each task suggests that it will be difficult to include all important estimation accuracy and bias factors in regression-based models. It is, for this reason, not realistic to expect regression models to replace human judgment in risk analyses and estimation improvement plans. We believe, nevertheless, that such models may be useful to support the human judgment processes.},
  booktitle = {Proceedings of Conference on Empirical                   Assessment in Software Engineering},
  pages = {117-128},
  publisher = {-},
  address = {Keele, England}
}

@Inproceedings{SE.5.Karahasanovic.2003,
  author = {Karahasanovic, A},
  title = {Is it Ethical to Log Users' Actions in Software Engineering Experiments?},
  year = {2003},
  abstract = {In recent years, there has been a significant increase of empirical studies involving human participants in software engineering. Such studies heightened the need for considering ethical issues. Researchers have not only moral but also pragmatic reasons to treat participants of their studies ethically. They want to maintain their access to the data source and to get funding for their research. Thus the researchers have to be aware of general ethical issues and those that are specific for a particular research practice. 
This paper briefly describes our experiences with a tool that logs all actions of the users/participants (commands, keystrokes and mouse movements), and collects their comments in a web based screen. The tool was used in two usability studies and raised ethical issues concerning assuring confidentiality and minimization of inconvenience. The paper also discusses some conflicts between ethical standards and assuring validity of the studies that appeared during these studies.},
  booktitle = {Informing Science and Information Technology                   Education Joint Conference},
  pages = {1211-1214},
  publisher = {The Informing Science Institute, Santa                   Rosa, CA, USA},
  address = {Pori, Finland}
}

@Inproceedings{SE.5.Mohagheghi.2003.a,
  author = {Mohagheghi, P and Nytun, J P and Selo, Mr and Najib, W},
  title = {MDA and Integration of Legacy Systems: An industrial case study},
  year = {2003},
  abstract = {},
  booktitle = {Workshop on Model Driven Architecture:                   Foundations and Applications, MDAFA'03},
  editor = {Arend Rensink},
  pages = {85-90},
  publisher = {-},
  address = {University of Twente, Enschede, The                   Netherlands},
  note = {CTIT Technical Report TR-CT I T-03-27}
}

@Inproceedings{SE.5.Mohagheghi.2003.b,
  author = {Mohagheghi, P and Conradi, R},
  title = {An Industrial Case Study of Product Family Development Using a Component Framework},
  year = {2003},
  abstract = {},
  booktitle = {Sixteenth International Conference on Software                   \& Systems Engineering and their Applications                   (ICSSEA'2003)},
  publisher = {-},
  address = {Paris},
  note = {ISSN: 1637-5033, 6 p.}
}

@Inproceedings{SE.5.Mohagheghi.2003.c,
  author = {Mohagheghi, P and Conradi, R},
  title = {Using Empirical Studies to Assess Software Development Approaches and Measurement Programs},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the ESEIW 2003 Workshop on                   Empirical Software Engineering (WSESE'03) -                   The Future of Empirical Studies in Software                   Engineering},
  pages = {65-76},
  publisher = {-},
  address = {Rome}
}

@Inproceedings{SE.5.Mohagheghi.2003.d,
  author = {Mohagheghi, P and Conradi, R},
  title = {Different Aspects of Product Family Adoption},
  year = {2003},
  abstract = {},
  booktitle = {Proceedings of the 5th International Workshop on                   Product Family Evolution (PFE-5)},
  editor = {Frank van der Linden},
  pages = {459-464},
  publisher = {Springer-Verlag},
  address = {Siena, Italy},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2003,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M},
  title = {Software Effort Estimation: Unstructured Group Discussion as a Method to Reduce Individual Biases},
  year = {2003},
  abstract = {The effort of software projects is often estimated, completely or partially, using expert judgement. This estimation process is subject to biases of the expert responsible. Generally, this bias seems to be towards too optimistic estimates regarding the effort needed to complete the project. The degree of bias varies depending on the expert involved, and seems to be connected to both conscious and unconscious decisions. One possible way to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment where experts with different backgrounds combined their estimates through group discussion. Twenty software professionals were asked to provide individual effort estimates of a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate through discussion and combination of knowledge. We found that the groups were less optimistic in their estimates than the individual experts. Interestingly, the group discussion-based estimates were closer to the effort used by the actual project than the average individual expert, i.e., the group discussions led to better estimates than a mechanical combination of the individual estimates. The groups{\textquoteright} ability to identify more project activities is among the possible explanations for this reduction of bias.},
  booktitle = {The 15th Annual Workshop of the Psychology of                   Programming Interest Group (PPIG 2003)},
  pages = {285-296},
  publisher = {-},
  address = {Keele University, UK}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2003.a,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M},
  title = {A Review of Surveys on Software Effort Estimation},
  year = {2003},
  abstract = {This paper tries to summarize estimation knowledge through a review of surveys on software effort estimation. Main findings were that: (1) Most projects (60-80\%) encounter effort and/or schedule overruns. The overruns, however, seem to be lower than the overruns reported by some consultancy companies. For example, Standish Group{\textquoteright}s {\textquoteleft}Chaos Report{\textquoteright} describes an average cost overrun of 89\%, which is much higher than the average overruns found in other surveys, i.e., 30-40\%. (2) The estimation methods in most frequent use are expert judgment-based. A possible reason for the frequent use of expert judgment is that there is no evidence that formal estimation models lead to more accurate estimates. (3) There is a lack of surveys including extensive analyses of the reasons for effort and schedule overruns.},
  booktitle = {IEEE International Symposium on Empirical                   Software Engineering (ISESE 2003)},
  pages = {223-230},
  publisher = {IEEE Computer Society},
  address = {Rome, Italy},
  note = {ISBN 0-7695-2002-2}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2003.b,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Software Effort Estimation: Planning XP Guidelines Compared to Research on Traditional Software Development},
  year = {2003},
  abstract = {In the {\textquotedblleft}Planning Extreme Programming{\textquotedblright} framework by Beck and Fowler, they propose several guidelines for project estimation. This paper focuses on the guideline to use estimation teams. There exist several methods from software engineering research on how to do this, with varying degree of formalism involved. We present an account of earlier research on how to combine expert estimates, and compare it with the XP guidelines. It seems that the proposed framework, with some modifications, is a reasonable way to improve accuracy.},
  booktitle = {Presentation at the Ph.D. Symposium in                   Fourth International Conference on eXtreme                   Programming and Agile Processes in Software                   Engineering (XP 2003)},
  pages = {441-442},
  publisher = {Springer-Verlag},
  address = {Genova, Italy},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Syversen.2003,
  author = {Syversen, E and Anda, B and Sj{\o}berg, Dag I. K},
  title = {An Evaluation of Applying Use Cases to Construct Design versus Validate Design},
  year = {2003},
  abstract = {Use case models capture and describe the functional requirements of a software system. A use case driven development process, where a use case model is the principal basis for constructing an object-oriented design, is recommended when applying UML. There are, however, some problems with use case driven development processes and alternative ways of applying a use case model have been proposed. One alternative is to apply the use case model in a responsibility-driven process as a means to validate the design model. We wish to study how a use case model best can be applied in an object-oriented development process and have conducted a pilot experiment with 26 students as subjects to compare a use case driven process against a responsibility-driven process in which a use case model is applied to validate the design model. Each subject was given detailed guidelines on one of the two processes, and used those to construct design models consisting of class and sequence diagrams. The resulting class diagrams were evaluated with regards to realism, that is, how well they satisfied the requirements, size and number of errors. The results show that the validation process produced more realistic class diagrams, but with a larger variation in the number of classes. This indicates that the use case driven process gave more, but not always more appropriate, guidance on how to construct a class diagram The experiences from this pilot experiment were also used to improve the experimental design, and the design of a follow-up experiment is presented.},
  booktitle = {Hawaii International Conference on System                   Sciences (HICSS-36)},
  editor = {Eileen Dennis},
  publisher = {-},
  address = {Big Island, Hawaii},
  isbn = {0-7695-1874-5}
}

@Techreport{SE.7.Arisholm.2003.a,
  author = {Arisholm, Erik and Briand, L and F{\o}yen, A},
  title = {Dynamic Coupling Measurement for Object-Oriented Software (Detailed Analyses)},
  year = {2003},
  abstract = {The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused ({\textquotedblleft}dead{\textquotedblright}) code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at run-time. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Similarly, static analysis is not a fully appropriate tool to account for dead code. Coupling measurement has traditionally been performed using static code analysis, because most of the existing work was done on non-object oriented code and because dynamic code analysis is more expensive and complex. 
This paper describes how coupling can be defined and precisely measured on the basis of dynamic analysis or equivalent dynamic models of the system. We refer to this type of coupling as dynamic coupling. A first empirical evaluation of the proposed dynamic coupling measures is reported in which we study the relationship of these measures with the change proneness of classes. Data from maintenance releases of a large Java system are used for this purpose. Preliminary results suggest that some dynamic coupling measures are strong indicators of change proneness and that they complement existing coupling measures based on static analysis.},
  institution = {Simula Research Laboratory},
  number = {2003-5},
  note = {Updated April 26, 2004}
}

@Techreport{SE.7.Arisholm.2003.b,
  author = {Arisholm, Erik and Sj{\o}berg, D I. K},
  title = {A Controlled Experiment with Professionals to Evaluate the Effect of a Delegated versus Centralized Control Style on the Maintainability of Object-Oriented Software},
  year = {2003},
  abstract = {One fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a {\textquotedblleft}bad{\textquotedblright} object-oriented design. This paper presents a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate and senior professional consultants from several international consultancy companies were hired for one day to take part in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs having a centralized and delegated control style, respectively. 
The results show that the most skilled developers, in particular the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. 
Thus, the maintainability of object-oriented software depends to a large extent on the skill of the developers who are going to maintain it. The results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems that eventually will be maintained by juniors may be unwise, since the cognitive complexity of such {\textquotedblleft}expert{\textquotedblright} designs might be unmanageable for less skilled maintainers.},
  institution = {Simula Research Laboratory},
  type = {Simula Technical Report},
  number = {2003-6}
}

@Techreport{SE.7.Arisholm.2003.c,
  author = {Arisholm, Erik and Ali, S A and Hove, S E},
  title = {An Initial Controlled Experiment to Evaluate the Effect of UML Design Documentation on the Maintainability of Object-Oriented Software in a Realistic Programming Environment},
  year = {2003},
  abstract = {The Unified Modeling Language (UML) is becoming the standard notation for expressing object-oriented analysis and design models. However, to date, there are only very few empirical studies that have attempted to evaluate UML, that is, to evaluate the potential costs, benefits and appropriate use of UML. For example, a common claim is that the UML documentation will improve the ease of understanding how to perform changes to the software it represents. However, this claim has not been evaluated empirically. Furthermore, using UML may incur additional costs, because the UML documents need to be developed and maintained. Thus, the tradeoffs between the costs and benefits of using UML need to be investigated. 

This paper describes a controlled experiment to evaluate the effect of UML documentation (versus no UML documentation) on the maintainability of object-oriented software. In this experiment, 20 students designed and coded several Java change (maintenance) tasks. About half of them received UML documentation, whereas the other half did not. The subjects who received UML documentation also had to update the documentation using a tool, Tau UML. 

The primary goal of the experiment was to evaluate to which extent the access to UML class and sequence diagrams improves the ease of understanding and changing object-oriented software. To assess the costs and benefits of using UML in object-oriented development projects, the experiment also attempted to assess the additional costs associated with updating the UML documentation. The dependent variables of the study were effort (in minutes spent to solve the tasks) and correctness. The effort data was reported by each subject using a task questionnaire. The correctness of the task solutions was assessed by the researchers. The subjects spent between five and eight hours solving tasks. Five observers were present during the experiment. 

A secondary goal of the experiment was to evaluate how a so-called {\textquotedblleft}think-aloud screen{\textquotedblright} affected the performance of subjects in controlled software engineering experiments, that is, whether the think-aloud screen decreases performance of the subjects (task solving time and correctness of the task solutions) compared with the silent condition, and whether the think-aloud screen provides valuable information that cannot be collected in the silent condition. This paper focuses on the evaluation of the effect of using UML to understand and change object-oriented software. Further discussion of the effects of using the think-aloud screen is discussed in [5].},
  institution = {Simula Research Laboratory},
  number = {2003-04}
}

@Misc{SE.7.Karahasanovic.2003,
  author = {Karahasanovic, A},
  title = {Think-Aloud Tool as a Means for Getting Insights into Learning Object-Oriented Concepts},
  year = {2003},
  abstract = {The increasing need for getting insights into learning object-oriented concepts raises the challenge of adequate methods and tools for data collection. 
We have developed a tool, called the think-aloud tool (Karahasanovic et al., 2003), to support the think-aloud method in a variety of studies with large number of subjects. This tool instructs the subjects at regular intervals to write down their thoughts on a web based screen, as opposed to the oral expression of thoughts in conventional think-aloud method. The tool collects the subjects{\textquoteright} feedback together with timestamps. 
Whereas we have experience with the think-aloud tool in software engineering experiments, there is also potential for using such a tool in studies exploring how well the students understand the object-oriented concepts. We believe that the tool is particularly useful for collecting qualitative data about subjects in large-scale studies. Comments collected by the think-aloud tool may help us to identify problems that students have in understanding object-oriented concepts. They may also help us to get insight into learning processes. However, the think-aloud tool provides a less complete picture of learning activities than does the think-aloud method with a human observer and should therefore be used as complementary to other data collection methods or in explorative phases of a study. The think-aloud tool may also affect the student{\textquoteright}s task-related understanding and problem solving. As writing comments on the think-aloud screen demands explanations and reflections it may improve student{\textquoteright}s learning and understanding. 
We plan to conduct an experiment designed to investigate usefulness of the think-aloud tool as a mean for getting insight into learning activities. 56 third year students of Oslo College will be asked to conduct change task on a medium-size Java application (6000 LOC). The students will be divided into four groups. Assignment to the respective groups will be done at random. The first group will be asked to think aloud, the second to give immediate retrospective reports, the third to use the think-aloud tool and the fourth to work without giving feedback {\textendash} the silent condition. For the third group, the think-aloud screen will appear every 15 minutes with the text {\textquotedblleft}What are you thinking now?{\textquotedblright} The subjects will be instructed to describe what they were thinking just before the screen appeared. The time available for writing comments will be limited to two minutes. 
The experiment will last six hours and data will be collected by the think-aloud tool and the classical think-aloud method. The students will be interviewed after the experiment.},
  howpublished = {Extended abstract, Workshop on Learning and                   Teaching Object-Orientation -- Scandinavian                   Perspectives}
}

@Phdthesis{SE.3.Moloekken-Oestvold.2004,
  author = {Mol{\o}kken-{\O}stvold, K J},
  title = {Effort and schedule estimation of software development projects},
  year = {2004},
  abstract = {This thesis explores different topics related to software project effort and schedule estimation. It consists of three sections which address 1) state of practice in the software industry, 2) different methods for improving estimation accuracy, and 3) general methodological aspects in research on software estimation. 
Related to international estimation practices and performance, a review of previous research performed as part of this thesis revealed that a majority of software projects (60-80\%) had encountered effort overruns. The average magnitude of these overruns was 30-40\%. The frequency (65-80\%) and magnitude (20-25\%) of schedule overruns was similar. The dominating estimation approach was expert judgement. Similar, a survey of software projects in Norway undertaken as part of this thesis found a frequency of 76\% effort overruns, with an average magnitude of 41\%. The frequency and magnitude of schedule overruns was 62\% and 25\% respectively. Expert estimation was by far the preferred estimation method. It was also observed that the type of customer had an impact on the magnitude of effort overruns. Public projects had an average effort overrun of 67\%, as opposed to the 21\% average in private projects. This observed difference appears to be caused by systematic differences between private and public organizations found at 1) the political level, 2) the organizational level, and 3) and the individual level. In an experiment on expert judgment, the results indicated that professionals in technical roles (project managers and developers) were significantly more (over)optimistic than professionals in non-technical roles (sales managers and user analysts) when estimating project effort. 
Regarding improvement of estimation practices, two different approaches were investigated. A controlled experiment showed that combination of expert estimates through unstructured group discussion can reduce the existing bias towards too optimistic estimates. In addition, it appears as if use of flexible development models (e.g. incremental, iterative or agile) leads to a lesser magnitude of effort overruns when compared to the traditional sequential waterfall model. 
It was also found that results in some previous studies on estimation accuracy may have been impacted by methodological shortcomings, and that there is little attention directed at ethical aspects in software engineering research in general. 
The main research contributions presented in this thesis indicate that most software projects face schedule and effort overruns. In addition, the frequency and magnitude of software effort and schedule overruns are significant, and appear to be independent of location and stable over time. Most practitioners favour expert judgement over formal estimation models. In order to improve estimation accuracy in ways that are easy and cost-efficient to implement, professional software organizations may rely more on flexible development methods, and combine estimates from different expert through group interaction.},
  school = {University of Oslo}
}

@Article{SE.4.Joergensen.2004.d,
  author = {J{\o}rgensen, M},
  title = {Top-Down and Bottom-Up Expert Estimation of Software Development Effort},
  year = {2004},
  abstract = {Expert estimation of software development effort may follow top-down or bottom-up strategies, i.e., the total effort estimate may be based on properties of the project as a whole and distributed over project activities (top-down) or calculated as the sum of the project activity estimates (bottom-up). The study reported in this paper examines differences between these two strategies based on measurement and video recording of the discussions of seven estimation teams. Each estimation team applied a top-down estimation strategy on one project and a bottom-up estimation strategy on another. The results from the study contribute, we believe, to an improved understanding of when to apply top-down and when to apply bottom-up estimation strategies.},
  journal = {Journal of Information and Software Technology},
  volume = {46},
  number = {1},
  pages = {3--16}
}

@Article{SE.4.Joergensen.2004.g,
  author = {J{\o}rgensen, M and Carelius, G J},
  title = {An Empirical Study of Software Project Bidding},
  year = {2004},
  abstract = {The study described in this paper reports from a real-life bidding process in which 35 companies were bidding for the same contract. The bidding process consisted of two separate phases: A pre-study phase and a bidding phase. In the pre-study phase 17 of the 35 bidding companies provided rough non-binding price indications based on a brief, incomplete description of user requirements. In the bidding phase, all 35 companies provided bids based on a more complete requirement specification that described a software system with substantially more functionality than the system indicated in the pre-study phase. The main result of the study is that the 17 companies involved in the pre-study phase presented bids that were on average about 70\% higher than the bids of the other companies. We propose a preliminary theory that has the potential to explain this difference. This preliminary theory is based, amongst other things, on the {\textquotedblleft}precautionary bidding effect{\textquotedblright} found in auctioning studies. Two important implications of our preliminary theory are that: 1) Software clients tend to achieve better price/uncertainty relationships, i.e., better prices, when the requirement uncertainty perceived by the bidders is low. 2) Software clients should not request early price indications based on limited and uncertain information when the final bids can be based on more complete and reliable information.},
  journal = {IEEE Transactions of Software                      Engineering},
  volume = {30},
  number = {12},
  pages = {953--969}
}

@Article{SE.4.Joergensen.2004.h,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Reasons for Software Effort Estimation Error: Impact of Respondents Role, Information Collection Approach, and Data Analysis method},
  year = {2004},
  abstract = {This study is a first step towards better processes of understanding why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through: (1) Interviews with estimation responsible employees in different roles, (2) Estimation experience reports from 68 completed projects, and, (3) Statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons for estimation error that were given. We found, for example, a strong tendency to perceive factors outside the respondents{\textquoteright} own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents{\textquoteright} own control, and were determined by the estimators{\textquoteright} skill or experience. This bias in types of reason means that the collection only of project managers{\textquoteright} viewpoints will not yield balanced models of reasons for estimation error. Unfortunately, previous studies on reasons for estimation error have tended to collect information from project managers only. We recommend that software companies combine estimation error information from in-depth interviews with stakeholders in all relevant roles, estimation experience reports, and results from statistical analyses of project characteristics.},
  journal = {IEEE Transactions of Software Engineering},
  volume = {30},
  number = {12},
  pages = {993--1007}
}

@Article{SE.4.Joergensen.2004.f,
  author = {J{\o}rgensen, Magne},
  title = {Regression Models of Software Development Effort Estimation Accuracy and Bias},
  year = {2004},
  abstract = {Traditionally, software professionals are requested to provide
minimum-maximum intervals to indicate the uncertainty of their effort
estimates. In this paper we claim that the traditional request is not optimal and
leads to over-optimistic views about the level of estimation uncertainty.
Instead, we propose, a person different from the estimator should identify
minimum and maximum values relevant for planning or bidding purposes and
request that the software professionals assess how likely it is that these values
are exceeded. Not only does this seem to increase realism, but it also leads to
more useful uncertainty assessments. Our claims are based on the results of a
previously reported experiment and field studies in two companies. The two
software companies were instructed to apply the traditional, and our
alternative, framing on random samples of their projects. In total, we collected
information about 47 projects applying the traditional framing and 23 projects
applying the alternative framing.},
  journal = {Journal of Empirical Software Engineering},
  volume = {9},
  number = {4},
  pages = {297--314}
}

@Article{SE.4.Joergensen.2004.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {The impact of customer expectation on software development effort estimates},
  year = {2004},
  abstract = {The results from the study described in this paper suggest that customer expectations of a project{\textquoteright}s total cost can have a very large impact on human judgment-based estimates (expert estimates) of the most likely use of software development effort. The information that the customer expectations are not valid estimation information did not remove the impact. Surprisingly, the estimators did not notice this impact or assessed it to be low. An implication of the results is that the provision of realistic project estimate of most likely use of effort may require that the estimators do not know the customer{\textquoteright}s expectations of the total cost of the project.},
  journal = {International Journal of Project Management},
  volume = {22},
  number = {4},
  pages = {317--325}
}

@Article{SE.4.Joergensen.2004.a,
  author = {J{\o}rgensen, M and Teigen, K H and Mol{\o}kken-{\O}stvold, K J},
  title = {Better sure than safe? Overconfidence in judgment based software development effort prediction intervals},
  year = {2004},
  abstract = {The uncertainty of a software development effort estimate can be indicated through a prediction interval, i.e., the estimated minimum and maximum effort corresponding to a specific confidence level. For example, a project manager may be {\textquotedblleft}90\% confident{\textquotedblright} or believe that is it {\textquotedblleft}very likely{\textquotedblright} that the effort required to complete a project will be between 8,000 and 12,000 work-hours. This paper describes results from four studies (Studies A-D) on human judgement (expert) based prediction intervals of software development effort. Study A examines the accuracy of the prediction intervals in real software projects. The results suggest that the prediction intervals were generally much too narrow to reflect the chosen level of confidence, i.e., that there was a strong over-confidence. Studies B, C and D try to understand the reasons for the observed over-confidence. Study B examines the possibility that the over-confidence is related to type of experience or estimation process. Study C examines the possibility that the concept of confidence level is difficult to interpret for software estimators. Finally, Study D examines the possibility that there are unfortunate feedback mechanisms that reward over-confidence.},
  journal = {Journal of Systems and Software},
  volume = {70},
  number = {1-2},
  pages = {79--93}
}

@Article{SE.4.Arisholm.2004.b,
  author = {Arisholm, Erik and Sj{\o}berg, D I. K},
  title = {Evaluating the Effect of a Delegated versus Centralized Control Style on the Maintainability of Object-Oriented Software},
  year = {2004},
  abstract = {A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a {\textquotedblleft}bad{\textquotedblright} object-oriented design. 
This paper presents a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. 
The results show that the most skilled developers, in particular the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. 
Thus, the maintainability of object-oriented software depends, to a large extent, on the skill of the developers who are going to maintain it. These results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems may eventually pose difficulties unless they make an effort to keep the designs simple, as the cognitive complexity of {\textquotedblleft}expert{\textquotedblright} designs might be unmanageable for less skilled maintainers.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {8},
  pages = {521-534}
}

@Article{SE.4.Joergensen.2004.c,
  author = {J{\o}rgensen, M},
  title = {A Review of Studies on Expert Estimation of Software Development Effort},
  year = {2004},
  abstract = {This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher{\textquoteright}s search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following twelve expert estimation {\textquotedblleft}best practice{\textquotedblright} guidelines are evaluated through the review: 1) Evaluate estimation accuracy, but avoid high evaluation pressure, 2) Avoid conflicting estimation goals, 3) Ask the estimators to justify and criticize their estimates, 4) Avoid irrelevant and unreliable estimation information, 5) Use documented data from previous development tasks, 6) Find estimation experts with relevant domain background and good estimation records, 7) Estimate top-down and bottom-up, independently of each other, 8) Use estimation checklists, 9) Combine estimates from different experts and estimation strategies, 10) Assess the uncertainty of the estimate, 11) Provide feedback on estimation accuracy and development task relations, and, 12) Provide estimation training opportunities. We found supporting evidence for all twelve estimation principles, and provide suggestions on how to implement them in software organizations.},
  journal = {Journal of Systems and Software},
  volume = {70},
  number = {1-2},
  pages = {37--60}
}

@Article{SE.4.Arisholm.2004.a,
  author = {Arisholm, Erik and Briand, L and F{\o}yen, A},
  title = {Dynamic Coupling Measurement for Object-Oriented Software},
  year = {2004},
  abstract = {The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused ({\textquotedblleft}dead{\textquotedblright}) code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at run-time. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Coupling measurement has traditionally been performed using static code analysis, because most of the existing work was done on non-object oriented code and because dynamic code analysis is more expensive and complex to perform. For modern software systems, however, this focus on static analysis can be problematic, because although dynamic binding existed before the advent of object-orientation, its usage has increased significantly in the last decade. 
This paper describes how coupling can be defined and precisely measured based on dynamic analysis of systems. We refer to this type of coupling as dynamic coupling. An empirical evaluation of the proposed dynamic coupling measures is reported in which we study the relationship of these measures with the change proneness of classes. Data from maintenance releases of a large Java system are used for this purpose. Preliminary results suggest that some dynamic coupling measures are significant indicators of change proneness and that they complement existing coupling measures based on static analysis.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {8},
  pages = {491--506}
}

@Article{SE.4.Moloekken-Oestvold.2004.b,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M},
  title = {Group Processes in Software Effort Estimation},
  year = {2004},
  abstract = {The effort required to complete software projects is often estimated, completely or partially, using the judgement of experts, whose assessment may be biased. In general, such bias as there is seems to be towards estimates that are overly optimistic. The degree of bias varies from expert to expert, and seems to depend on both conscious and unconscious processes. One possible approach to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment in which experts with different backgrounds combined their estimates in group discussion. First, twenty software professionals were asked to provide individual estimates of the effort required for a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate via the pooling of knowledge in discussion. We found that the groups submitted less optimistic estimates than the individuals. Interestingly, the group discussion-based estimates were closer to the effort expended on the actual project than the average of the individual expert estimates were, i.e., the group discussions led to better estimates than a mechanical averaging of the individual estimates. The groups{\textquoteright} ability to identify a greater number of the activities required by the project is among the possible explanations for this reduction of bias.},
  journal = {Empirical Software Engineering},
  volume = {9},
  number = {4},
  pages = {315--334}
}

@Article{SE.4.Joergensen.2004.e,
  author = {J{\o}rgensen, M},
  title = {Increasing Realism in Effort Estimation Uncertainty Assessments: It Matters How You Ask},
  year = {2004},
  abstract = {Traditionally, software professionals are requested to provide
minimum-maximum intervals to indicate the uncertainty of their effort
estimates. In this paper we claim that the traditional request is not optimal and
leads to over-optimistic views about the level of estimation uncertainty.
Instead, we propose, a person different from the estimator should identify
minimum and maximum values relevant for planning or bidding purposes and
request that the software professionals assess how likely it is that these values
are exceeded. Not only does this seem to increase realism, but it also leads to
more useful uncertainty assessments. Our claims are based on the results of a
previously reported experiment and field studies in two companies. The two
software companies were instructed to apply the traditional, and our
alternative, framing on random samples of their projects. In total, we collected
information about 47 projects applying the traditional framing and 23 projects
applying the alternative framing.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {4},
  pages = {209--217}
}

@Inproceedings{SE.5.Karahasanovic.2004,
  author = {Karahasanovic, A and Fjuk, A and Sj{\o}berg, Dag I. K and Thomas, R},
  title = {A Controlled Experiment to Evaluate the Reactivity and Usefulness of the Think-Aloud Tool},
  year = {2004},
  abstract = {This paper presents ongoing work on the evaluation of a tool for collecting feedback in large-scale software engineering and program comprehension experiments. Two most important questions one can ask about a new verbal procedure are whether it is useful and whether it is reactive. We have developed a tool, called the think-aloud tool, to support the think-aloud method in a variety of experiments. This tool instructs the subjects at regular intervals to write down their thoughts on a web based screen, as opposed to the oral expression of thoughts in conventional think-aloud method. The tool collects the subjects{\textquoteright} feedback together with timestamps. This paper describes an experiment designed to investigate whether the think-aloud tool affects performance of the subjects and whether it collects information beyond that collected in traditional think-aloud and retrospective reports. Furthermore, we will conduct a qualitative comparison of mouse and keying rates with think-aloud and retrospective reports. In addition, the experiment aims to check for any effect from using the think-aloud tool on the subject{\textquoteright}s task-related understanding and problem solving.},
  booktitle = {Information Resources Management Association                      International Conference (IRMA'04), Human                      Computer Interaction Track},
  editor = {unknown},
  pages = {1033--1034},
  publisher = {Idea Group Publishing},
  address = {New Orleans, Louisiana, USA},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Vokac.2004,
  author = {Vok{\a\'a}c, Marek and Jensen, O},
  title = {Using a Reference Application with Design Patterns to produce Industrial Software},
  year = {2004},
  abstract = {System architectures are described in abstract terms, often using Design 
Patterns. Actual reuse based on such descriptions requires that each development 
project derive a concrete architecture from the chosen Patterns, and 
then implement it in code. 
This paper describes a case study of an industrial development project that 
adopted a reference application as a starting point, in order to avoid the need to 
design a complete architecture. Reference applications are usually made by 
platform or component vendors. Such applications are used to provide executable 
examples of best practices, for a particular platform, component set, or other 
technology. In this case study, the Pet Store application for the J2EE platform 
was chosen. Pet Store is documented by Design Patterns and is also available as 
source code. The development consisted of replacing the application logic, 
while keeping the architecture intact. The project was thus transformed from an 
ab initio development project into a major reuse/modification project. This development 
project was part of a software process improvement effort looking at 
processes for and with reuse. 
Our results indicate that this approach works well, provided that the functional 
and non-functional requirements of the project match those of the reference 
application. The development time was shortened by approximately one 
third relative to the original estimate, and a well-designed application was produced 
despite lack of experience with the platform and n-layer architectures. 
Surprisingly, the production deployment to a different application server was 
still problematic, even though the original reference application was available 
as a sample for that server.},
  booktitle = {PROFES 2004},
  pages = {333--347},
  publisher = {Springer-Verlag},
  address = {Kansai Science City, Japan},
  series = {Lecture Notes on Computer Science}
}

@Article{Simula.SC.100,
  author = {Tveito, Aslak and Lines, Glenn Terje},
  title = {A condition for setting off ectopic waves in computational models of excitable cells},
  year = {2008},
  abstract = {},
  journal = {Mathematical Biosciences},
  volume = {213},
  number = {2},
  pages = {141-150}
}

@Inproceedings{Simula.SE.215,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Judgment-updating among software professionals},
  year = {2008},
  abstract = {Initial judgments related to key decisions in software projects are often based on one-sided or misleading information. The initial assessment of the benefits of introducing a new development tool may for example be based a vendor{\textquoteright}s sales demonstration or a reference client{\textquoteright}s favorable description. In this paper we study software professionals{\textquoteright} abilities to adjust their early, biased judgments when receiving contradicting or less biased information.  The first study, involving 160 software professionals, found a strong under-adjustment for the impact of misleading information and one-sided argument. A follow-up two weeks later found that this under-adjustment was not removed over time. The second study, involving 65 software professionals, found that the ability to update biased judgments may sometimes be quite good, but that it is hard to predict when. A practical consequence of our results is that software professionals should strongly emphasize the avoidance of biased and potentially misleading information and not trust that they are able to adjust their judgments and beliefs when more reliable and unbiased information are available.},
  booktitle = {The 2nd international conference on software knowledge information management and applications (SKIMA)},
  editor = {Hossain and Ouzrout},
  pages = {62-67},
  publisher = {SKIMA conference organising committee},
  isbn = {9781851432516}
}

@Inproceedings{Simula.ND.104,
  author = {Xin, Qin and Zhang, Yan},
  title = {Optimal Fault-tolerant Broadcasting in Wireless Mesh Networks},
  year = {2008},
  abstract = {Wireless Mesh Networks (WMNs) is an emerging communication paradigm to enable resilient, cost-efficient and reliable services for the future-generation wireless networks. In this paper, we study the broadcasting (one-to-all communication) in WMNs with known topology, i.e., where for each primitive the schedule of transmissions is precomputed in advance based on full knowledge about the size and the topology of the network. We show that broadcasting can complete in $D+O(\log n)$ time units in the WMN with size $n$ and diameter $D$. Furthermore, we explore the fault-tolerant broadcasting in WMN. We show an $O(n)$-time deterministic radio broadcasting schedule with large number of link failures. This is an optimal schedule in the sense that there exists a network topology in which the broadcasting cannot complete in less than $\Omega(n)$ units of time.},
  booktitle = {International Conference on High Performance Switching and Routing (HPSR'08)},
  publisher = {IEEE press}
}

@Mastersthesis{Simula.ND.66,
  author = {Stensland, H{\r a}kon Kvale},
  title = {Fault-tolerant routing in SCI networks},
  year = {2006},
  abstract = {Fault-tolerant routing has been a hot topic in the academic community for quite some
time now, and several different approaches have been suggested. In the interconnect
industry however, fault-tolerant routing has not been implemented to the same extent. In
this thesis we have adapted and implemented a local fault-tolerant routing approach in
SCI interconnect technology produced by Dolphin Interconnect Solutions. The existing
technology used in SCI is based in a static reconfiguration approach, where the traffic is
disabled, while the new routing is calculated by a central front-end and distributed out to
the nodes.

Our algorithm builds upon the principle of enabling the nodes to make routing decisions
from the information that is available to them locally, and having the rest of the nodes in
the cluster to be prepared for this unexpected traffic. The algorithm has been tested on
real hardware, and we have shown that it can handle several levels of traffic in the
network. The test has also proven that our method gives the same performance both
before and after the error occurs if the packets have the same conditions, such as
competing traffic and link length. Our routing algorithm is currently integrated as a part
of Dolphin Interconnect Solutions driver in the last official release.},
  school = {University of Oslo}
}

@Inproceedings{Simula.SE.275,
  author = {Kampenes, Vigdis By and Anda, Bente and Dyb{\r a}, Tore},
  title = {Flexibility in Research Designs in Empirical Software Engineering},
  year = {2008},
  abstract = {Problem outline:A common way of classifying empirical research designs is in qualitative and quantitative designs. Typically, particular research methods (e.g., case studies, action research, experiments and surveys) are associated to one or the other of these designs. Studies in empirical software engineering (ESE) are often exploratory and often involve software developers and development organizations. As a consequence, it may be difficult to pre-plan all aspects of the studies, and to be successful, ESE studies must often be designed to handle upcoming changes during the conduct of the study. A problem with the above classification is that it does not cater for the flexibility in the design.  Position: This paper suggests viewing research in ESE along the axis of flexible and fixed designs, which is both orthogonal to the axis of quantitative and qualitative designs, and independent of the particular research method. According to the traditional view of ESE, changes to the research design in the course of a study are typically regarded as threats to the validity of the study results. However, by viewing the study designs as flexible, practical challenges can be turned to useful information. The validity of the results of studies with flexible research designs can be established by applying techniques that are traditionally used for qualitative designs. This paper urges for increased recognition of flexible designs in ESE and discusses techniques for establishing the trustworthiness in flexible designs.},
  booktitle = {Evaluation and Assessment in Software Engineering (EASE 2008) },
  editor = {Giuseppe Visaggio  Maria Teresa Baldassarre Steve Linkman Mark Turner },
  publisher = {BCS},
  isbn = {0}
}

@Inproceedings{Simula.ND.163,
  author = {Xu, W and Chen, J and Zhang, Y and Xiao, Y and Sun, Y},
  title = {Optimal Rate Routing in Wireless Sensor Networks with Guaranteed Lifetime},
  year = {2008},
  abstract = {},
  booktitle = {IEEE Global Communications Conference (IEEE GLOBECOM 2008)}
}

@Inproceedings{Simula.ND.213,
  author = {Sayrafian-Pour, Kamran and Kaspar, Dominik},
  authorURLs = {http://www.oaee.umd.edu/facstaff/sayrafian.html and /people/kaspar},
  title = {A Robust Model-based Approach to Indoor Positioning Using Signal Strength},
  year = {2008},
  abstract = {A simple technique to estimate the position of a mobile node inside a building is based on the Received Signal Strength (RSS). In previous publications, we investigated the effectiveness of using circular array antennas and beamforming in order to enable an access point to estimate the position of a mobile inside a building. We also discussed the feasibility of using model-based radio maps to reduce the need for extensive offline measurements. In this paper, a positioning algorithm based on the relative order of the received signal strengths is discussed. This algorithm in conjunction with the ray-tracing propagation model can have promising performance for indoor environments and essentially eliminates the needs for an extensive set of a priori measurement, training or intricate calibration.},
  booktitle = {IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)}
}

@Misc{Simula.SC.140,
  author = {L{\o}vgren, Alf Emil and Linge, Svein and Mardal, Kent-Andre and Haughton, Victor and Langtangen, Hans Petter},
  title = {CFD analysis of cerebrospinal fluid flow in the cranio-cervical region},
  year = {2008},
  abstract = {},
  howpublished = {Invited talk at the 21st Nordic Seminar on Computational Mechanics, Trondheim, October 16-17}
}

@Inproceedings{Simula.SC.144,
  author = {L{\o}vgren, Alf Emil and Linge, Svein and Mardal, Kent-Andre and Haughton, Victor and Langtangen, Hans Petter},
  title = {CFD analysis of cerebrospinal fluid flow in the cranio-cervical region},
  year = {2008},
  abstract = {},
  booktitle = {21st Nordic Seminar on Computational Mechanics},
  editor = {T.Kvamsdal, K.M.Mathisen, B.Pettersen},
  pages = {94-97},
  publisher = {CIMNE},
  isbn = {978-84-96736-56-6}
}

@Conference{Simula.SC.158,
  author = {Clark, Stuart R and Gurnis, Michael and Muller, R Dietmar},
  title = {Slabs in the mantle - dynamic topography and mantle rheology in the south-western pacific},
  year = {2008},
  abstract = {The study of mantle convection has been strongly linked to the effect such convection has on the Earth{\textquoteright}s surface and vice versa. We have constructed a coupled plate kinematic/mantle convection model for the Southwest Pacific from the Vitiaz trench to New Zealand to evaluate the origin of Tertiary volcanics on Northland, New Zealand and the driving forces of anomalous subsidence of the Taranaki and South Fiji Basins. The coupled mantle convection/plate kinematic model accurately constrains the location of slabs in the mantle implied by particular plate reconstructions. The veracity of the kinematic model is checked by utilizing spherical shell overlays in the visualisation program 4DLM to compare the generated slabs in the mantle convection model with seismic anomalies. The mantle rheological model {\textendash} the viscosity contrast between upper and lower mantle and the strength of the clapeyron slopes at the transition zones - used also has an effect on the location of slabs in the mantle, as slabs are various deflected at the transition zone, held up for a time or move straight through. The rheological model affects the dynamic topography outcome and the mantle model can therefore be constrained with reference to data-derived anomalous depth, in particular in the South Fiji Basin. Lastly, we test alternative plate reconstruction scenarios for the region, investigating the importance of the surmised Loyalty slab, and corresponding north or south dipping subduction zone close to North Island, on the dynamic topography in the Taranaki Basin. We will demonstrate how the ability to visualise the virtual evolution of dynamic topography, plate boundaries and the mantle through time provides for an intuitive understanding of the time-dependent development of the system, while comparing the models with observables enables the computer-simulations to constantly be referred back to the real earth.},
  booktitle = {Proceedings of the 33rd International Geological Congress},
  publisher = {IGC (Oslo, 2008)}
}

@Conference{Simula.SC.160,
  author = {Clark, Stuart R and Bruaset, Are Magnus},
  title = {Lithospheric Modelling: Research Directions at Simula},
  year = {2008},
  abstract = {},
  booktitle = {GeoMath08 Workshop, Santa Fe, New Mexico},
  publisher = {Computational Infrastructure for Geodynamics}
}

@Techreport{Simula.SE.315,
  author = {Arisholm, Erik and Briand, Lionel Claude and Johannessen, Eivind B},
  title = {A Systematic and Comprehensive Investigation of Methods to Build and Evaluate Fault Prediction Models},
  year = {2008},
  abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets entailing different data collection costs, such as source code structural measures and historic change/fault (process) measures, and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).  The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas across releases {\textendash} both in terms of ROC area and cost-effectiveness. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. The regular confusion matrix criteria, although popular, are not clearly related to what we consider to be a crucial aspect, namely the cost-effectiveness of using fault-proneness prediction models to focus verification effort where it is the most needed.},
  institution = {Simula Research Laboratory},
  number = {TR 2008-06}
}

@Misc{Simula.SC.164,
  author = {Clark, Stuart R},
  title = {Time-Dependant Evolution of Subduction Zones},
  year = {2008},
  abstract = {},
  howpublished = {Invited Talk}
}

@Misc{Simula.SC.128,
  author = {Myklebust, Oddrun Christin and Mardal, Kent-Andr{\a`e} and Hentschel, Susanne},
  title = {Patient-specific Computational Fluid Dynamic Simulations in the Circle of Willis},
  year = {2008},
  abstract = {}
}

@Inproceedings{Simula.ND.211,
  author = {Kaspar, Dominik and Hansen, Audun F and Griwodz, Carsten},
  title = {Multilink Transfer over Heterogeneous Networks},
  year = {2008},
  abstract = {Multilink transfer is the method of connecting a single host over multiple access networks to the Internet, with the potential of increasing communication quality. However, link heterogeneity causes severe challenges on end-to-end traffic. We explain the problems in more detail and how we intend to solve them using a proxy solution.},
  booktitle = {IEEE International Conference on Network Protocols (ICNP), Poster Session}
}

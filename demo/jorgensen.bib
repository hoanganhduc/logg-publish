
@Inproceedings{Simula.SE.215,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Judgment-updating among software professionals},
  year = {2008},
  abstract = {Initial judgments related to key decisions in software projects are often based on one-sided or misleading information. The initial assessment of the benefits of introducing a new development tool may for example be based a vendor{\textquoteright}s sales demonstration or a reference client{\textquoteright}s favorable description. In this paper we study software professionals{\textquoteright} abilities to adjust their early, biased judgments when receiving contradicting or less biased information.  The first study, involving 160 software professionals, found a strong under-adjustment for the impact of misleading information and one-sided argument. A follow-up two weeks later found that this under-adjustment was not removed over time. The second study, involving 65 software professionals, found that the ability to update biased judgments may sometimes be quite good, but that it is hard to predict when. A practical consequence of our results is that software professionals should strongly emphasize the avoidance of biased and potentially misleading information and not trust that they are able to adjust their judgments and beliefs when more reliable and unbiased information are available.},
  booktitle = {The 2nd international conference on software knowledge information management and applications (SKIMA)},
  editor = {Hossain and Ouzrout},
  pages = {62-67},
  publisher = {SKIMA conference organising committee},
  isbn = {9781851432516}
}

@Article{SE.4.Arisholm.2001,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and J{\o}rgensen, M},
  title = {Assessing the Changeability of two Object-Oriented Design Alternatives - a Controlled Experiment},
  year = {2001},
  abstract = {},
  journal = {Empirical Software Engineering},
  volume = {6},
  number = {3},
  pages = {231-277}
}

@Article{SE.4.Joergensen.2001.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Impact of Effort Estimates on Software Project Work},
  year = {2001},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {43},
  number = {15},
  pages = {939-948}
}

@Article{SE.4.Joergensen.2001.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Software Process Improvement and Human Judgement Heuristics},
  year = {2001},
  abstract = {},
  journal = {Scandinavian Journal of Information Systems},
  volume = {13},
  number = {1},
  pages = {99-122}
}

@Inproceedings{SE.5.Anda.2001.a,
  author = {Anda, Bente Cecilie Dahlum and Dreiem, Hege and Sj{\o}berg, Dag Ingar Kondrup and J{\o}rgensen, Magne},
  title = {Estimating Software Development Effort Based on Use Cases - Experiences from Industry},
  year = {2001},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Several methods for estimating software development effort are based on attributes of a use case model. This paper reports the results of three industrial case studies on the application of a method for effort estimation based on use case points. The aim of this paper is to provide guidance for other organizations that want to improve their estimation process applying use cases. Our results support existing claims that use cases can be used successfully in estimating software development effort. The results indicate that the guidance provided by the use case points method can support expert knowledge in the estimation process. Our experience is also that the design of the use case models has a strong impact on the estimates. 
},
  booktitle = {4th International Conference on the Unified                   Modeling Language (UML2001)},
  editor = {Gogolla, M. and Kobryn, C.},
  pages = {487-502},
  publisher = {Springer-Verlag},
  address = {Toronto, Canada},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-42667-1}
}

@Inproceedings{SE.5.Anda.2001.b,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag Ingar Kondrup and J{\o}rgensen, Magne},
  title = {Quality and Understandability of Use Case Models},
  year = {2001},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Use case models are also used in communication between stakeholders in development projects. It is therefore important that the use case models are constructed in such a way that they support the development process and promote a good understanding of the requirements among the stakeholders. Despite this, there are few guidelines on how to construct use case models. This paper describes an explorative study where three different sets of guidelines were used for constructing and documenting use case models. An experiment with 139 undergraduate students divided into 31 groups was conducted. Each group used one out of the three sets of guidelines when constructing a use case model from an informal requirements specification. After completing the use case model, each student answered a questionnaire. The results of the experiment indicate that guidelines based on templates support the construction of use case models that are easier to understand for the readers, than guidelines without specific details on how to document each use case. The guidelines based on templates were also considered as the most useful when constructing use cases. In addition to better understandability, our experiment indicates that the guidelines based on templates result in better use case models regarding also other quality attributes. Our results further indicate that it may be beneficial to combine the template guidelines with another set of guidelines that focus on the documentation of the flow of events of each use case. Keywords. Object-oriented analysis, Requirements specification, Use Cases, UML, Understandability, Experiment},
  booktitle = {ECOOP 2001 - Object-Oriented Programming, 15th                   European Conference},
  editor = {J. Lindskov Knudsen},
  pages = {402-428},
  publisher = {Springer-Verlag},
  address = {Budapest, Hungary},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-42206-4}
}

@Inproceedings{SE.5.Bratthall.2001.a,
  author = {Bratthall, L and Arisholm, Erik and J{\o}rgensen, M},
  title = {Program Understanding Behaviour During Estimation of Enhancement on Small Java Programs},
  year = {2001},
  abstract = {},
  booktitle = {3rd International Conference on Product                   Focused Software Process Improvement (PROFES                   2001)},
  publisher = {-},
  address = {Kaiserslautern, Germany}
}

@Inproceedings{SE.5.Bratthall.2001.b,
  author = {Bratthall, L and J{\o}rgensen, M},
  title = {Can you Trust a Single Data-source Exploratory Software Engineering Case Study},
  year = {2001},
  abstract = {},
  booktitle = {EASE 2001},
  publisher = {-},
  address = {Keele, England}
}

@Inproceedings{SE.5.Joergensen.2001.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, D I. K},
  title = {Anchoring Effects: An Important Cause of Too Optimistic Effort Estimates in Software Development Projects},
  year = {2001},
  abstract = {},
  booktitle = {24nd IRIS Conference (Information Systems                   Research Seminar In Scandinavia)},
  editor = {-},
  publisher = {-},
  address = {Ulvik, Norway},
  isbn = {0}
}

@Inproceedings{SE.5.Joergensen.2001.b,
  author = {J{\o}rgensen, M and Indahl, U and Sj{\o}berg, D I. K},
  title = {Software Effort Estimation by Analogy and Regression Toward the Mean},
  year = {2001},
  abstract = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon {\textquotedblleft}regression toward the mean{\textquotedblright} (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  booktitle = {The Thirteenth International Conference on                   Software Engineering \& Knowledge Engineering                   (SEKE01)},
  editor = {-},
  pages = {253 -- 262 },
  publisher = {Elsevier Science Inc},
  address = {Buenos Aires, Argentina},
  isbn = {0}
}

@Inproceedings{SE.5.Karahasanovic.2001.c,
  author = {Karahasanovic, A and Sj{\o}berg, D I. K and J{\o}rgensen, M},
  title = {Data Collection in Software Engineering Experiments},
  year = {2001},
  abstract = {This paper presents ongoing work on developing a mechanism for automatic data collection during software engineering experiments. The importance of experiments in software engineering can hardly be overemphasised. Conducting such experiments raises the challenge of efficient and reliable data collection and analysis. During the experiment subjects (students, experienced programmers, etc.) perform tasks on the software engineering artefacts (e.g., design documents and code) using the technology under study in the given context (e.g. operating system). We are typically interested in data concerning subjects, their interaction with the technology and context, and changes of the software engineering artefacts. To conduct a series of experiments on software engineering technology (methods and tools), we envisage an experimental environment for data collection and analysis. As a first step, we have developed a logging mechanism that collects data about subjects of the experiment and their usage of the technology under study. We believe that our logging mechanism will alleviate data collection and increase the validity of experiments.},
  booktitle = {Managing Information Technology in a Global                   Economy, Information Resources Management                   Association International Conference IRMA 2001,                   Software Engineering Track},
  editor = {-},
  pages = {1027-1028},
  publisher = {Idea Group Publishing},
  address = {Toronto, Ontario Canada},
  isbn = {0}
}

@Inproceedings{SE.5.Sjoeberg.2001,
  author = {Sj{\o}berg, D I. K and Arisholm, Erik and J{\o}rgensen, M},
  title = {Conducting Experiments on Software Evolution},
  year = {2001},
  abstract = {},
  booktitle = {4th International Workshop on Principles of                   Software Evolution (IWPSE 2001)},
  editor = {Tetsuo Tamai and Mikio Aoyama and Keith                   Bennet},
  pages = {142-145},
  publisher = {-},
  address = {Vienna, Austria},
  isbn = {1-58113-508-4}
}

@Article{SE.4.Bratthall.2002,
  author = {Bratthall, L and J{\o}rgensen, M},
  title = {Can you trust a single data-source exploratory software engineering case-study},
  year = {2002},
  abstract = {},
  journal = {Journal of Empirical Software Engineering},
  volume = {7},
  pages = {9-26}
}

@Article{SE.4.Joergensen.2002.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Impact of Experience on Maintenance Skills},
  year = {2002},
  abstract = {This study reports results from an empirical study of 54 software maintainers in the software maintenance department of a Norwegian company. The study addresses the relationship between amount of experience and maintenance skills. The findings were, amongst others: (1) While there may have been a reduction in the frequency of major unexpected problems from tasks solved by very inexperienced to medium experienced maintainers, additional years of general software maintenance experience did not lead to further reduction. More application specific experience, however, further reduced the frequency of major unexpected problems. (2) The most experienced maintainers did not predict maintenance problems better than maintainers with little or medium experience. (3) A simple one-variable model outperformed the maintainers{\textquoteright} predictions of maintenance problems, i.e., the average prediction performance of the maintainers seems poor. An important reason for the weak correlation between length of experience and ability to predict maintenance problems may be the lack of meaningful feedback on the predictions.},
  journal = {Software Maintenance: Research and Practice},
  volume = {14},
  number = {2},
  pages = {123-146}
}

@Article{SE.4.Joergensen.2002.b,
  author = {J{\o}rgensen, M},
  title = {Comments on: A simulation tool for efficient analogy based cost estimation},
  year = {2002},
  abstract = {},
  journal = {Journal of Empirical Software Engineering},
  volume = {7},
  pages = {375-376}
}

@Inproceedings{SE.5.Joergensen.2002.a,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Combination of software development effort prediction intervals: Why, when and how?},
  year = {2002},
  abstract = {The uncertainty of a software development effort estimate may be
described through a prediction interval, e.g., that the most likely
use of effort is 1.500 work-hours and that it is 90 \% probable
(90\% confidence level) that the actual use of effort will be
between 1.000 (minimum) and 2.000 (maximum) work-hours.
Previous studies suggest that software development effort
prediction intervals are, on average, much too narrow to reflect
high confidence levels, i.e., the uncertainty is under-estimated.
This paper analyses when and how a combination of several
individual prediction intervals of the same task improves the
correspondence between hit rate and confidence level of effort
prediction intervals. We analyse three combination strategies: (1)
Average of the individual minimum and maximum values, (2)
Maximum and minimum of the individual maximum and
minimum values, and (3) Group process (discussion) based
prediction intervals. Based on an empirical study with software
professionals we found that strategy (1) did not lead to much
correspondence improvement compared with the individual
prediction intervals, mainly because of a, as expected, strong
individual bias towards too narrow prediction intervals. Strategy
(2) and (3) both improved the correspondence. However, Strategy
(3) used the uncertainty information more efficiently, i.e., had
narrower prediction intervals for the same hit rate. Our empirical
results suggest that group discussion based combination of
prediction intervals should be used instead of {\a`\i}mechanical{\^\i}
combinations of individual prediction intervals. Clearly, there is
no best combination strategy for all prediction interval situations,
and the choice of strategy should be based on an investigation of
factors that impact the usefulness of a strategy.},
  booktitle = {Fourteenth IEEE Conference on Software                   Engineering and Knowledge Engineering (SEKE'02)},
  pages = {425-428},
  publisher = {-},
  address = {Ischia, Italy}
}

@Inproceedings{SE.5.Joergensen.2002.b,
  author = {J{\o}rgensen, M and Teigen, K H},
  title = {Uncertainty Intervals versus Interval Uncertainty: An Alternative Method for Eliciting Effort Prediction Intervals in Software Development Projects},
  year = {2002},
  abstract = {Frequently, there is a poor correspondence between the judged and the actual uncertainty of effort usage in software development projects. This may to some extent be a consequence of the uncertainty elicitation process. Traditionally, software developers are asked to provide the minimum and maximum effort of development work for a given confidence level, e.g., minimum and maximum effort that includes the actual effort usage with a 90\% probability. An alternative uncertainty elicitation process is to instruct the software developers to provide the uncertainty of a given effort interval, e.g., the probability that the actual effort is between 50\% and 200\% of the estimated most likely effort. In an empirical investigation, this alternative process led to significant improvement of prediction interval accuracy. The observed improvement using this alternative elicitation process can, we believe, be explained through a simplified interpretation of historical prediction accuracy data, less {\textquotedblleft}conflicting estimation goal{\textquotedblright}, and less influence from the {\textquotedblleft}anchoring effect{\textquotedblright}.},
  booktitle = {Proceedings of International Conference on Project                   Management (ProMAC)},
  pages = {343-352},
  publisher = {-},
  address = {Singapore}
}

@Inproceedings{SE.5.Joergensen.2002.c,
  author = {J{\o}rgensen, M and L{\o}vstad, N and Moen, L},
  title = {Combining quantitative software development cost estimation precision data with qualitative data from project experience reports at Ericsson Design Center in Norway},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Conference on Empirical                   Assesssments of Software Engineering},
  publisher = {-},
  address = {Keele, UK}
}

@Inproceedings{SE.5.Joergensen.2002.d,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {A simple effort prediction interval approach},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Conference on Achieving Quality in Information Systems},
  editor = {-},
  pages = {19-30},
  publisher = {-},
  address = {Venezia},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Sjoeberg.2002,
  author = {Sj{\o}berg, Dag I. K and Anda, B and Arisholm, Erik and Dyb{\r a}, T and J{\o}rgensen, M and Karahasanovic, A and Koren, E and Vok{\a\'a}c, Marek},
  title = {Conducting Realistic Experiments in Software Engineering},
  year = {2002},
  abstract = {An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. 
This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes some of the experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.},
  booktitle = {ISESE'2002 (First International Symposium on                   Empirical Software Engineering)},
  editor = {not found},
  pages = {17-26},
  publisher = {IEEE Computer Society},
  isbn = {0-7695-1796-X}
}

@Inproceedings{SE.5.Teigen.2002,
  author = {Teigen, K H and J{\o}rgensen, M},
  title = {Probability Intervals and Interval Probabilities are Not the Same},
  year = {2002},
  abstract = {},
  booktitle = {Annual Meeting Society for Judgment and                   Decision Making (Poster)},
  publisher = {-},
  address = {Kansas, USA}
}

@Article{SE.4.Joergensen.2003.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K and Indahl, U},
  title = {Software Effort Estimation by Analogy and Regression Toward the Mean},
  year = {2003},
  abstract = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon {\textquotedblleft}regression toward the mean{\textquotedblright} (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  journal = {Journal of Systems and Software},
  volume = {68},
  number = {3},
  pages = {253-262}
}

@Article{SE.4.Joergensen.2003.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {An effort prediction interval approach based on the empirical distribution of previous estimation accuracy},
  year = {2003},
  abstract = {When estimating software development effort, it may be useful to describe the uncertainty of the estimate through an effort prediction interval (PI). An effort PI consists of a minimum and a maximum effort value and a confidence level. We introduce and evaluate a software development effort PI approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the effort PIs of new projects. First, we demonstrate the applicability and different variants of the approach on a data set of 145 software development tasks. Then, we experimentally compare the performance of one variant of the approach with human (software professionals{\textquoteright}) judgment and regression analysis-based effort PIs on a data set of 15 development tasks. Finally, based on the experiment and analytical considerations, we discuss when to base effort PIs on human judgment, regression analysis, or our approach.},
  journal = {Journal of Information and Software Technology},
  volume = {45},
  number = {3},
  pages = {123-136}
}

@Article{SE.4.Joergensen.2003.c,
  author = {J{\o}rgensen, M},
  title = {How much does a vacation cost? or What is a software cost estimate?},
  year = {2003},
  abstract = {What is a software cost estimate? Is it the most likely cost, the planned cost, the budget, the price, or,
something else? Through comparison with vacation cost estimation and a real-life case we illustrate that it is not
meaningful to compare and analyze cost estimates unless it is clear which interpretation is applied. Unfortunately,
the software industry, software engineering textbooks and scientific estimation studies do frequently not clarify how
they apply the term {\textquoteleft}cost estimate{\textquoteright}. We argue that this lack of clarity may lead to conflicting estimation goals,
communication problems, and, learning problems, and provide recommendations on how to deal with these
problems.},
  journal = {ACM Software Engineering Notes},
  volume = {28},
  number = {6},
  pages = {30}
}

@Article{Simula.SE.311,
  author = {J{\o}rgensen, Magne and Halkjelsvik, Torleif},
  title = {The Effects of Request Formats on Judgment-based Effort Estimation},
  year = {2008},
  abstract = {In this paper we study the effects of a change from the traditional request {\textquotedblleft}How much effort is required to complete X?{\textquotedblright} to the alternative {\textquotedblleft}How much can be completed in Y work-hours?{\textquotedblright}. Studies 1 and 2 report that software professionals receiving the alternative format provided much lower, and presumably more optimistic, effort estimates of the same software development work than those receiving the traditional format. Studies 3 and 4 suggest that the effect belongs to the family of anchoring effects. An implication of our results is that project managers and clients should avoid the alternative estimation request format.},
  journal = {Journal of Systems and software}
}

@Article{Simula.SE.223,
  author = {J{\o}rgensen, Magne and Gruschke, Tanja},
  title = {The Impact of Lessons-Learned Sessions on Effort Estimation and Uncertainty Assessments},
  year = {2008},
  abstract = {Inaccurate estimates of software development effort is a frequently reported cause of IT-project failures. We report results from a study that investigated the effect of introducing lessons learned sessions on estimation accuracy and the assessment of uncertainty. Twenty software professionals were randomly allocated to a Learning group or a Control group and instructed to estimate and complete the same five development tasks. Those in the Learning group, but not those in the Control group, were instructed to spend at least 30 minutes on identifying, analyzing, and summarizing their effort estimation and uncertainty assessment experience after completing each task. We found that the estimation accuracy and the realism of the uncertainty assessment were not better in the Learning group than in the Control group. A follow-up study with 83 software professionals was completed to better understand this lack of improvement from lessons-learned sessions. The follow-up study found that receiving feedback about other software professionals{\textquoteright} estimation performance led to more realistic uncertainty assessments than receiving the same feedback of one{\textquoteright}s own estimates. Lessons-learned sessions, we argue, have to be carefully designed to avoid wasting resources on learning processes that stimulate rather than reduce learning biases related to assessment of own estimation performance.},
  journal = {IEEE Transactions of Software Engineering}
}

@Article{Simula.SE.299,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment},
  year = {2008},
  abstract = {Several studies have reported that software development effort estimates can be strongly affected by effort-irrelevant and misleading information without the estimators being aware of this effect. These studies were conducted in laboratory (artificial) estimation contexts. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from Eastern European and East Asian countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: i) reduced length of requirement specification with no change of content, ii) information about the low effort spent on the development of the old system to be replaced, iii) information about the client{\textquoteright}s unrealistic expectations about low cost, and iv) a restriction of a short development period with start up a few months ahead (which should, rationally speaking, lead to an increase in effort). All manipulations led to decreased median effort estimates, but only manipulation iv) led to a large, statistically significant decrease. A comparison of the effects of similar types of irrelevant and misleading information in laboratory and field settings suggests that the effect of manipulations i), ii) and iii) where much lower in field settings than in laboratory settings, while the effect of manipulation iv) was almost at the same level. We conclude that the tendency towards a smaller  effect in field settings means that laboratory studies are frequently only useful for demonstrating the existence of a software engineering phenomenon, or for understanding it better, and that we need field studies to analyze its importance.},
  journal = {IEEE Transactions on Software Engineering}
}

@Article{Simula.SE.235,
  author = {J{\o}rgensen, Magne},
  title = {Selection of Effort Estimation Strategies},
  year = {2008},
  abstract = {We currently know little about the factors that motivate the selection and change of estimation strategy in judgment-based effort estimation context. A better understanding of these issues may lead to more accurate judgment-based effort estimates and motivates the four experiments reported in this paper. The experiments{\textquoteright} two main results are the identification of the importance of {\textquotedblleft}estimation surprises{\textquotedblright} (large estimation errors) to motivate estimation strategy change and the large individual variation in the initial choice of estimation strategy. The individual variation seems not only to be a result of differences in previous experiences, but also a result of differences in the mental {\textquotedblleft}accessibility{\textquotedblright} of the strategies. We found, for example, that the use of a strategy was increased when we instructed a developer to use the same type of strategy on unrelated tasks immediately before. The laboratory contexts of the studies means that the results should be interpreted as a first step towards more knowledge about expert estimation strategies and that there is a strong need for more studies, preferably in field situations, before recommending actions on the basis of the findings.},
  journal = {International Journal of Forecasting}
}

@Article{Simula.SE.159,
  author = {J{\o}rgensen, Magne},
  title = {How to Avoid Selecting Providers with Bids Based on Over-Optimistic Cost Estimates},
  year = {2009},
  abstract = {It is well known that software development companies tend to produce over-optimistic cost estimates and that this over-optimism may lead to delivery problems for the clients as well as the providers. In this paper we summarize evidence suggesting that the clients can reduce the likelihood of selecting providers with bids based on over-optimistic cost estimates through their control of the bidding processes. Important means for this purpose include: avoid inviting many bidders when price is an important criterion for selection; avoid using price as an important criterion for selection when the ability to assess provider competence is low; apply bidding processes that ensure that the provider understands the complexity of the project; avoid budget or price expectation related information in the bidding material and avoid a negotiation process in which you ask for bid updates on reduced functionality. The evidence presented in this paper can also be used by software providers to identify bidding rounds where it is likely that they win only when strongly over-optimistic about the cost.},
  journal = {IEEE Software (May/June)}
}

@Misc{Simula.SE.151,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {How much does feedback and performance review improve software development effort estimation? An Empirical Study (extended abstract)},
  year = {2006},
  abstract = {Over-optimistic and over-confident software development effort estimates are more the rule than the exception. Several software improvement process frameworks, e.g., the Capability Maturity Model, are based on the assumption that improved feedback and use of performance reviews leads to higher degree of realism and better project performance. This paper investigates that assumptions, i.e., whether effort estimation and uncertainty assessment skills improve with better feedback processes and mandatory estimation performance reviews.

We recruited 20 professional software developers with the necessary technological experience and skill to complete the same 5 software development tasks on an in-use web-system. They were paid standard industry wage corresponding to their level of experience and expertise. Work conditions were like those in industry. Ten of the participants received outcome feedback on their estimation performance and followed our estimation performance review instructions, the other ten acted as a control group with no performance review instructions and only on-the-job feedback. 

We found no or only minor differences in performance between the treatment groups and conclude that the feedback and performance reviews did not lead to improvement of estimation or uncertainty assessment skill. Possible reasons for this surprising finding include: a) Low motivation for learning (A high performance on effort estimation and uncertainty assessment may have been perceived as a much less important goal than being perceived as a skilled programmer through low effort estimates and high confidence.), and, b) The software developers who were instructed to learn from previous performance may have believed that they have learned more than they actually did.

In a follow-up experiment 81 software professionals assessed the uncertainty of the effort estimates provided by the developers in the original experiment. We found that the software professionals assessing the uncertainty of other developers{\textquoteright} effort estimates achieved a much higher realism and better learning from history.
},
  howpublished = {In International Symposium on Forecasting, edited by Antonio Garc{\a\'\i}a-Ferrer, Santander, 12-14 June 2006. International Institute of Forecasters, pages 103}
}

@Article{Simula.SE.160,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {The Impact of Irrelevant Information on Estimates of Software Development Effort},
  year = {2008},
  abstract = {Software professionals typically estimate software development effort on the basis of a requirement specification. Parts of this specification frequently contain information that is irrelevant to the estimation of the actual effort involved in the development of software. We hypothesize that effort-irrelevant information sometimes has a strong impact on effort estimates. To test this hypothesis, we conducted three controlled experiments with software professionals. In each of the experiments, the software professionals received specifications describing the same requirements. However, we gave one group of the software professionals a version of the requirement specification where we had included additional, effort-irrelevant, information. In all three experiments, we observed that the estimates of most likely effort increased when the estimates were based on requirement specifications that contained the information irrelevant to development effort. The results suggest that when estimation-irrelevant information is included as input to expert judgment-based estimation processes, the estimators find it difficult to ignore it. The results also show that it is difficult to predict the impact of estimation-irrelevant information and that software professionals seem to be unaware of the impact. One possible (and advisable) course of action, given our findings, would be to remove estimation-irrelevant information from the requirement specification prior to the use of it as input to estimation work.},
  journal = {JSS}
}

@Misc{Simula.SE.152,
  author = {J{\o}rgensen, Magne},
  title = {How to get a low price on your next software development project and why you should avoid it},
  year = {2007},
  abstract = {Software providers are well known for over-optimistic forecasts of how much the development of software systems will cost. We claim that the software clients, to a far greater degree than is commonly believed, are responsible for the providers{\textquoteright} over-optimism. Our claim is based on empirical studies of how client controlled factors impact the level of optimism in providers{\textquoteright} software development cost forecasts. The studies suggest that forecasting over-optimism is related to clients who communicate unrealistic price expectations or present attractive future opportunities related to winning the bidding round systematically. The studies also suggest that there are commonly used formats of software project bidding processes that strongly increase the risk of selecting providers with prices based on over-optimistic cost forecasts. We find, for example, that bidding processes with negotiation tend to increase the level of over-optimism when the negotiation involves asking for price updates on a reduced version of the initial software requirements. Evidence suggests that the clients may not benefit from a low price based on an over-optimistic cost forecast. One reason for this, applying concepts from the principal-agency theory, is the high level of information asymmetry in most software development project. As an illustration, the clients{\textquoteright} poor ability to specify and monitor software quality attributes, such as the maintainability of the software, makes it possible for the providers to deliver software with poorer than expected quality. We observe that social factors that otherwise may be sufficient to avoid opportunistic behaviour by the clients, e.g., the software developers{\textquoteright} self-imposed code of ethics, are less important in situations with over-optimistic effort estimates, e.g., in situations where the provider tries to avoid financial losses. Based on the above findings we outline elements of how the bidding processes should be designed to reduce the probability of receiving bids based on over-optimistic forecasts.},
  howpublished = {International Symposium on Forecasting}
}

@Inproceedings{SE.5.Joergensen.2003,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {A Preliminary Checklist for Software Cost Management},
  year = {2003},
  abstract = {This paper presents a process framework and a preliminary checklist for software cost management.
While most textbooks and research papers on cost estimation look mainly at the {\textquotedblleft}Estimation{\textquotedblright} phase, our
framework and checklist includes the phases relevant to estimation: {\textquotedblleft}Preparation{\textquotedblright}, {\textquotedblleft}Estimation{\textquotedblright},
{\textquotedblleft}Application{\textquotedblright}, and {\textquotedblleft}Learning{\textquotedblright}. We believe that cost estimation processes and checklists should support these
phases to enable high estimation accuracy. The checklist we suggest is based on checklists from a number of
sources, e.g., a handbook in forecasting and checklists present in several Norwegian software companies. It
needs, however, to be extended through feedback from other researchers and software practitioners. There is
also a need for a provision of conditions for meaningful use of the checklist issues and descriptions of the
strength and sources of evidence in favor of the checklist issues. The present version of the checklist should
therefore be seen as preliminary and we want to get feedback from the conference participants and other
readers of this paper for further improvements.},
  booktitle = {IEEE International Conference on Quality                   Software},
  pages = {134-140},
  publisher = {-},
  address = {Dallas, USA}
}

@Inproceedings{SE.5.Joergensen.2003.a,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Situational and Task Characteristics Systematically Associated With Accuracy of Software Development Effort Estimates},
  year = {2003},
  abstract = {},
  booktitle = {Information Resources Management Association                   Conference},
  pages = {824-826},
  publisher = {-},
  address = {Philadelphia, USA}
}

@Inproceedings{SE.5.Joergensen.2003.b,
  author = {J{\o}rgensen, M},
  title = {An Attempt to Model the Software Development Effort Estimation Accuracy and Bias},
  year = {2003},
  abstract = {This paper describes regression models of estimation accuracy and bias, i.e., models aiming at explaining the accuracy and bias variation of an organization{\textquoteright}s software development effort estimates. We collected information about variables we believed would impact the estimation accuracy or bias of tasks completed by the organization. In total, information about 49 software development tasks was collected. We found that the following conditions led to decrease in estimation accuracy: 1) Estimates were provided by a person in the role {\textquoteleft}software developer{\textquoteright} instead of {\textquoteleft}project leader{\textquoteright}, 2) The project had its highest priority on time-to-delivery instead of quality or cost, and, 3) The estimator did not participate in the completion of the task. The following conditions led to increased bias towards under-estimation: 1) Estimates were provided by a person with the role {\textquoteleft}software developer{\textquoteright} instead of {\textquoteleft}project leader{\textquoteright}, and, 2) The estimator assessed the accuracy of own estimates of similar, previously completed tasks to be low (more than 20\% deviation). Although all variables included in the regression models were significant (p<0.1), the explanatory and predictive power of both models were poor, i.e., most of the variance in estimation accuracy and bias was not explained or predicted by our models. An analysis of the estimators{\textquoteright} own descriptions of the reasons for achieved estimation accuracy on each task suggests that it will be difficult to include all important estimation accuracy and bias factors in regression-based models. It is, for this reason, not realistic to expect regression models to replace human judgment in risk analyses and estimation improvement plans. We believe, nevertheless, that such models may be useful to support the human judgment processes.},
  booktitle = {Proceedings of Conference on Empirical                   Assessment in Software Engineering},
  pages = {117-128},
  publisher = {-},
  address = {Keele, England}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2003,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M},
  title = {Software Effort Estimation: Unstructured Group Discussion as a Method to Reduce Individual Biases},
  year = {2003},
  abstract = {The effort of software projects is often estimated, completely or partially, using expert judgement. This estimation process is subject to biases of the expert responsible. Generally, this bias seems to be towards too optimistic estimates regarding the effort needed to complete the project. The degree of bias varies depending on the expert involved, and seems to be connected to both conscious and unconscious decisions. One possible way to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment where experts with different backgrounds combined their estimates through group discussion. Twenty software professionals were asked to provide individual effort estimates of a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate through discussion and combination of knowledge. We found that the groups were less optimistic in their estimates than the individual experts. Interestingly, the group discussion-based estimates were closer to the effort used by the actual project than the average individual expert, i.e., the group discussions led to better estimates than a mechanical combination of the individual estimates. The groups{\textquoteright} ability to identify more project activities is among the possible explanations for this reduction of bias.},
  booktitle = {The 15th Annual Workshop of the Psychology of                   Programming Interest Group (PPIG 2003)},
  pages = {285-296},
  publisher = {-},
  address = {Keele University, UK}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2003.a,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M},
  title = {A Review of Surveys on Software Effort Estimation},
  year = {2003},
  abstract = {This paper tries to summarize estimation knowledge through a review of surveys on software effort estimation. Main findings were that: (1) Most projects (60-80\%) encounter effort and/or schedule overruns. The overruns, however, seem to be lower than the overruns reported by some consultancy companies. For example, Standish Group{\textquoteright}s {\textquoteleft}Chaos Report{\textquoteright} describes an average cost overrun of 89\%, which is much higher than the average overruns found in other surveys, i.e., 30-40\%. (2) The estimation methods in most frequent use are expert judgment-based. A possible reason for the frequent use of expert judgment is that there is no evidence that formal estimation models lead to more accurate estimates. (3) There is a lack of surveys including extensive analyses of the reasons for effort and schedule overruns.},
  booktitle = {IEEE International Symposium on Empirical                   Software Engineering (ISESE 2003)},
  pages = {223-230},
  publisher = {IEEE Computer Society},
  address = {Rome, Italy},
  note = {ISBN 0-7695-2002-2}
}

@Article{SE.4.Joergensen.2004.d,
  author = {J{\o}rgensen, M},
  title = {Top-Down and Bottom-Up Expert Estimation of Software Development Effort},
  year = {2004},
  abstract = {Expert estimation of software development effort may follow top-down or bottom-up strategies, i.e., the total effort estimate may be based on properties of the project as a whole and distributed over project activities (top-down) or calculated as the sum of the project activity estimates (bottom-up). The study reported in this paper examines differences between these two strategies based on measurement and video recording of the discussions of seven estimation teams. Each estimation team applied a top-down estimation strategy on one project and a bottom-up estimation strategy on another. The results from the study contribute, we believe, to an improved understanding of when to apply top-down and when to apply bottom-up estimation strategies.},
  journal = {Journal of Information and Software Technology},
  volume = {46},
  number = {1},
  pages = {3--16}
}

@Article{SE.4.Joergensen.2004.g,
  author = {J{\o}rgensen, M and Carelius, G J},
  title = {An Empirical Study of Software Project Bidding},
  year = {2004},
  abstract = {The study described in this paper reports from a real-life bidding process in which 35 companies were bidding for the same contract. The bidding process consisted of two separate phases: A pre-study phase and a bidding phase. In the pre-study phase 17 of the 35 bidding companies provided rough non-binding price indications based on a brief, incomplete description of user requirements. In the bidding phase, all 35 companies provided bids based on a more complete requirement specification that described a software system with substantially more functionality than the system indicated in the pre-study phase. The main result of the study is that the 17 companies involved in the pre-study phase presented bids that were on average about 70\% higher than the bids of the other companies. We propose a preliminary theory that has the potential to explain this difference. This preliminary theory is based, amongst other things, on the {\textquotedblleft}precautionary bidding effect{\textquotedblright} found in auctioning studies. Two important implications of our preliminary theory are that: 1) Software clients tend to achieve better price/uncertainty relationships, i.e., better prices, when the requirement uncertainty perceived by the bidders is low. 2) Software clients should not request early price indications based on limited and uncertain information when the final bids can be based on more complete and reliable information.},
  journal = {IEEE Transactions of Software                      Engineering},
  volume = {30},
  number = {12},
  pages = {953--969}
}

@Article{SE.4.Joergensen.2004.h,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Reasons for Software Effort Estimation Error: Impact of Respondents Role, Information Collection Approach, and Data Analysis method},
  year = {2004},
  abstract = {This study is a first step towards better processes of understanding why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through: (1) Interviews with estimation responsible employees in different roles, (2) Estimation experience reports from 68 completed projects, and, (3) Statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons for estimation error that were given. We found, for example, a strong tendency to perceive factors outside the respondents{\textquoteright} own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents{\textquoteright} own control, and were determined by the estimators{\textquoteright} skill or experience. This bias in types of reason means that the collection only of project managers{\textquoteright} viewpoints will not yield balanced models of reasons for estimation error. Unfortunately, previous studies on reasons for estimation error have tended to collect information from project managers only. We recommend that software companies combine estimation error information from in-depth interviews with stakeholders in all relevant roles, estimation experience reports, and results from statistical analyses of project characteristics.},
  journal = {IEEE Transactions of Software Engineering},
  volume = {30},
  number = {12},
  pages = {993--1007}
}

@Article{SE.4.Joergensen.2004.f,
  author = {J{\o}rgensen, Magne},
  title = {Regression Models of Software Development Effort Estimation Accuracy and Bias},
  year = {2004},
  abstract = {Traditionally, software professionals are requested to provide
minimum-maximum intervals to indicate the uncertainty of their effort
estimates. In this paper we claim that the traditional request is not optimal and
leads to over-optimistic views about the level of estimation uncertainty.
Instead, we propose, a person different from the estimator should identify
minimum and maximum values relevant for planning or bidding purposes and
request that the software professionals assess how likely it is that these values
are exceeded. Not only does this seem to increase realism, but it also leads to
more useful uncertainty assessments. Our claims are based on the results of a
previously reported experiment and field studies in two companies. The two
software companies were instructed to apply the traditional, and our
alternative, framing on random samples of their projects. In total, we collected
information about 47 projects applying the traditional framing and 23 projects
applying the alternative framing.},
  journal = {Journal of Empirical Software Engineering},
  volume = {9},
  number = {4},
  pages = {297--314}
}

@Article{SE.4.Joergensen.2004.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {The impact of customer expectation on software development effort estimates},
  year = {2004},
  abstract = {The results from the study described in this paper suggest that customer expectations of a project{\textquoteright}s total cost can have a very large impact on human judgment-based estimates (expert estimates) of the most likely use of software development effort. The information that the customer expectations are not valid estimation information did not remove the impact. Surprisingly, the estimators did not notice this impact or assessed it to be low. An implication of the results is that the provision of realistic project estimate of most likely use of effort may require that the estimators do not know the customer{\textquoteright}s expectations of the total cost of the project.},
  journal = {International Journal of Project Management},
  volume = {22},
  number = {4},
  pages = {317--325}
}

@Article{SE.4.Joergensen.2004.a,
  author = {J{\o}rgensen, M and Teigen, K H and Mol{\o}kken-{\O}stvold, K J},
  title = {Better sure than safe? Overconfidence in judgment based software development effort prediction intervals},
  year = {2004},
  abstract = {The uncertainty of a software development effort estimate can be indicated through a prediction interval, i.e., the estimated minimum and maximum effort corresponding to a specific confidence level. For example, a project manager may be {\textquotedblleft}90\% confident{\textquotedblright} or believe that is it {\textquotedblleft}very likely{\textquotedblright} that the effort required to complete a project will be between 8,000 and 12,000 work-hours. This paper describes results from four studies (Studies A-D) on human judgement (expert) based prediction intervals of software development effort. Study A examines the accuracy of the prediction intervals in real software projects. The results suggest that the prediction intervals were generally much too narrow to reflect the chosen level of confidence, i.e., that there was a strong over-confidence. Studies B, C and D try to understand the reasons for the observed over-confidence. Study B examines the possibility that the over-confidence is related to type of experience or estimation process. Study C examines the possibility that the concept of confidence level is difficult to interpret for software estimators. Finally, Study D examines the possibility that there are unfortunate feedback mechanisms that reward over-confidence.},
  journal = {Journal of Systems and Software},
  volume = {70},
  number = {1-2},
  pages = {79--93}
}

@Article{SE.4.Joergensen.2004.c,
  author = {J{\o}rgensen, M},
  title = {A Review of Studies on Expert Estimation of Software Development Effort},
  year = {2004},
  abstract = {This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher{\textquoteright}s search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following twelve expert estimation {\textquotedblleft}best practice{\textquotedblright} guidelines are evaluated through the review: 1) Evaluate estimation accuracy, but avoid high evaluation pressure, 2) Avoid conflicting estimation goals, 3) Ask the estimators to justify and criticize their estimates, 4) Avoid irrelevant and unreliable estimation information, 5) Use documented data from previous development tasks, 6) Find estimation experts with relevant domain background and good estimation records, 7) Estimate top-down and bottom-up, independently of each other, 8) Use estimation checklists, 9) Combine estimates from different experts and estimation strategies, 10) Assess the uncertainty of the estimate, 11) Provide feedback on estimation accuracy and development task relations, and, 12) Provide estimation training opportunities. We found supporting evidence for all twelve estimation principles, and provide suggestions on how to implement them in software organizations.},
  journal = {Journal of Systems and Software},
  volume = {70},
  number = {1-2},
  pages = {37--60}
}

@Article{SE.4.Moloekken-Oestvold.2004.b,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M},
  title = {Group Processes in Software Effort Estimation},
  year = {2004},
  abstract = {The effort required to complete software projects is often estimated, completely or partially, using the judgement of experts, whose assessment may be biased. In general, such bias as there is seems to be towards estimates that are overly optimistic. The degree of bias varies from expert to expert, and seems to depend on both conscious and unconscious processes. One possible approach to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment in which experts with different backgrounds combined their estimates in group discussion. First, twenty software professionals were asked to provide individual estimates of the effort required for a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate via the pooling of knowledge in discussion. We found that the groups submitted less optimistic estimates than the individuals. Interestingly, the group discussion-based estimates were closer to the effort expended on the actual project than the average of the individual expert estimates were, i.e., the group discussions led to better estimates than a mechanical averaging of the individual estimates. The groups{\textquoteright} ability to identify a greater number of the activities required by the project is among the possible explanations for this reduction of bias.},
  journal = {Empirical Software Engineering},
  volume = {9},
  number = {4},
  pages = {315--334}
}

@Article{SE.4.Joergensen.2004.e,
  author = {J{\o}rgensen, M},
  title = {Increasing Realism in Effort Estimation Uncertainty Assessments: It Matters How You Ask},
  year = {2004},
  abstract = {Traditionally, software professionals are requested to provide
minimum-maximum intervals to indicate the uncertainty of their effort
estimates. In this paper we claim that the traditional request is not optimal and
leads to over-optimistic views about the level of estimation uncertainty.
Instead, we propose, a person different from the estimator should identify
minimum and maximum values relevant for planning or bidding purposes and
request that the software professionals assess how likely it is that these values
are exceeded. Not only does this seem to increase realism, but it also leads to
more useful uncertainty assessments. Our claims are based on the results of a
previously reported experiment and field studies in two companies. The two
software companies were instructed to apply the traditional, and our
alternative, framing on random samples of their projects. In total, we collected
information about 47 projects applying the traditional framing and 23 projects
applying the alternative framing.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {4},
  pages = {209--217}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2004.a,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M and Tanilkan, S S and Gallis, H and Lien, A C and Hove, S E},
  title = {A Survey on Software Estimation in the Norwegian Industry},
  year = {2004},
  abstract = {This paper seeks to provide an overview of the estimation methods that software companies apply to estimate their projects, why those methods are chosen, and how accurate they are. In order to improve software estimation accuracy, such knowledge is essential. We conducted an in-depth survey, where information was collected through structured interviews with senior managers from 18 different companies and project managers of 52 different projects. We collected and analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, completeness of delivered functionality and other estimation related information. Our results suggest, for example, that average effort overruns are 41\%, that the software estimation performance has not changed much the last 10-20 years, that expert estimation is the dominating estimation method, that estimation accuracy is not much impacted by use of formal estimation models, and that software managers tend to believe that the estimation accuracy of their company is better than it actually is.},
  booktitle = {10th International Software Metrics Symposium (METRICS 2004)},
  pages = {208--219},
  publisher = {IEEE Computer Society Press},
  address = {Chicago, USA}
}

@Inproceedings{SE.5.Moloekken-Oestvold.2004.b,
  author = {Mol{\o}kken-{\O}stvold, K J and Lien, A C and J{\o}rgensen, M and Tanilkan, S S and Gallis, H and Hove, S E},
  title = {Does Use of Development Model Affect Estimation Accuracy and Bias?},
  year = {2004},
  abstract = {Objective. To investigate how the use of incremental and evolutionary development models affects the accuracy and bias of effort and schedule estimates of software projects. Rationale. Advocates of incremental and evolutionary development models often claim that use of these models results in improved estimation accuracy. Design of study. We conducted an in-depth survey, where information was collected through structured interviews with 22 software project managers in 10 different companies. We collected and analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, completeness of delivered functionality and other estimation related information. Results. We found no impact from the development model on the estimation approach. However, we found that incremental and evolutionary projects were less prone to effort overruns. The degree of delivered functionality and schedule estimation accuracy, on the other hand, were seemingly independent of development model. Conclusion. The use of incremental and evolutionary development models may reduce the chance of effort overruns.},
  booktitle = {Product Focused Software Process                      Improvement: 5th International Conference                      (PROFES 2004)},
  pages = {17--29},
  publisher = {Springer-Verlag},
  address = {Kansai Science City, Japan},
  series = {Lecture Notes in Computer Science},
  note = {ISBN: 3-540-21421-6}
}

@Inproceedings{SE.5.Kitchenham.2004,
  author = {Kitchenham, B and Dyb{\r a}, T and J{\o}rgensen, M},
  title = {Evidence-based Software Engineering},
  year = {2004},
  abstract = {Objective: Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. 
Method: We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). 
Results: EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. 
Conclusions: Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.},
  booktitle = {International Conference on Software Engineering (ICSE'04)},
  pages = {273--281},
  publisher = {IEEE Computer Society, Washington DC, USA},
  address = {Edinburgh}
}

@Inproceedings{SE.5.Joergensen.2004.b,
  author = {J{\o}rgensen, M and Mol{\o}kken-{\O}stvold, K J},
  title = {Eliminating Over-Confidence in Software Development Effort Estimates},
  year = {2004},
  abstract = {Minimum-maximum effort intervals are applied in the planning of
software development projects in order to, among other things, determine the
contingency buffer. Several studies suggest that judgment-based minimummaximum
intervals are based on a systematic over-confidence in the accuracy
of the effort estimates. In this paper, we investigate whether the possession by
estimators of information about previous estimation error for similar projects
reduces this over-confidence. Nineteen realistically composed estimation teams
provided minimum-maximum effort intervals for the same software project.
Ten of the teams (Group A) received no instructions about the uncertainty
assessment process. The remaining nine teams (Group B) were instructed to
begin the minimum-maximum effort interval assessment by recalling the
distribution of estimation error for similar projects. We found that the recall of
the error distribution of the Group B teams did have an impact, but mainly on
the assessment of the estimated minimum effort, not on the maximum effort.
We discuss reasons for this finding and provide recommendations.},
  booktitle = {Conference on Product Focused Software                      Process Improvement},
  pages = {174--184},
  publisher = {Springer-Verlag},
  address = {Japan},
  series = {Lecture Notes in Computer Science}
}

@Inproceedings{SE.5.Joergensen.2004.c,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Generalization and Theory-Building in Software Engineering Research},
  year = {2004},
  abstract = {The main purpose of this paper is to generate discussions
which may improve how we conduct empirical software
engineering studies. Our position is that statistical
hypothesis testing plays a too large role in empirical
software engineering studies. The problems of applying
statistical hypothesis testing in empirical software
engineering studies is illustrated by the finding: Only 3
out of the 47 studies in Journal of Empirical Software
Engineering which applied statistical hypothesis testing,
were able to base their statistical testing on well-defined
populations and random samples from those populations.
The frequent use of statistical hypothesis testing may also
have had unwanted consequences on the study designs,
e.g., it may have contributed to a too low focus on theory
building. We outline several steps we believe are useful
for a change in focus from {\textquotedblleft}generalizing from a random
sample to a larger population{\textquotedblright} to {\textquotedblleft}generalizing across
populations through theory-building{\textquotedblright}},
  booktitle = {Empirical Assessment in Software Engineering (EASE2004)},
  editor = {unknown},
  pages = {29--36},
  publisher = {IEE Proceedings},
  isbn = {0 86341 435 4}
}

@Techreport{SE.7.Moloekken-Oestvold.2004,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M and Tanilkan, S S and Gallis, H and Lien, A C and Hove, S E},
  title = {Project Estimation in the Norwegian Software Industry -- A Summary},
  year = {2004},
  abstract = {This report provides an overview of the results obtained from a survey on project estimation in Norwegian software companies. The survey was conducted between February and November 2003. The main results are:


{\textbullet} We observed that 76\% of the projects used more effort than estimated, while 19\% used less. The average effort overrun was 41\%. 


{\textbullet} Average effort overrun was 67\% in projects with a public client, compared to an average effort overrun of 21\% for projects with a private client. 


{\textbullet} Projects that used an incremental or evolutionary development approach had an average effort overrun of 24\%, as opposed to the average 55\% overrun for projects that used a waterfall-based development approach. 


The frequency and magnitude of effort overruns found in this survey seems to be similar to results reported from surveys conducted in other countries in the past 20 years. The observed differences in effort overruns between private and public projects may be caused by differences in bidding procedures, level of client involvement or acceptance procedures. 

In order to reduce the risk of effort overruns, software companies should: 


{\textbullet} Focus on analyzing their own estimation performance, and invest in estimation improvement (for instance through experience databases or work breakdown structures). 


{\textbullet} Differentiate risk buffers based on the type of the customer, development approach and the size of the project. 


{\textbullet} Try to establish an {\textquotedblleft}as close as possible{\textquotedblright} dialogue with the customers (e.g. through an incremental development approach). 


All companies should analyse completed projects, in order to benchmark their performance. This allows for improvement efforts to be identified.},
  institution = {Simula Research Laboratory},
  type = {Research Report},
  number = {2004-03},
  note = {Results from the BEST-Pro (Better Estimation
                      of Software Tasks and Process Improvement)
                      survey}
}

@Misc{SE.8.Moloekken-Oestvold.2004.j,
  author = {Mol{\o}kken-{\O}stvold, K J and J{\o}rgensen, M and S{\o}rgaard, P},
  title = {Offentlig fare},
  year = {2004},
  abstract = {},
  howpublished = {Newspaper article in Computerworld}
}

@Misc{SE.8.Joergensen.2004.a,
  author = {J{\o}rgensen, M},
  title = {Uncertainty assessments in software development projects},
  year = {2004},
  abstract = {},
  howpublished = {Presentation at: Causality,                      Uncertainty \& Ignorance, 3rd International                      Summer School 2004, University of Konstanz,                      Germany}
}

@Misc{SE.8.Joergensen.2004.b,
  author = {J{\o}rgensen, M},
  title = {Software cost overruns - how large are they and how should they be measured?},
  year = {2004},
  abstract = {},
  howpublished = {Invited talk at: 60th seminar of the Software Reliability and Metrics Club, London}
}

@Misc{SE.8.Joergensen.2004.c,
  author = {J{\o}rgensen, M},
  title = {Lav pris gir lav kvalitet},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.d,
  author = {J{\o}rgensen, M},
  title = {Erfaring er ikke ekspertise},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.e,
  author = {J{\o}rgensen, M},
  title = {Hvor gode er v{\r a}re it-akt{\o}rer},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.f,
  author = {J{\o}rgensen, M},
  title = {Kunsten {\r a} m{\r a}le},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.g,
  author = {J{\o}rgensen, M},
  title = {Simpsons paradoks},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.h,
  author = {J{\o}rgensen, M},
  title = {Optimisme: styrke og svakhet},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.i,
  author = {J{\o}rgensen, M},
  title = {Produktet som ikke kan m{\r a}lbindes},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld, Under lupen}
}

@Misc{SE.8.Joergensen.2004.k,
  author = {J{\o}rgensen, M},
  title = {Overskridelser i offentlige IT-prosjekter},
  year = {2004},
  abstract = {},
  howpublished = {Computerworld: Kronikk}
}

@Misc{SE.8.Joergensen.2004.l,
  author = {J{\o}rgensen, M},
  title = {Hvordan f{\r a} tak i den reelle usikkerheten i IT-prosjekter},
  year = {2004},
  abstract = {},
  howpublished = {Den Norske Dataforening: Presentasjon p{\r a} seminar}
}

@Misc{SE.8.Joergensen.2004.m,
  author = {J{\o}rgensen, M},
  title = {IT-sprekk skyldes urealistiske kunder},
  year = {2004},
  abstract = {},
  howpublished = {digi.no: Innlegg}
}

@Misc{SE.8.Joergensen.2004.n,
  author = {J{\o}rgensen, M},
  title = {Estimering av IT-prosjekter},
  year = {2004},
  abstract = {},
  howpublished = {Simula-seminar: Presentasjon}
}

@Misc{SE.8.Joergensen.2004.o,
  author = {J{\o}rgensen, M},
  title = {Software Cost Overruns - How Large Are They? Critical comments on the CHAOS Report},
  year = {2004},
  abstract = {The Standish Group (www.standishgroup.com) claims that the results of their CHAOS research, i.e., their large-scaled surveys conducted in 1994, 1996, 1998, 2000 and 2002, are the most widely quoted statistics in the IT industry. This may very well be true. Quoted with particular frequency are the results described in the 1994 CHAOS report [1], probably because the 1994 CHAOS report is free and easily can be downloaded from the web. The results of that report have been used in several recent governmental reports, project reviews, and research studies. Examples are the PITAC 1999 report [2] and the cost estimation study described in [3]. An important result from the 1994 CHAOS research is the reported 189\% average cost overrun of so-called challenged projects, i.e., projects not on time, on cost, and with all specified functionality. In this paper we argue that the 189\% average cost overrun number, as it is commonly interpreted, is not consistent with results of other cost accuracy surveys and probably far too high to reflect the average cost overrun in that period. The measures and the research method of the CHAOS survey are insufficiently described to evaluate the quality of the results, e.g., there are many possible interpretations of what is meant by {\textquoteleft}cost overrun{\textquoteright} in the CHAOS reports. We should therefore cease to trust the 189\% average cost overrun as a reference point for performance of software projects until such time as the Standish Group disclose how they measure cost overrun and how they conduct their research.},
  howpublished = {SPIKE-seminar: Presentasjon}
}

@Inproceedings{Jorgensen.2005.1,
  author = {J{\o}rgensen, Magne and Gruschke, Tanja Milijana},
  title = {Industrial Use of Formal Software Cost Estimation Models: Expert Estimation in Disguise?},
  year = {2005},
  abstract = {The goal of this paper is to propose and evaluate the hypothesis that software cost estimates based on formal estimation models are frequently expert estimation in disguise, i.e., that the cost estimates are not as mechanically derived as prescribed and assumed. We test implications of the hypothesis through discussion of related work and an empirical study of function point-based effort estimation of software projects. The actual effort estimates of the projects were compared with the effort estimates one would expect if the formal function point model was applied as prescribed. We observed several large deviations between the actual and the mechanically derived effort estimates, which we interpret as indications of a strong impact from expert judgment. Important limitations of our study are that the hypothesis is formulated vaguely, that there is not much evidence available, and, that we may have had a tendency to bias our search towards supporting evidence. More studies are therefore needed, preferably from independent researchers. If our hypothesis is correct, implementation of formal software cost estimation models should include means to avoid unwanted effects of initial beliefs and irrelevant information.},
  booktitle = {Proceedings of EASE, Keele, UK, April 11-13},
  pages = {1-7},
  publisher = {Keele University}
}

@Inproceedings{Molokken-ostvold.2005.1,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and J{\o}rgensen, Magne and S{\o}rgaard, P{\r a}l and Grimstad, Stein},
  title = {Management of Public Software Projects: Avoiding Overruns},
  year = {2005},
  abstract = {Avoiding overruns is one of the major challenges in public software projects. In order to reduce overruns and, in turn, increase the chances of project success, it is essential to have an unbiased overview of the frequency and magnitude of overruns, as well as knowledge about factors which may prevent overruns. In a recent survey of the software industry it was found that the average magnitude of overruns in work-hours was 67\% for projects with a public client, compared to 21\% for projects with a private client. This paper presents results from two surveys of software companies and professionals that investigate the possible differences between public and private clients, and a review of research on public IT-projects. The results indicate that there are several properties of public projects on the political, organizational and individual level which may contribute to the large overruns. These include artificial deadlines based on political terms, regulations on procurement and development processes, and lack of technology and project management skills. The challenges appear to be common for several countries in the OECD region which have investigated this topic. In order to reduce overruns of public projects, both contractors and government officials have to address these issues.},
  booktitle = {Hawaiian International Conference on Business, May 25-28, Hawaii, USA},
  publisher = {Hawaiian International Conference on Business}
}

@Article{Dyba.2005.1,
  author = {Dyb{\r a}, Tore and Kitchenham, Barbara and J{\o}rgensen, Magne},
  title = {Evidence-based Software Engineering for Practitioners},
  year = {2005},
  abstract = {Objective: Our objective is to explain how practitioners in industry can use evidencebased
software engineering (EBSE) to assist decisions concerning the adoption of new
techniques. Rationale: Practitioners may make incorrect technology adoption decisions if
they do not consider scientific evidence about the efficacy of techniques. Method: We
adapt procedures used for evidence-based medicine to software engineering and discuss
how these procedures map to software process improvement. Results: We found EBSE
fitted well with current ideas concerning process improvement and that it could be an
important means for closing the gap between research and practice. However EBSE
presents difficulties for practitioners because current software engineering research is
limited and not reported in a manner that assists accumulation and evaluation of evidence.
Conclusions: EBSE has potential benefits for software engineering practitioners, but will
be hindered without changes to software engineering research.
Keywords: Evidence, empirical software engineering, evaluation.},
  journal = {IEEE Software},
  volume = {22},
  number = {1},
  pages = {58-65}
}

@Inproceedings{Jorgensen.2005.2,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Over-optimism in Software Development Projects: {\textquotedblleft}The winner{\textquoteright}s curse{\textquotedblright}},
  year = {2005},
  abstract = {},
  booktitle = {Proceedings of IEEE CONIELECOMP, Puebla, Mexico, February 28-March 2},
  pages = {280--285 },
  publisher = {IEEE Computer Society}
}

@Article{Jorgensen.2005.3,
  author = {J{\o}rgensen, Magne},
  title = {Practical guidelines for better support of expert judgement-based software effort estimation},
  year = {2005},
  abstract = {},
  journal = {IEEE Software},
  volume = {22},
  number = {3},
  pages = {57--63}
}

@Article{Teigen.2005.1,
  author = {Teigen, Karl Halvor and J{\o}rgensen, Magne},
  title = {When 90\% confidence intervals are only 50\% certain: On the credibility of credible intervals},
  year = {2005},
  abstract = {Estimates of confidence intervals for general knowledge items are usually too narrow. We report five experiments showing that people have much less confidence in these intervals than dictated by the assigned level of confidence. For instance, 90\% intervals can be associated with an estimated confidence of 50\% or less (and still lower hit rates). Moreover, interval width appears to remain stable over a wide range of instructions (high and low numeric and verbal confidence levels). This leads to a high degree of overconfidence for 90\% intervals, but less for 50\% intervals or for free choice intervals (without an assigned degree of confidence). To increase interval width one may have to ask exclusion rather than inclusion questions, for instance by soliciting {\textquotedblleft}improbable{\textquotedblright} upper and lower values (Experiment 4), or by asking separate {\textquotedblleft}more than{\textquotedblright} and {\textquotedblleft}less than{\textquotedblright} questions (Experiment 5). We conclude that interval width and degree of confidence have different determinants, and cannot be regarded as equivalent ways of expressing uncertainty.},
  journal = {Applied Cognitive Psychology},
  volume = {19},
  number = {4},
  pages = {455--475}
}

@Article{Molokken-ostvold.2005.4,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and J{\o}rgensen, Magne},
  title = {Expert Estimation of the Effort of Web-Development Projects: Are Software Professionals in Technical Roles More Optimistic Than Those in Non-Technical Roles?},
  year = {2005},
  abstract = {},
  journal = {Journal of Empirical Software Engineering},
  volume = {10},
  number = {1},
  pages = {7-30}
}

@Article{Karahasanovic.2005.2,
  author = {Karahasanovic, Amela and Anda, Bente Cecilie Dahlum and Arisholm, Erik and Hove, Siw Elisabeth and J{\o}rgensen, Magne and Sj{\o}berg, Dag I. K and Welland, Ray},
  title = {Collecting Feedback during Software Engineering Experiments},
  year = {2005},
  abstract = {Objective: To improve the qualitative data obtained from software engineering experiments by gathering feedback during experiments. Rationale: Existing techniques for collecting quantitative and qualitative data from software engineering experiments do not provide sufficient information to validate or explain all our results. Therefore, we would like a cost-effective and unobtrusive method of collecting feedback from subjects during an experiment to augment other sources of data. Design of study: We formulated a set of qualitative questions that might be answered by collecting feedback during software engineering experiments. We then developed a tool to collect such feedback from experimental subjects. This feedback-collection tool was used in four different experiments and we evaluated the usefulness of the feedback obtained in the context of each experiment. The feedback data was triangulated with other sources of quantitative and qualitative data collected for the experiments. Results: We have demonstrated that the collection of feedback during experiments provides useful additional data to: validate the data obtained from other sources about solution times and quality of solutions; check process conformance; understand problem solving processes; identify problems with experiments; and understand subjects{\textquoteright} perception of experiments. Conclusions: Feedback collection has proved useful in four experiments and we intend to use the feedback-collection tool in a range of other experiments to further explore the cost-effectiveness and limitations of this technique. It is also necessary to carry out a systematic study to more fully understand the impact of the feedback-collecting tool on subjects{\textquoteright} performance in experiments.},
  journal = {Empirical Software Engineering},
  volume = {10},
  number = {2},
  pages = {113-147}
}

@Misc{Jorgensen.2004.1,
  author = {J{\o}rgensen, Magne},
  title = {Uncertainty Assessments in Situations With Unknown Probabilities, Unknown Events and Partial Impact of Outcome},
  year = {2004},
  abstract = {},
  howpublished = {Misc},
  note = {Presentation at conference on "Causality, Uncertainty and Ignorance", Koblenz, August, 2004}
}

@Article{Jorgensen.2005.5,
  author = {J{\o}rgensen, Magne},
  title = {Evidence-Based Guidelines for Assessment of Software Development Cost Uncertainty},
  year = {2005},
  abstract = {},
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {11},
  pages = {942-954}
}

@Article{Molokken-ostvold.2005.5,
  author = {Mol{\o}kken-{\O}stvold, Kjetil Johan and J{\o}rgensen, Magne},
  title = {A Comparison of Software Project Overruns {\textendash} Flexible vs. Sequential Development Models},
  year = {2005},
  abstract = {Flexible software development models, e.g., evolutionary and incremental models, have become increasingly popular. Advocates of these models claim that among the benefits of these models are reduced software project overruns, which is one of the main challenges of software project management. This paper describes an in-depth survey of software development projects. The results support the claim that projects which employ a flexible development model experience less effort overruns than do those who employ a sequential model. The reason for the difference is not obvious. We found, for example, no variation in project size, estimation process, or delivered proportion of planned functionality between projects applying different types of development model. When the managers were asked to provide reasons for software overruns and/or estimation accuracy, the largest difference were that more of flexible projects cited good requirement specifications and good collaboration/communication with clients as contributing to accurate estimates.
},
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {9},
  pages = {754-766},
  note = {Project questions are dowloadable}
}

@Inproceedings{Gruschke.2005.1,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {Assessing Uncertainty of Software Development Effort Estimates:The Learning From Outcome Feedback},
  year = {2005},
  abstract = {To enable properly sized software projects budgets and
plans it is important to be able to assess the uncertainty of
the estimates of most likely effort required to complete the
projects. Previous studies show that software professionals
tend to be too optimistic about the uncertainty of their effort
estimates. This paper reports the results from a preliminary
study on the role of outcome feedback in the learning
process on effort estimation uncertainty assessment.
Software developers were given repeated and immediate
outcome feedback, i.e. feedback about the discrepancy
between the estimated most likely effort and the actual
effort, for the purpose of investigating how much, and
how, they improve (learn). We found that a necessary
condition for improvement of uncertainty assessments of
effort estimates may be the use of explicitly formulated
uncertainty assessment strategies. By contrast, intuitionbased
uncertainty assessment strategies may lead to no or
little learning.},
  booktitle = {11th International Software Metrics Symposium (METRICS 2005), Como, Italy, September 19-22},
  pages = {p.4 (1-10)},
  publisher = {IEEE},
  note = {Proceedings not published on paper, only on CD-ROM. Therefore, the page numbers are displayed differently than usual.}
}

@Inproceedings{Grimstad.2005.2,
  author = {Grimstad, Stein and J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {The Clients' Impact on Effort Estimation Accuracy in Software Development Projects},
  year = {2005},
  abstract = {It seems clear that there is no simple solution to improved predictability of software development projects. Over several decades various aspects of software development and their relationship to estimation accuracy have been investigated. This paper focus on one such relationship; the clients' impact on estimation accuracy in software development projects. Factors contributing to overruns as well as factors preventing overruns are investigated. Based on a literature review and a survey of 300 software professionals we find that: 1) It is software professionals' perception that clients impact estimation accuracy. Changed and new requirements are perceived as the clients' most frequent contribution to overruns, while overruns are prevented by the availability of competent clients and capable decision makers. 2) Survey results should not be used in estimation accuracy improvement initiatives without further analysis. Surveys typically identify direct and project specific causes for overruns, while substantial improvement is only possible when the underlying causes are understood. 

Keywords: software estimation, survey, client-vendor relationship, overrun causes},
  booktitle = {11th IEEE International Software Metrics Symposium (METRICS 2005), Como, Italy, September 19-22},
  pages = {3},
  publisher = {IEEE},
  edition = {0},
  series = {0}
}

@Inproceedings{Jorgensen.2005.4,
  author = {J{\o}rgensen, Magne and Kitchenham, Barbara and Dyb{\r a}, Tore},
  title = {Teaching Evidence-Based Software Engineering to University Students },
  year = {2005},
  abstract = {Evidence-based software engineering (EBSE) describes a process of identifying, understanding and evaluating findings from research and practice-based experience. This process aims at improving software engineering decisions. For the last three years, EBSE has been taught to university students at Hedmark University College, Rena, Norway. The motivation for the EBSE-course is that it is essential for the students, as future practitioners, to learn how to base important software engineering decisions on the systematic and critical evaluation of the best available evidence. The main purpose of this paper is to inspire and support other universities in their work on developing their own EBSE-courses. For this purpose we report on how our course has been organized and what lessons have been learned. There are currently no studies available on the effects of teaching EBSE and, as far as we know, only we have gained practice-based experience. To acquire more knowledge about the costs and benefits of teaching EBSE we hope that other universities will develop their own EBSE-courses and report their experience.},
  booktitle = {11th IEEE International Software Metrics Symposium, Como, Italy, September 19-22},
  publisher = { }
}

@Inproceedings{Jorgensen.2005.7,
  author = {J{\o}rgensen, Magne},
  title = {The "Magic Step" of Judgment-Based Software Effort Estimation},
  year = {2005},
  abstract = {},
  booktitle = {International Conference on Cognitive Economics},
  pages = {105--114},
  publisher = {New Bulgarian University, Sofia, August 5-8}
}

@Misc{Jorgensen.2005.10,
  author = {J{\o}rgensen, Magne and Karl Halvor, Teigen},
  title = {Kan vi unng{\r a} at "s{\r a} {\r a} si helt sikkert" bare betyr "60\% sikkert"?},
  year = {2005},
  abstract = {},
  howpublished = {Prosjektledelse (norsk fagtidsskrift for prosjektledere), Nr 2, s. 29-31}
}

@Misc{Jorgensen.2006.2,
  author = {J{\o}rgensen, Magne},
  title = {Software Cost Overruns - How Large Are They and How Should They be Measured? A critique of the Standish Group Chaos report.},
  year = {2006},
  abstract = {},
  howpublished = {Invited talk at the conference: OOP (Object oriented programming), M{\"u}nchen, January,}
}

@Misc{Jorgensen.2005.11,
  author = {J{\o}rgensen, Magne and Grimstad, Stein and Gruschke, Tanja Milijana},
  title = {The magic step in expert estimation of software cost. Why does 1000 feel more right than 1500?},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at: JavaZone, 2005}
}

@Misc{Jorgensen.2005.15,
  author = {J{\o}rgensen, Magne},
  title = {Valg av IT-leverand{\o}r basert p{\r a} laveste pris:VINNERENS FORBANNELSE},
  year = {2005},
  abstract = {},
  howpublished = {Presentation at the DnD conference Software 2005}
}

@Misc{Jorgensen.2005.16,
  author = {J{\o}rgensen, Magne},
  title = {Teaching evidence-based software engineering},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at EASE 2005}
}

@Misc{Jorgensen.2005.17,
  author = {J{\o}rgensen, Magne},
  title = {Hvordan unng{\r a} VINNERENS FORBANNELSE i anbudsrunder med mange tilbydere og sterk fokusering p{\r a} pris? },
  year = {2005},
  abstract = {},
  howpublished = {Presenation at SPIKE seminar, June}
}

@Misc{Jorgensen.2005.18,
  author = {J{\o}rgensen, Magne and Grimstad, Stein and Gruschke, Tanja Milijana},
  title = {"The magic step" i ekspertestimering av IT-utviklingskostnader. Hvorfor f{\o}les 1000 timeverk mer riktig enn 1500?},
  year = {2005},
  abstract = {},
  howpublished = {Invited talk at JavaZone 2005}
}

@Misc{Jorgensen.2005.19,
  author = {J{\o}rgensen, Magne},
  title = {Overoptimisme i IT-prosjekter: Hvorfor l{\ae}rer vi aldri?},
  year = {2005},
  abstract = {},
  howpublished = {Simula seminar with approx. 100 software professionals}
}

@Misc{Jorgensen.2005.20,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Hvorfor er vi overoptimistiske igjen og igjen og igjen ...},
  year = {2005},
  abstract = {},
  howpublished = {Presentation IT-Fornebu halvtimen}
}

@Misc{Jorgensen.2005.21,
  author = {J{\o}rgensen, Magne},
  title = {Practical Guidelines for Expert-Judgment-Based Software Effort Estimation},
  year = {2005},
  abstract = {},
  howpublished = {Invited article to SPIKE newsletter IMPROVE}
}

@Inproceedings{Jorgensen.2006.1,
  author = {J{\o}rgensen, Magne and Faugli, Bj{\o}rn},
  title = {Prediction of Optimistic Expert Judgment-Based Predictions},
  year = {2006},
  abstract = {(Paper selected for publication in JSS)},
  booktitle = {EASE 2006},
  editor = {B. Kitchenham},
  pages = {40-49},
  publisher = {BCS},
  address = {UK},
  isbn = {ISSN 1477-9358}
}

@Inproceedings{Mair.2005.2,
  author = {Mair, Carolyn and Shepperd, Martin and J{\o}rgensen, Magne},
  title = {An analysis of data sets used to train and validate cost prediction systems},
  year = {2005},
  abstract = {},
  booktitle = {International Conference on Software Engineering: Proceedings of the 2005 workshop on Predictor models in software engineering },
  pages = {1--6},
  publisher = {ACM Press},
  address = {New York, NY, USA },
  note = {ISBN:-159593-125-2 
}
}

@Article{Grimstad.2006.1,
  author = {Grimstad, Stein and J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Software Effort Estimation Terminology: The Tower of Babel},
  year = {2006},
  abstract = {This paper provides a review of how software development effort estimation terms are used in software engineering textbooks and research papers. We found that the term {\textquoteleft}effort estimate{\textquoteright} frequently is applied without any clarification of its meaning. It is therefore difficult to determine whether the authors{\textquoteright} intended interpretation is an estimate of {\textquoteleft}most likely effort{\textquoteright}, {\textquoteleft}planned effort{\textquoteright}, {\textquoteleft}budgeted effort{\textquoteright}, or something else. This is problematic as these terms are not equivalent and are used for different purposes. The lack of clarity of {\textquoteleft}effort estimate{\textquoteright} lowers the quality and interpretability of surveys on software effort estimation accuracy, i.e., it is not clear what the estimation accuracy results really mean. This reduces the estimation evaluation and learning possibilities. We suggest guidelines on how to reduce this terminology ambiguity. To the authors{\textquoteright} knowledge, this is the first published review of software cost estimation terminology.},
  journal = {Journal of Information and Software Technology},
  volume = {48},
  number = {4},
  pages = {302-310}
}

@Article{Jorgensen.2006.3,
  author = {J{\o}rgensen, Magne},
  title = {The Effects of the Format of Software Project Bidding Processes},
  year = {2006},
  abstract = {This study investigates how differences in format of the bidding process affect companies' bids for software projects. Thirty outsourcing companies from different Asian and European countries participated in the bidding for a software project. A participating company either started with the provision of a bid based on a reduced version of the specification and then continued with a bid based on the full specification (the Increase situation), or started with the full specification and then continued with the reduced one (the Decrease situation). We observed important differences in bids for the same project as a result of different bidding sequences. Our results constitute evidence that in situations similar to the one we studied, the client will typically select a provider with about a 40\% lower price in the Decrease situation than in the Increase situation. The difference in bids seems to be explained by whether the first bid was provided on the full or the reduced specification, not by the process of updating, i.e., increasing or decreasing, the bids. We warn against manipulation of the bidding processes to receive lower bids. This increases the risk of over-optimistic bids. Over-optimistic bids frequently have as a consequence "the winner's curse", which leads easily to "the client's curse".},
  journal = {International Journal of Project Management},
  volume = {24},
  number = {6},
  pages = {522-528 }
}

@Inproceedings{Grimstad.2006.2,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {A Framework for the Analysis of Software Cost Estimation Accuracy},
  year = {2006},
  abstract = {},
  booktitle = {ISESE 2006},
  editor = {Maldonado, Wohlin, Mendes	  },
  pages = {58--65},
  publisher = {ACM Press},
  isbn = {1-59593-218-6}
}

@Article{Jorgensen.2006.4,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {How Large Are Software Cost Overruns? Critical Comments on the Standish Group{\textquoteright}s CHAOS Reports},
  year = {2006},
  abstract = {The Standish Group reported in 1994 that the average cost overrun of software projects was as high as 189\%. This cost overrun number is used as input in recent governmental reports on software development and as benchmark for several recent projects{\textquoteright} estimation performances. It is therefore important that we can trust this number. More recent cost overrun results reported by the Standish Group and others, however, show much lower cost overrun. Does this mean that software companies have improved their estimation ability strongly the last 10 years? In this paper we argue that there are reasons to doubt the validity of the Standish Group{\textquoteright}s 1994 cost overrun results and that a continued use of these results may hinder progress.},
  journal = {Information and Software Technology},
  volume = {48},
  number = {4},
  pages = {297-301}
}

@Inbook{Sjoberg.2003.1,
  author = {Sj{\o}berg, Dag I. K and Anda, Bente Cecilie Dahlum and Arisholm, Erik and Dyb{\r a}, Tore and J{\o}rgensen, Magne and Karahasanovic, Amela and Vok{\a\'a}c, Marek},
  title = {Challenges and Recommendations When Increasing the Realism of Controlled Software Engineering Experiments},
  year = {2003},
  abstract = {An important goal of most empirical software engineering experiments is the transfer of the research results to industrial applications. To convince industry about the validity and applicability of the results of controlled software engineering experiments, the tasks, subjects and the environments should be as realistic as practically possible. Such experiments are, however, more demanding and expensive than experiments involving students, small tasks and pen-and-paper environments. This chapter describes challenges of increasing the realism of controlled experiments and lessons learned from the experiments that have been conducted at Simula Research Laboratory. },
  booktitle = {Empirical Methods and Studies in Software Engineering: Experiences from ESERNET},
  editor = {Reidar Conradi and Alf Inge Wang},
  publisher = {Springer},
  address = {Berlin / Heidelberg},
  series = {Lecture Notes in Computer Science, Volume 2765 },
  chapter = {Part II: Method Chapters},
  pages = {24--38},
  isbn = {978-3-540-40672-3}
}

@Inproceedings{Jorgensen.2006.7,
  author = {J{\o}rgensen, Magne},
  title = {Software Cost Estimation: When to Use Expert Judgment and When to Use Models},
  year = {2006},
  abstract = {},
  booktitle = {International Symposium on Forecasting},
  editor = {Antonio Garc{\a\'\i}a-Ferrer},
  pages = {53},
  publisher = {International Institute of Forecasters},
  isbn = {999-999-999}
}

@Inproceedings{Jorgensen.2006.8,
  author = {J{\o}rgensen, Magne},
  title = {A Preliminary Model of Judgment-based Project Software Effort Predictions},
  year = {2006},
  abstract = {An improvement of the software industry{\textquoteright}s software development effort estimation processes may benefit from a better understanding of the mental, partly unconscious, processes involved in estimating effort. This paper proposes and tests a theory potentially explaining essential parts of typical judgment-based effort estimation processes ({\textquotedblleft}expert estimation{\textquotedblright}). The theory is based on findings from the human judgment research literature and proposes that judgment-based effort estimation is based on: i) an early categorization of the project to be estimated, ii) a resistance towards a change of the chosen category, and, iii) a {\textquotedblleft}regression{\textquotedblright} of the effort estimate towards a reference value of the chosen category, where the amount of regression depends on the level of uncertainty of the project work. Implications of the theory are tested with results from three software effort estimation experiments. All examined studies confirmed the theory. There is, however, a strong need for more work, independent evidence and clearer description of scope and concepts part of the theory. Finally, we outline a study planned for further testing of essential parts of the theory.},
  booktitle = {IRNOP VIII, Project Research Conference},
  editor = {Lixiong Ou, Rodney Turner},
  pages = {661-668},
  publisher = {Publishing House of Electronci Industry},
  address = {Beijing},
  isbn = {7-121-03252-X}
}

@Inbook{Jorgensen.2006.9,
  author = {J{\o}rgensen, Magne and Sj{\o}berg, Dag},
  title = {Expert Estimation of Software Development Work},
  year = {2006},
  abstract = {},
  booktitle = {Software Evolution and Feedback: Theory and Practice},
  editor = {Nazim H.Madhavji, Juan Fernandez-Ramli, Dewayne Perry},
  publisher = {Wiley, ISBN-0-470-87180-6},
  chapter = {25},
  pages = {489-503}
}

@Misc{Jorgensen.2006.11,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {Wie gross sind die kosten{\"u}berschreitungen tats{\"a}chlich},
  year = {2006},
  abstract = {},
  howpublished = {Article in OBJEKTspektrum (in German)}
}

@Misc{Jorgensen.2006.12,
  author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil Johan},
  title = {How Large Are Software Cost Overruns? A Review of the 1994 CHAOS Report},
  year = {2006},
  abstract = {},
  howpublished = {Article in Software Practitioner (Vol 16, no 4\&5, p 13-14)}
}

@Misc{Jorgensen.2006.13,
  author = {J{\o}rgensen, Magne},
  title = {How to achieve lower software bids and why not to do this},
  year = {2006},
  abstract = {},
  howpublished = {Article in Improve (Newsletter 2-2006, p 1-2)}
}

@Misc{Jorgensen.2006.14,
  author = {J{\o}rgensen, Magne},
  title = {Herre i eget hus?},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, nr 24, s. 40.}
}

@Article{Jorgensen.2007.1,
  author = {J{\o}rgensen, Magne and Shepperd, Martin},
  title = {A Systematic Review of Software Development Cost Estimation Studies},
  year = {2007},
  abstract = {This paper aims to provide a basis for the improvement of software estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. Based on the review, we provide recommendations for future software cost estimation research: 1) Increase the breadth of the search for relevant studies, 2) Search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) Conduct more research on basic software cost estimation topics, 4) Conduct more studies of software cost estimation in real-life settings, 5) Conduct more studies on estimation methods commonly used by the software industry, and, 6) Conduct fewer studies that evaluate methods based on arbitrarily chosen data sets.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {33},
  number = {1},
  pages = {33-53}
}

@Article{Grimstad.2007.1,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {Inconsistency in Expert Judgment-based Estimates of Software Development Effort },
  year = {2007},
  abstract = {Effort estimation of software development work is partly based on non-mechanical and unconscious processes, i.e., human judgment. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions may lead to different effort estimates.  In this paper, we report from an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the tasks were estimated twice. We found a high degree of inconsistency in a subject{\textquoteright}s effort estimates of the same task. The mean difference of the effort estimates of the same task by the same estimator was as much as 71\%! Surprisingly, the software professionals{\textquoteright} estimation accuracy on previously completed development tasks did not correlate with their degree of estimation inconsistency in the experiment. We discuss reasons for and consequences of our findings. },
  journal = {Journal of Systems and Software},
  volume = {80},
  number = {11},
  pages = {1770--1777}
}

@Article{Gruschke.2007.1,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {Assessing Uncertainty of Software Development Effort Estimates: Learning from Outcome Feedback},
  year = {2008},
  abstract = {To enable properly-sized software project budgets and plans it is important to be able to assess the uncertainty of the estimates of the most likely effort required to complete the projects. Previous studies show that software professionals tend to be too optimistic about the uncertainty of their effort estimates, i.e., they tend to be overconfident. This paper reports the results from a study on the role of outcome feedback in the uncertainty assessment learning process. Software developers were given repeated and immediate outcome feedback about the discrepancy between the estimated most likely effort and the actual effort. We found that one condition for the improvement of uncertainty assessments of effort estimates may be the use of explicitly-formulated strategies for uncertainty assessment. By contrast, intuition-based uncertainty assessment strategies may lead to no or little improvement. This has the implication that we cannot expect much improvement in uncertainty assessment unless we train and instruct software developers to use the previous estimation accuracy experience as input to an explicitly defined uncertainty assessment process.},
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {17},
  number = {4},
  pages = {20-35}
}

@Article{Jorgensen.2007.2,
  author = {J{\o}rgensen, Magne},
  title = {Estimation of Software Development Work Effort:Evidence on Expert Judgment and Formal Models},
  year = {2007},
  abstract = {The main goal of the review presented in this paper is to examine when to use expert judgment, when to use formal models, and, when to combine these two approaches when estimating software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgment-based effort estimates was better than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgment-based effort estimates seem to be that: i) the estimation models are not calibrated to the organization using the model, and, ii) the experts possess important context information not included in the formal estimation models. The use of models, on the other hand, may be particularly useful in estimation situations believed to lead to stronger than usual degree of over-optimism, particularly very large projects. Five of the reviewed studies evaluated estimates based on a combination of expert judgment and model. In all five studies the average accuracy of combination-based effort estimates had similar or more accurate estimates than the average accuracy of the expert estimates and of the best model.},
  journal = {International Journal of Forecasting},
  volume = {23},
  number = {3},
  pages = {449-462}
}

@Article{Jorgensen.2007.3,
  author = {J{\o}rgensen, Magne and Faugli, Bj{\o}rn and Gruschke, Tanja Milijana},
  title = {Characteristics of Software Engineers with Optimistic Predictions},
  year = {2007},
  abstract = {This paper examines to which degree level of optimism in software engineers{\textquoteright} predictions is related to optimism on previous predictions, scores on tests of general level of optimism (explanatory style, life orientation and self-assessed optimism), development skill, confidence in accuracy of own predictions, and, ability to recall effort used on previous tasks. Results from four experiments suggest that more optimistic software engineers are characterized by more optimistic previous predictions, higher confidence in accuracy of own predictions, lower development skills, poorer ability to recall effort on previous tasks, and, higher optimism scores. A substantial part of the variation of level of optimism seems, however, to be random.},
  journal = {Journal of Systems and Software},
  volume = {80},
  number = {9},
  pages = {1472-1482}
}

@Inproceedings{Grimstad.2007.2,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {The Impact of Irrelevant Information on Estimates of Software Development Effort},
  year = {2007},
  abstract = {Software professionals typically estimate software development effort based on a requirement specification. Parts of this specification frequently contain information that is irrelevant to the estimation of the actual effort involved in the development of software. We hypothesize that effort-irrelevant information sometimes has a strong impact on effort estimates. To test this hypothesis, we conducted two controlled experiments with software professionals. In each of the experiments, the software professionals received specifications describing the same requirements. However, we gave one group of the software professionals a version of the requirement specification where we had included additional, effort-irrelevant, information. In both experiments we observed that the estimates of most likely effort increased when the estimates were based on requirement specifications that contained the information irrelevant to development effort. The results suggest that when estimation-irrelevant information is included as input to expert judgment-based estimation processes, the estimators find it difficult to distinguish between the estimation-relevant and the estimation-irrelevant information. A possible consequence of our findings is that estimation-irrelevant information should be removed from the requirement specification prior to the use of it as input to estimation work.},
  booktitle = {The Australian Software Engineering Conference (Paper received "Best Paper Award")},
  editor = {Doug Grant},
  pages = {359-368},
  publisher = {IEEE Computer Society},
  isbn = {999},
  note = {Awarded a prize for the conference's best paper.}
}

@Misc{Jorgensen.2006.5,
  author = {J{\o}rgensen, Magne},
  title = {Min venstre hjernehalvdel h{\o}rer ikke p{\r a} meg},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, No 42, p 40}
}

@Misc{Jorgensen.2006.6,
  author = {J{\o}rgensen, Magne},
  title = {L{\ae}ring av erfaring},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, No 30, p 36}
}

@Misc{Jorgensen.2006.10,
  author = {J{\o}rgensen, Magne},
  title = {En jungel av forskningsresultater},
  year = {2006},
  abstract = {},
  howpublished = {Article in Computerworld Norge, No 36, p 40}
}

@Misc{Jorgensen.2006.15,
  author = {J{\o}rgensen, Magne},
  title = {Software cost estimation research at Simula Research Laboratory},
  year = {2006},
  abstract = {},
  howpublished = {Seminar presentation at New Bulgarian University, Sofia}
}

@Misc{Jorgensen.2006.16,
  author = {J{\o}rgensen, Magne},
  title = {Hvor rasjonelle er v{\r a}re beslutninger ved valg av metoder og verkt{\o}y?},
  year = {2006},
  abstract = {},
  howpublished = {Keynote at SPIKE conference, Oslo, April 4}
}

@Misc{Jorgensen.2006.17,
  author = {J{\o}rgensen, Magne},
  title = {Overoptimisme i IT-prosjekter: Hvorfor l{\ae}rer vi aldri?},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at OMG-seminar, Oslo, April 19}
}

@Misc{Jorgensen.2006.19,
  author = {J{\o}rgensen, Magne},
  title = {Overoptimisme i IT-prosjekter?},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at Veivesenets 11th summer meeting, Svolv{\ae}r, Juni 1}
}

@Misc{Jorgensen.2006.20,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {N{\r a}r skal vi bruke hodet og n{\r a}r skal vi bruke estimeringsmodeller?},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at JavaZone, September 13}
}

@Misc{Jorgensen.2006.21,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Hvordan bli bedre til {\r a} estimere? (Inkludert NM i estimering)},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at JavaZone, September 13}
}

@Misc{Jorgensen.2006.22,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {How to avoid impact from irrelevant and misleading information on your cost estimates},
  year = {2006},
  abstract = {},
  howpublished = {Presentation at Simula's Annual Estimation Seminar, November 21}
}

@Misc{Jorgensen.2006.23,
  author = {J{\o}rgensen, Magne},
  title = {Hvorfor kostnadsestimater er overoptimistiske. Kundens ansvar er sterkt undervurdert!},
  year = {2006},
  abstract = {},
  howpublished = {Presenation at IKT-Norge seminar, Desember 5}
}

@Inproceedings{Gruschke.2006.5,
  author = {Gruschke, Tanja Milijana and J{\o}rgensen, Magne},
  title = {To know or not to know: when does feedback lead to better assessment of uncertainty of own beliefs?},
  year = {2006},
  abstract = {People are frequently overconfident about the accuracy of their own beliefs. The goal of this experiment is to examine whether, and if so under what conditions, very large amounts of feedback lead to better assessments of the uncertainty of one{\textquoteright}s own beliefs. Fifteen participants answered the same 960 general knowledge questions over two days. Questions were selected from the board game {\textquotedblleft}Who wants to be a millionaire?{\texttrademark}{\textquotedblright}. Each question had four answer alternatives. When an answer alternative had been chosen, the participants were asked to assess the probability that it was the correct answer. They did this by choosing from a list of predefined confidence intervals. The questions belonged to one out of six difficulty categories, as decided by the board game developers. Participants answered piles of 80 questions of the same difficulty at a time. Feedback about the correct answer was received immediately, and feedback about correspondence between {\textquotedblleft}hit rate{\textquotedblright} and confidence level was received immediately after a pile of questions had been answered. In addition, a summary of the first day{\textquoteright}s performance was provided at the start of Day 2. We found that thirteen out of the fifteen participants improved in the correspondence between hit rate and confidence level on the second day, which suggests that the feedback had an effect. The strongest improvement was achieved on questions with high {\textquotedblleft}global{\textquotedblright} difficulty, i.e., high difficulty as assessed by the board game developers, and low {\textquotedblleft}internal{\textquotedblright} difficulty, i.e., low difficulty as perceived by the participants.},
  booktitle = {IAREP/SABE congress (Behavioural Economics and Economic Psychology)},
  editor = {Christine Roland-L{\a\'e}vy},
  pages = {O-226 },
  publisher = {Elsevier},
  isbn = {999-999-999}
}

@Article{Simula.SE.9,
  author = {Hannay, Jo Erskine and J{\o}rgensen, Magne},
  title = {The Role of Artificial Design Elements in Software Engineering Experiments},
  year = {2008},
  abstract = {Increased realism in software engineering experiments is often promoted as an important means to increase generalizability and industrial relevance. In this context, artificiality, e.g., the use of constructed tasks in place of realistic tasks, is seen as a threat. In this article, we examine the opposite view, that deliberately introduced artificial design elements may increase knowledge gain and enhance both generalizability and relevance. In the first part of the article, we identify and evaluate arguments and examples in favor of, and against, deliberately introducing artificiality into software engineering experiments. In the second part of the article, we summarize a content analysis of articles reporting software engineering experiments published over the ten-year period 1993-2002. The analysis reveals a striving for realism and external validity, but little awareness of for what and when, various degrees of artificiality and realism are appropriate. We conclude that an increased awareness and deliberation in these respects is essential. However, arguments in favor of artificial design elements should not be used to justify studies that are badly designed or that have research questions of low relevance.},
  journal = {Transactions on Software Engineering},
  volume = {34},
  number = {2},
  pages = {242--259}
}

@Inproceedings{Simula.SE.13,
  author = {Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore and J{\o}rgensen, Magne},
  title = {The Future of Empirical Methods in Software Engineering Research},
  year = {2007},
  abstract = {We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research. },
  booktitle = {Future of Software Engineering (FOSE '07)},
  editor = {Briand L. and Wolf A.},
  pages = {358-378},
  publisher = {IEEE-CS Press},
  isbn = {432663526}
}

@Article{Simula.SE.112,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {How to Avoid Impact from Irrelevant and Misleading Information When Estimating Software Development Effort},
  year = {2008},
  abstract = {Software development effort estimates are reported to be highly inaccurate and systematically over-optimistic. We provide empirical evidence that suggests that this problem is caused, to some extent, by the influence of irrelevant and misleading information, e.g., information about the client{\textquoteright}s budget, present in the estimation material. The only really effective means of eliminating this influence is to avoid exposure to this type of information. Other means, such as more use of formal effort estimation models, improved analysis of requirement specifications, and better selection of estimators, had a positive effect but did not eliminate the influence. We propose process elements that are designed to avoid irrelevant and misleading information and illustrate how this process may lead to more realism in effort estimation.},
  journal = {IEEE Software},
  number = {May/June},
  pages = {78-83}
}

@Inproceedings{Simula.SE.113,
  author = {J{\o}rgensen, Magne},
  title = {Individual Differences in How Much People are Affected by Irrelevant and Misleading Information},
  year = {2007},
  abstract = {People differ in how much they update their beliefs based on new information. According to a recently proposed theory, individual differences in belief updating are, to some extent, determined by neurological differences, i.e., differences in the organization of the brain. The same neurological differences that affect belief updating may also affect handedness. In particular, more mixed-handed people may have a lower threshold for updating beliefs than strongly right-handed people. On the basis of the proposed theory, we hypothesize that mixed-handed software engineers will be more affected by irrelevant and misleading information when providing expert judgments. This hypothesis is tested in five experiments conducted in software engineering contexts. All five experiments supported the hypothesis and suggest that a low threshold for updating beliefs, as measured by degree of mixed-handedness correlates with inaccurate judgment in situations that contain irrelevant or misleading information. On the basis of the results, we argue that software engineering decisions, problem solving and estimation processes should take into account differences in individuals{\textquoteright} threshold for updating belief and not be based on the assumption that one {\textquotedblleft}process fits all{\textquotedblright}.},
  booktitle = {Second European Conference on Cognitive Science},
  editor = {Stella Vosniadou, Daniel Kayser, Athanassios Protopapaps},
  pages = {347-352},
  publisher = {Hellenic Cognitive Science Society},
  isbn = {978-1-84169-696-6}
}

@Misc{Simula.SE.182,
  author = {J{\o}rgensen, Magne},
  title = {Hvordan kundens anbudsprosess f{\r a}r deg til {\r a} estimere overoptimistisk og hva du kan gj{\o}re med det},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at JavaZone 2007}
}

@Misc{Simula.SE.183,
  author = {J{\o}rgensen, Magne},
  title = {Estimering av IT-prosjekter: Ekspertestimat eller modell?},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at Simula's annual estimation seminar}
}

@Misc{Simula.SE.184,
  author = {J{\o}rgensen, Magne},
  title = {Estimering av IT-prosjekter},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at Lindorff-seminar}
}

@Misc{Simula.SE.186,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {Software Development Effort Estimation},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at Making Waves-seminar}
}

@Misc{Simula.SE.187,
  author = {J{\o}rgensen, Magne},
  title = {Det kommer an p{\r a} hvordan man sp{\o}r},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.188,
  author = {J{\o}rgensen, Magne},
  title = {Overtro p{\r a} f{\o}rsteinntrykket},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.189,
  author = {J{\o}rgensen, Magne},
  title = {Hva slags informasjon stoler vi mest p{\r a}?},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.191,
  author = {J{\o}rgensen, Magne},
  title = {Hypnose, rasjonalisering og litt m{\r a}leteori},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.192,
  author = {J{\o}rgensen, Magne},
  title = {Kunder, leverand{\o}rer, yrkesetikk og informasjonsasymmetri},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.185,
  author = {J{\o}rgensen, Magne},
  title = {Software Cost Estimation:When to Use Estimation Models and When to Use Expert Judgment},
  year = {2007},
  abstract = {},
  howpublished = {Keynote at 1st Conference on Software Productivity Analysis and Cost Estimation (SPACE'07)}
}

@Misc{Simula.SE.180,
  author = {J{\o}rgensen, Magne},
  title = {Help! My Brain is Out of Control! Impact from Irrelevant and Misleading Information in Software Effort Estimation},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at BEKK-seminar}
}

@Misc{Simula.SE.181,
  author = {J{\o}rgensen, Magne},
  title = {Rasjonelle beslutninger ved valg av metoder og verkt{\o}y},
  year = {2007},
  abstract = {},
  howpublished = {Presentation at JavaZone 2007}
}

@Misc{Simula.SE.190,
  author = {J{\o}rgensen, Magne},
  title = {Er selvbeherskelsen din sliten og utrent?},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.193,
  author = {J{\o}rgensen, Magne},
  title = {Sannsynligvis vil noe usannsynlig skje},
  year = {2007},
  abstract = {},
  howpublished = {Article in Computerworld Norge}
}

@Misc{Simula.SE.194,
  author = {J{\o}rgensen, Magne and Grimstad, Stein},
  title = {How should we (not) design empirical studies of software development? },
  year = {2007},
  abstract = {},
  howpublished = {Invited presentation at the University of New South Wales, Sydney}
}

@Misc{Simula.SE.122,
  author = {Grimstad, Stein and J{\o}rgensen, Magne},
  title = {The impact of irrelevant information on inconsistency in expert judgment-based estimates of software development work},
  year = {2007},
  abstract = {Effort estimation is an important activity in software development and provides essential input to pricing, planning and budgeting processes. Unfortunately, many software effort estimates are inaccurate. The consequences of inaccurate estimates can be severe, e.g., budget-overruns, delayed time-to-market, and poor quality software. 

Most software effort estimation work is at least party based on expert judgment, i.e. non-mechanical and unconscious processes. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions sometimes lead to different effort estimates. We have conducted an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the sixty tasks were estimated twice. We found a surprisingly high degree of inconsistency in the software professionals{\textquoteright} effort. The mean difference of the effort estimates of the same task by the same estimator was as much 71\% (median 50\%) The correlation between the corresponding estimates was 0,7. Highly inconsistent effort estimates will, on average, be inaccurate and difficult to learn from. It is consequently important to focus estimation process improvement on consistency issues.

Evidence from other forecasting fields recommends that to reduce inconsistency, only the most important estimation information is used as input to the estimation work. We have empirically examined this advice{\textquoteright}s applicability to software effort estimation by analysing inter-estimator agreement in six software effort estimation experiments that report on the impact of including information of low or no relevance in the input to estimation work. The main findings are that inconsistency can increase when information of low or no relevance is present, and that this happens in spite of the software professionals knowing and accepting the lack of relevance. At present, the only safe way to remove the impact seems to be to remove the irrelevant information from the estimation material before it is given to the estimators. 
},
  howpublished = {International Symposium on Forecasting (ISF)}
}

@Inproceedings{Simula.SE.164,
  author = {J{\o}rgensen, Magne},
  title = {A Critique of How We Measure and Interpret the Accuracy of Software Development Effort Estimation},
  year = {2007},
  abstract = {This paper criticizes current practice regarding the measurement and interpretation of the accuracy of software development effort estimation. The shortcomings we discuss are related to: 1) the meaning of {\textquoteleft}effort estimate{\textquoteright}, 2) the meaning of {\textquoteleft}estimation accuracy{\textquoteright}, 3) estimation of moving targets, and 4)  assessment of the estimation process, and not only the discrepancy between the estimated and the actual effort, to evaluate estimation skill. It is possible to correct several of the discussed shortcomings by better practice. However, there are also inherent problems related to both laboratory and field analyses of the accuracy of software development effort estimation. It is essential that both software researchers and professionals are aware of these problems and their implications for the analysis of the measurement of effort estimation accuracy.},
  booktitle = {1st International Workshop on Software Productivity Analysis and Cost Estimation},
  editor = {Jacky Keung},
  pages = {15-22},
  publisher = {Information Processing Society of Japan},
  isbn = {ISBN 978-4-915256-72-1 C3040}
}

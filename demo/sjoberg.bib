
@Book{SE.2.Kirby.2001,
  editor = {Kirby, G NC and Dearle, A and Sj{\o}berg, Dag I. K},
  title = {9th International Workshop, POS-9},
  year = {2001},
  abstract = {},
  publisher = {Springer-Verlag},
  address = {Lillehammer, Norway},
  number = {2135},
  isbn = {3-540-42735-X}
}

@Article{SE.4.Arisholm.2001,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and J{\o}rgensen, M},
  title = {Assessing the Changeability of two Object-Oriented Design Alternatives - a Controlled Experiment},
  year = {2001},
  abstract = {},
  journal = {Empirical Software Engineering},
  volume = {6},
  number = {3},
  pages = {231-277}
}

@Article{SE.4.Joergensen.2001.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Impact of Effort Estimates on Software Project Work},
  year = {2001},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {43},
  number = {15},
  pages = {939-948}
}

@Article{SE.4.Joergensen.2001.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Software Process Improvement and Human Judgement Heuristics},
  year = {2001},
  abstract = {},
  journal = {Scandinavian Journal of Information Systems},
  volume = {13},
  number = {1},
  pages = {99-122}
}

@Inproceedings{SE.5.Anda.2001.a,
  author = {Anda, Bente Cecilie Dahlum and Dreiem, Hege and Sj{\o}berg, Dag Ingar Kondrup and J{\o}rgensen, Magne},
  title = {Estimating Software Development Effort Based on Use Cases - Experiences from Industry},
  year = {2001},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Several methods for estimating software development effort are based on attributes of a use case model. This paper reports the results of three industrial case studies on the application of a method for effort estimation based on use case points. The aim of this paper is to provide guidance for other organizations that want to improve their estimation process applying use cases. Our results support existing claims that use cases can be used successfully in estimating software development effort. The results indicate that the guidance provided by the use case points method can support expert knowledge in the estimation process. Our experience is also that the design of the use case models has a strong impact on the estimates. 
},
  booktitle = {4th International Conference on the Unified                   Modeling Language (UML2001)},
  editor = {Gogolla, M. and Kobryn, C.},
  pages = {487-502},
  publisher = {Springer-Verlag},
  address = {Toronto, Canada},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-42667-1}
}

@Inproceedings{SE.5.Anda.2001.b,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag Ingar Kondrup and J{\o}rgensen, Magne},
  title = {Quality and Understandability of Use Case Models},
  year = {2001},
  abstract = {Use case models are used in object-oriented analysis for capturing and describing the functional requirements of a system. Use case models are also used in communication between stakeholders in development projects. It is therefore important that the use case models are constructed in such a way that they support the development process and promote a good understanding of the requirements among the stakeholders. Despite this, there are few guidelines on how to construct use case models. This paper describes an explorative study where three different sets of guidelines were used for constructing and documenting use case models. An experiment with 139 undergraduate students divided into 31 groups was conducted. Each group used one out of the three sets of guidelines when constructing a use case model from an informal requirements specification. After completing the use case model, each student answered a questionnaire. The results of the experiment indicate that guidelines based on templates support the construction of use case models that are easier to understand for the readers, than guidelines without specific details on how to document each use case. The guidelines based on templates were also considered as the most useful when constructing use cases. In addition to better understandability, our experiment indicates that the guidelines based on templates result in better use case models regarding also other quality attributes. Our results further indicate that it may be beneficial to combine the template guidelines with another set of guidelines that focus on the documentation of the flow of events of each use case. Keywords. Object-oriented analysis, Requirements specification, Use Cases, UML, Understandability, Experiment},
  booktitle = {ECOOP 2001 - Object-Oriented Programming, 15th                   European Conference},
  editor = {J. Lindskov Knudsen},
  pages = {402-428},
  publisher = {Springer-Verlag},
  address = {Budapest, Hungary},
  series = {Lecture Notes in Computer Science},
  isbn = {3-540-42206-4}
}

@Inproceedings{SE.5.Joergensen.2001.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, D I. K},
  title = {Anchoring Effects: An Important Cause of Too Optimistic Effort Estimates in Software Development Projects},
  year = {2001},
  abstract = {},
  booktitle = {24nd IRIS Conference (Information Systems                   Research Seminar In Scandinavia)},
  editor = {-},
  publisher = {-},
  address = {Ulvik, Norway},
  isbn = {0}
}

@Inproceedings{SE.5.Joergensen.2001.b,
  author = {J{\o}rgensen, M and Indahl, U and Sj{\o}berg, D I. K},
  title = {Software Effort Estimation by Analogy and Regression Toward the Mean},
  year = {2001},
  abstract = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon {\textquotedblleft}regression toward the mean{\textquotedblright} (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  booktitle = {The Thirteenth International Conference on                   Software Engineering \& Knowledge Engineering                   (SEKE01)},
  editor = {-},
  pages = {253 -- 262 },
  publisher = {Elsevier Science Inc},
  address = {Buenos Aires, Argentina},
  isbn = {0}
}

@Inproceedings{SE.5.Karahasanovic.2001.a,
  author = {Karahasanovic, A and Sj{\o}berg, D I. K},
  title = {Visualizing Impacts of Database Schema Changes -- A Controlled Experiment},
  year = {2001},
  abstract = {Research in schema evolution has been driven by the need for more effective software development and maintenance. Finding impacts of schema changes on the applications and presenting them in an appropriate way are particularly challenging. We have developed a tool that finds impacts of schema changes on applications in object-oriented systems. This tool displays components (packages, classes, interfaces, methods and fields) of a database application system as a graph. Components potentially affected by a change are indicated by changing the shape of the boxes representing those components. Two versions of the tool are available. One version identifies affected parts of applications at the granularity of packages, classes, and interfaces, whereas the other version identifies affected parts at the finer granularity of fields and methods. This paper presents the design and results of a controlled student experiment testing these two granularity levels with respect to productivity and user satisfaction. There are indications that identifying impacts at the finer granularity can reduce the time needed to conduct schema changes and reduce the number of errors. Our results also show that the subjects of the experiment appreciated the idea of visualizing the impacts of schema changes.},
  booktitle = {2001 IEEE Symposium on Visual/Multimedia                   Approaches to Programming and Software                   Engineering},
  editor = {-},
  pages = {358-365},
  publisher = {IEEE Computer Society},
  address = {Stresa, Italy},
  isbn = {0-7695-0474-4}
}

@Inproceedings{SE.5.Karahasanovic.2001.c,
  author = {Karahasanovic, A and Sj{\o}berg, D I. K and J{\o}rgensen, M},
  title = {Data Collection in Software Engineering Experiments},
  year = {2001},
  abstract = {This paper presents ongoing work on developing a mechanism for automatic data collection during software engineering experiments. The importance of experiments in software engineering can hardly be overemphasised. Conducting such experiments raises the challenge of efficient and reliable data collection and analysis. During the experiment subjects (students, experienced programmers, etc.) perform tasks on the software engineering artefacts (e.g., design documents and code) using the technology under study in the given context (e.g. operating system). We are typically interested in data concerning subjects, their interaction with the technology and context, and changes of the software engineering artefacts. To conduct a series of experiments on software engineering technology (methods and tools), we envisage an experimental environment for data collection and analysis. As a first step, we have developed a logging mechanism that collects data about subjects of the experiment and their usage of the technology under study. We believe that our logging mechanism will alleviate data collection and increase the validity of experiments.},
  booktitle = {Managing Information Technology in a Global                   Economy, Information Resources Management                   Association International Conference IRMA 2001,                   Software Engineering Track},
  editor = {-},
  pages = {1027-1028},
  publisher = {Idea Group Publishing},
  address = {Toronto, Ontario Canada},
  isbn = {0}
}

@Inproceedings{SE.5.Sjoeberg.2001,
  author = {Sj{\o}berg, D I. K and Arisholm, Erik and J{\o}rgensen, M},
  title = {Conducting Experiments on Software Evolution},
  year = {2001},
  abstract = {},
  booktitle = {4th International Workshop on Principles of                   Software Evolution (IWPSE 2001)},
  editor = {Tetsuo Tamai and Mikio Aoyama and Keith                   Bennet},
  pages = {142-145},
  publisher = {-},
  address = {Vienna, Austria},
  isbn = {1-58113-508-4}
}

@Article{SE.4.Arisholm.2002,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and Carelius, G J and Lindsj{\o}rn, Y},
  title = {A Web-based Support Environment for Software Engineering Experiments},
  year = {2002},
  abstract = {The software engineering communities frequently propose new software engineering technologies, such as new development techniques, programming languages and tools, without rigorous scientific evaluation. One way to evaluate software engineering technologies is through controlled experiments where the effects of the technology can be isolated from confounding factors, i.e., establishing cause-effect relationships. For practical and financial reasons, however, such experiments are often quite unrealistic, typically involving students in a class-room environment solving small pen-and-paper tasks. A common criticism of the results of the experiments is their lack of external validity, i.e., that the results are not valid outside the experimental conditions. To increase the external validity of the experimental results, the experiments need to be more realistic. The realism can be increased using professional developers as subjects who conduct larger experimental tasks in their normal work environment. However, the logistics involved in running such experiments are tremendous. More specifically, the experimental materials (e.g., questionnaires, task descriptions, code and tools) must be distributed to each programmer, the progress of the experiment needs to be controlled and monitored, and the results of the experiment need to be collected and analyzed. To support this logistics for large-scale, controlled experiments, we have developed a web-based experiment support environment called SESE. This paper describes SESE, its development and the experiences from using it to conduct a large controlled experiment in industry.},
  journal = {Nordic Journal of Computing},
  volume = {9},
  number = {4},
  pages = {231-247}
}

@Article{SE.4.Joergensen.2002.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Impact of Experience on Maintenance Skills},
  year = {2002},
  abstract = {This study reports results from an empirical study of 54 software maintainers in the software maintenance department of a Norwegian company. The study addresses the relationship between amount of experience and maintenance skills. The findings were, amongst others: (1) While there may have been a reduction in the frequency of major unexpected problems from tasks solved by very inexperienced to medium experienced maintainers, additional years of general software maintenance experience did not lead to further reduction. More application specific experience, however, further reduced the frequency of major unexpected problems. (2) The most experienced maintainers did not predict maintenance problems better than maintainers with little or medium experience. (3) A simple one-variable model outperformed the maintainers{\textquoteright} predictions of maintenance problems, i.e., the average prediction performance of the maintainers seems poor. An important reason for the weak correlation between length of experience and ability to predict maintenance problems may be the lack of meaningful feedback on the predictions.},
  journal = {Software Maintenance: Research and Practice},
  volume = {14},
  number = {2},
  pages = {123-146}
}

@Inproceedings{SE.5.Anda.2002.b,
  author = {Anda, B and Sj{\o}berg, Dag I. K},
  title = {Towards an Inspection Technique for Use Case Models},
  year = {2002},
  abstract = {A use case model describes the functional requirements of a software system and is used as input to several activities in a software development project. The quality of the use case model therefore has an important impact on the quality of the resulting software product. Software inspection is regarded as one of the most efficient methods for verifying software documents. There are inspection techniques for most documents produced in a software development project, but no comprehensive inspection technique exists for use case models. This paper gives an overview of typical defects in use case models and proposes a checklist-based inspection technique for detecting such defects. This inspection technique was evaluated in two studies with undergraduate students as subjects. The results indicate that inspections are useful for detecting defects in use case models. However, more work is needed to establish appropriate inspection techniques.},
  booktitle = {Fourteenth IEEE Conference on Software                   Engineering and Knowledge Engineering (SEKE'02)},
  editor = {B Anda},
  pages = {127-134},
  publisher = {-},
  isbn = {1-58113-556-4}
}

@Inproceedings{SE.5.Arisholm.2002.c,
  author = {Arisholm, Erik and Sj{\o}berg, Dag I. K and Carelius, G J and Lindsj{\o}rn, Y},
  title = {SESE -- an Experiment Support Environment for Evaluating Software Engineering Technologies},
  year = {2002},
  abstract = {},
  booktitle = {NWPER'2002 (Tenth Nordic Workshop on                   Programming and Software Development Tools and                   Techniques)},
  editor = {-},
  pages = {81-98},
  publisher = {-},
  address = {Copenhagen, Denmark},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Joergensen.2002.d,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {A simple effort prediction interval approach},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the Conference on Achieving Quality in Information Systems},
  editor = {-},
  pages = {19-30},
  publisher = {-},
  address = {Venezia},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Karahasanovic.2002,
  author = {Karahasanovic, A and Sj{\o}berg, Dag I. K},
  title = {Visualising Impacts of Change in Evolving Object-Oriented Systems: An Explorative Study},
  year = {2002},
  abstract = {},
  booktitle = {Proceedings of the International Workshop on Graph-Based Tools GraBaTs'02},
  editor = {-},
  pages = {22-31},
  publisher = {Technische Universitat Berlin},
  address = {Barcelona, Spain},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Sjoeberg.2002,
  author = {Sj{\o}berg, Dag I. K and Anda, B and Arisholm, Erik and Dyb{\r a}, T and J{\o}rgensen, M and Karahasanovic, A and Koren, E and Vok{\a\'a}c, Marek},
  title = {Conducting Realistic Experiments in Software Engineering},
  year = {2002},
  abstract = {An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. 
This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes some of the experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.},
  booktitle = {ISESE'2002 (First International Symposium on                   Empirical Software Engineering)},
  editor = {not found},
  pages = {17-26},
  publisher = {IEEE Computer Society},
  isbn = {0-7695-1796-X}
}

@Article{SE.4.Joergensen.2003.a,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K and Indahl, U},
  title = {Software Effort Estimation by Analogy and Regression Toward the Mean},
  year = {2003},
  abstract = {Estimation by analogy is, simplified, the process of finding one or more projects that are similar to the one to be estimated and then derive the estimate from the values of these projects. If the selected projects have an unusual high or low productivity, then we should adjust the estimates toward productivity values of more average projects. The size of the adjustments depends on the expected accuracy of the estimation model. This paper evaluates one adjustment approach, based on the findings made by Sir Francis Galton in the late 1800s regarding the statistical phenomenon {\textquotedblleft}regression toward the mean{\textquotedblright} (RTM). We evaluate this approach on several data sets and find indications that it improves the estimation accuracy. Surprisingly, current analogy based effort estimation models do not, as far as we know, include adjustments related to extreme analogues and inaccurate estimation models. An analysis of several industrial software development and maintenance projects indicates that the effort estimates provided by software professionals, i.e., expert estimates, to some extent are RTM-adjusted. A student experiment confirms this finding, but also indicates a rather large variance in how well the need for RTM-adjustments is understood among software developers.},
  journal = {Journal of Systems and Software},
  volume = {68},
  number = {3},
  pages = {253-262}
}

@Article{SE.4.Joergensen.2003.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {An effort prediction interval approach based on the empirical distribution of previous estimation accuracy},
  year = {2003},
  abstract = {When estimating software development effort, it may be useful to describe the uncertainty of the estimate through an effort prediction interval (PI). An effort PI consists of a minimum and a maximum effort value and a confidence level. We introduce and evaluate a software development effort PI approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the effort PIs of new projects. First, we demonstrate the applicability and different variants of the approach on a data set of 145 software development tasks. Then, we experimentally compare the performance of one variant of the approach with human (software professionals{\textquoteright}) judgment and regression analysis-based effort PIs on a data set of 15 development tasks. Finally, based on the experiment and analytical considerations, we discuss when to base effort PIs on human judgment, regression analysis, or our approach.},
  journal = {Journal of Information and Software Technology},
  volume = {45},
  number = {3},
  pages = {123-136}
}

@Inproceedings{SE.5.Anda.2003,
  author = {Anda, B and Sj{\o}berg, Dag I. K},
  title = {Applying Use Cases to Design versus Validate Class Diagrams -- A Controlled Experiment Using a Professional Modelling Tool},
  year = {2003},
  abstract = {Several processes have been proposed for the transition from functional requirements to an object-oriented design, but these processes have been subject to little empirical validation. A use case driven development process is often recommended when applying UML. Nevertheless, it has been reported that this process leads to problems, such as the developers missing some requirements and mistaking requirements for design. This paper describes a controlled experiment, with 53 students as subjects, conducted to investigate two alternative processes for applying a use case model in an object-oriented design process. One process was use case driven, while the other was a responsibility-driven process in which the use case model was applied as a means of validating the resulting class diagram. Half of the subjects used the modelling tool Tau UML Suite from Telelogic; the other half used pen and paper. The results show that the validation process led to class diagrams implementing more of the requirements. The use case driven process did, however, result in class diagrams with a better structure. The results also show that those who used the modelling tool spent more time on constructing class diagrams than did those who used pen and paper. We experienced that it requires much more effort to organize an experiment with a professional modelling tool than with only pen and paper.},
  booktitle = {ISESE'2003 (Second International Symposium on                   Empirical Software Engineering)},
  editor = {not found},
  pages = {50-60},
  publisher = {IEEE Computer Society},
  address = {Rome, Italy},
  isbn = {not found}
}

@Inproceedings{SE.5.Conradi.2003.b,
  author = {Conradi, R and Dyb{\r a}, T and Sj{\o}berg, Dag I. K and Ulsund, T},
  title = {Lessons Learned and Recommendations from two Large Norwegian SPI Programmes},
  year = {2003},
  abstract = {},
  booktitle = {9th European Workshop on Software Process                   Technology (EWSPT 2003)},
  editor = {Oquendo},
  pages = {32-45},
  publisher = {Springer-Verlag},
  address = {Helsinki, Finland},
  series = {Lecture Notes in Computer Science},
  isbn = {978-3-540-40764-5}
}

@Article{Simula.SE.309,
  author = {Hannay, Jo Erskine and Dyb{\r a}, Tore and Arisholm, Erik and Sj{\o}berg, Dag},
  title = {The Effectiveness of Pair-Programming: A Meta-Analysis},
  year = {2008},
  abstract = {The meta-analysis provided in this article shows a small significant positive overall effect of pair programming (PP) compared with solo programming on quality, a medium  significant positive overall effect on duration, and a medium  significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on PP. A more detailed examination of the evidence suggests that PP is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. However, the higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that  greater attention should be given to moderating factors on the effect of PP.},
  journal = {Information and Software Technology}
}

@Article{Simula.SE.162,
  author = {Dyb{\r a}, Tore and Arisholm, Erik and Sj{\o}berg, Dag and Hannay, Jo Erskine and Shull, Forrest},
  title = {Are Two Heads Better than One? On the Effectiveness of Pair Programming},
  year = {2007},
  abstract = {},
  journal = {IEEE Software},
  volume = {24},
  number = {6},
  pages = {12-15}
}

@Inproceedings{SE.5.Syversen.2003,
  author = {Syversen, E and Anda, B and Sj{\o}berg, Dag I. K},
  title = {An Evaluation of Applying Use Cases to Construct Design versus Validate Design},
  year = {2003},
  abstract = {Use case models capture and describe the functional requirements of a software system. A use case driven development process, where a use case model is the principal basis for constructing an object-oriented design, is recommended when applying UML. There are, however, some problems with use case driven development processes and alternative ways of applying a use case model have been proposed. One alternative is to apply the use case model in a responsibility-driven process as a means to validate the design model. We wish to study how a use case model best can be applied in an object-oriented development process and have conducted a pilot experiment with 26 students as subjects to compare a use case driven process against a responsibility-driven process in which a use case model is applied to validate the design model. Each subject was given detailed guidelines on one of the two processes, and used those to construct design models consisting of class and sequence diagrams. The resulting class diagrams were evaluated with regards to realism, that is, how well they satisfied the requirements, size and number of errors. The results show that the validation process produced more realistic class diagrams, but with a larger variation in the number of classes. This indicates that the use case driven process gave more, but not always more appropriate, guidance on how to construct a class diagram The experiences from this pilot experiment were also used to improve the experimental design, and the design of a follow-up experiment is presented.},
  booktitle = {Hawaii International Conference on System                   Sciences (HICSS-36)},
  editor = {Eileen Dennis},
  publisher = {-},
  address = {Big Island, Hawaii},
  isbn = {0-7695-1874-5}
}

@Techreport{SE.7.Arisholm.2003.b,
  author = {Arisholm, Erik and Sj{\o}berg, D I. K},
  title = {A Controlled Experiment with Professionals to Evaluate the Effect of a Delegated versus Centralized Control Style on the Maintainability of Object-Oriented Software},
  year = {2003},
  abstract = {One fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a {\textquotedblleft}bad{\textquotedblright} object-oriented design. This paper presents a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate and senior professional consultants from several international consultancy companies were hired for one day to take part in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs having a centralized and delegated control style, respectively. 
The results show that the most skilled developers, in particular the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. 
Thus, the maintainability of object-oriented software depends to a large extent on the skill of the developers who are going to maintain it. The results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems that eventually will be maintained by juniors may be unwise, since the cognitive complexity of such {\textquotedblleft}expert{\textquotedblright} designs might be unmanageable for less skilled maintainers.},
  institution = {Simula Research Laboratory},
  type = {Simula Technical Report},
  number = {2003-6}
}

@Article{SE.4.Joergensen.2004.b,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {The impact of customer expectation on software development effort estimates},
  year = {2004},
  abstract = {The results from the study described in this paper suggest that customer expectations of a project{\textquoteright}s total cost can have a very large impact on human judgment-based estimates (expert estimates) of the most likely use of software development effort. The information that the customer expectations are not valid estimation information did not remove the impact. Surprisingly, the estimators did not notice this impact or assessed it to be low. An implication of the results is that the provision of realistic project estimate of most likely use of effort may require that the estimators do not know the customer{\textquoteright}s expectations of the total cost of the project.},
  journal = {International Journal of Project Management},
  volume = {22},
  number = {4},
  pages = {317--325}
}

@Article{SE.4.Arisholm.2004.b,
  author = {Arisholm, Erik and Sj{\o}berg, D I. K},
  title = {Evaluating the Effect of a Delegated versus Centralized Control Style on the Maintainability of Object-Oriented Software},
  year = {2004},
  abstract = {A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a {\textquotedblleft}bad{\textquotedblright} object-oriented design. 
This paper presents a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. 
The results show that the most skilled developers, in particular the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. 
Thus, the maintainability of object-oriented software depends, to a large extent, on the skill of the developers who are going to maintain it. These results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems may eventually pose difficulties unless they make an effort to keep the designs simple, as the cognitive complexity of {\textquotedblleft}expert{\textquotedblright} designs might be unmanageable for less skilled maintainers.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {8},
  pages = {521-534}
}

@Inproceedings{SE.5.Karahasanovic.2004,
  author = {Karahasanovic, A and Fjuk, A and Sj{\o}berg, Dag I. K and Thomas, R},
  title = {A Controlled Experiment to Evaluate the Reactivity and Usefulness of the Think-Aloud Tool},
  year = {2004},
  abstract = {This paper presents ongoing work on the evaluation of a tool for collecting feedback in large-scale software engineering and program comprehension experiments. Two most important questions one can ask about a new verbal procedure are whether it is useful and whether it is reactive. We have developed a tool, called the think-aloud tool, to support the think-aloud method in a variety of experiments. This tool instructs the subjects at regular intervals to write down their thoughts on a web based screen, as opposed to the oral expression of thoughts in conventional think-aloud method. The tool collects the subjects{\textquoteright} feedback together with timestamps. This paper describes an experiment designed to investigate whether the think-aloud tool affects performance of the subjects and whether it collects information beyond that collected in traditional think-aloud and retrospective reports. Furthermore, we will conduct a qualitative comparison of mouse and keying rates with think-aloud and retrospective reports. In addition, the experiment aims to check for any effect from using the think-aloud tool on the subject{\textquoteright}s task-related understanding and problem solving.},
  booktitle = {Information Resources Management Association                      International Conference (IRMA'04), Human                      Computer Interaction Track},
  editor = {unknown},
  pages = {1033--1034},
  publisher = {Idea Group Publishing},
  address = {New Orleans, Louisiana, USA},
  isbn = {0000-0000}
}

@Inproceedings{SE.5.Joergensen.2004.c,
  author = {J{\o}rgensen, M and Sj{\o}berg, Dag I. K},
  title = {Generalization and Theory-Building in Software Engineering Research},
  year = {2004},
  abstract = {The main purpose of this paper is to generate discussions
which may improve how we conduct empirical software
engineering studies. Our position is that statistical
hypothesis testing plays a too large role in empirical
software engineering studies. The problems of applying
statistical hypothesis testing in empirical software
engineering studies is illustrated by the finding: Only 3
out of the 47 studies in Journal of Empirical Software
Engineering which applied statistical hypothesis testing,
were able to base their statistical testing on well-defined
populations and random samples from those populations.
The frequent use of statistical hypothesis testing may also
have had unwanted consequences on the study designs,
e.g., it may have contributed to a too low focus on theory
building. We outline several steps we believe are useful
for a change in focus from {\textquotedblleft}generalizing from a random
sample to a larger population{\textquotedblright} to {\textquotedblleft}generalizing across
populations through theory-building{\textquotedblright}},
  booktitle = {Empirical Assessment in Software Engineering (EASE2004)},
  editor = {unknown},
  pages = {29--36},
  publisher = {IEE Proceedings},
  isbn = {0 86341 435 4}
}

@Article{Vokac.2004.2,
  author = {Vok{\a\'a}c, Marek and Tichy, W and Sj{\o}berg, Dag I. K and Arisholm, Erik and Aldrin, M},
  title = {A Controlled Experiment Comparing the Maintainability of Programs Designed with and without Design Patterns -- a Replication in a real Programming Environment},
  year = {2004},
  abstract = {Software "Design Patterns" seek to package proven solutions to design prob- 
lems in a form that makes it possible to find, adapt and reuse them. To support the 
industrial use of Design Patterns, this research investigates when, and how, using patterns is beneficial, and whether some patterns are more di{\textpm}cult to use than others. This paper describes a replication of an earlier controlled experiment on Design Patterns in maintenance, with major extensions. Experimental realism was increased by using a real programming environment instead of pen and paper, and paid professionals from multiple major consultancy companies as subjects. 


Measurements of elapsed time and correctness were analyzed using regression models and an estimation method that took into account the correlations present in the raw data. Together with on-line logging of the subjects' work, this made possible a better qualitative understanding of the results. 


The results indicate quite strongly that some patterns are much easier to understand and use than others. In particular, the Visitor pattern caused much confusion. Conversely, the patterns Observer and, to a certain extent, Decorator were grasped and used intuitively, even by subjects with little or no knowledge of patterns. 


The implication is that Design Patterns are not universally good or bad, but must be used in a way that matches the problem and the people. When approaching a program with documented Design Patterns, even basic training can improve both the speed and quality of maintenance activities
},
  journal = {Empirical Software Engineering},
  volume = {9},
  number = {3},
  pages = {149--195}
}

@Inproceedings{Benestad.2005.1,
  author = {Benestad, Hans Christian and Arisholm, Erik and Sj{\o}berg, Dag I. K},
  title = {How to Recruit Professionals as Subjects in Software Engineering Experiments},
  year = {2005},
  abstract = {Controlled experiments are the classical scientific method for identifying cause-effect relationships, and are complementary to case studies and surveys as a means to empirically evaluate information systems development methods and practices. Most controlled experiments that evaluate software development methods and practices use students as subjects. Using students as subjects is convenient. However, a common criticism of controlled experiments in which students are used as subjects is that it is difficult to generalize the results to industrial settings. Consequently, Simula Research Laboratory has included professionals as subjects in software engineering experiments. At present, more than 750 professional software developers from 46 software development organizations have participated in our experiments. From this experience we have identified three important principles for research groups that want to include professional software developers as subjects in controlled experiments. First, practical constraints must be considered when defining the target population of software developers.  Second, the participating organizations must be offered flexibility and value using a planned communication strategy, in order to ensure adequately sized representative samples of organizations and individuals. Third, to ensure long-term, relationships with the organizations, high professional and ethical standards must be employed.},
  booktitle = {IRIS (Information Systems Research in Scandinavia), August 6-9, Kristiansand Norway},
  editor = {Hustad, E., Munkvold, B.E., Rolland, K. and Flak, L.S.},
  publisher = {Department of Information Systems, Agder University College},
  isbn = {0000-0000}
}

@Article{Anda.2005.2,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag I. K},
  title = {Investigating the Role of Use Cases in the Construction of Class Diagrams},
  year = {2005},
  abstract = {Several approaches have been proposed for the transition from functional requirements to object-oriented design. In a use case-driven development process, the use cases are important input to the identification of classes and their methods. There is, however, no established, empirically validated technique for the transition from use cases to class diagrams. One recommended technique is to derive classes by analyzing the use cases. It has, nevertheless, been reported that this technique leads to problems, such as the developers missing requirements and mistaking requirements for design. An alternative technique is to identify classes from a textual requirements specification and subsequently apply the use case model to validate the resulting class diagram. This paper describes two controlled experiments conducted to investigate these two approaches to applying a use case model in an object-oriented design process. The first experiment was conducted with 53 students as subjects. Half of the subjects used a professional modelling tool; the other half used pen and paper. The second experiment was conducted with 22 professional software developers as subjects, all of whom used one of several modelling tools. The first experiment showed that applying use cases to validate class diagrams constructed from textual requirements led to more complete class diagrams than did the derivation of classes from a use case model. In the second experiment, however, we found no such difference between the two techniques. In both experiments, deriving class diagrams from the use cases led to a better structure of the class diagrams. The results of the experiments therefore show that the technique chosen for the transition from use cases to class diagrams affect the quality of the class diagrams, but also that the effects of the techniques depend on the categories of developer applying it and on the tool with which the technique is applied.},
  journal = {Empirical Software Engineering},
  volume = {10},
  number = {3},
  pages = {285-309}
}

@Article{Karahasanovic.2005.2,
  author = {Karahasanovic, Amela and Anda, Bente Cecilie Dahlum and Arisholm, Erik and Hove, Siw Elisabeth and J{\o}rgensen, Magne and Sj{\o}berg, Dag I. K and Welland, Ray},
  title = {Collecting Feedback during Software Engineering Experiments},
  year = {2005},
  abstract = {Objective: To improve the qualitative data obtained from software engineering experiments by gathering feedback during experiments. Rationale: Existing techniques for collecting quantitative and qualitative data from software engineering experiments do not provide sufficient information to validate or explain all our results. Therefore, we would like a cost-effective and unobtrusive method of collecting feedback from subjects during an experiment to augment other sources of data. Design of study: We formulated a set of qualitative questions that might be answered by collecting feedback during software engineering experiments. We then developed a tool to collect such feedback from experimental subjects. This feedback-collection tool was used in four different experiments and we evaluated the usefulness of the feedback obtained in the context of each experiment. The feedback data was triangulated with other sources of quantitative and qualitative data collected for the experiments. Results: We have demonstrated that the collection of feedback during experiments provides useful additional data to: validate the data obtained from other sources about solution times and quality of solutions; check process conformance; understand problem solving processes; identify problems with experiments; and understand subjects{\textquoteright} perception of experiments. Conclusions: Feedback collection has proved useful in four experiments and we intend to use the feedback-collection tool in a range of other experiments to further explore the cost-effectiveness and limitations of this technique. It is also necessary to carry out a systematic study to more fully understand the impact of the feedback-collecting tool on subjects{\textquoteright} performance in experiments.},
  journal = {Empirical Software Engineering},
  volume = {10},
  number = {2},
  pages = {113-147}
}

@Article{Sjoberg.2005.1,
  author = {Sj{\o}berg, Dag I. K and Hannay, Jo E and Hansen, Ove and Kampenes, Vigdis By and Karahasanovic, Amela and Liborg, Nils-Kristian and Rekdal, Anette C},
  title = {A Survey of Controlled Experiments in Software Engineering},
  year = {2005},
  abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research. },
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {9},
  pages = {733-753}
}

@Misc{Gallis.2003.3,
  author = {Gallis, Hans Enger and Arisholm, Erik and Dyb{\r a}, Tore and Sj{\o}berg, Dag},
  title = {Pair Programming versus Individual Programming on Maintenance Tasks with Professional Subjects: A Controlled Experiment},
  year = {2003},
  abstract = {},
  howpublished = {Scientific presentation to ISERN, October 4th}
}

@Misc{Sjoberg.2005.2,
  author = {Sj{\o}berg, Dag},
  title = {Use of Professionals in Experiments},
  year = {2005},
  abstract = {},
  howpublished = {Letter to Editor, IEEE Software, Sep/Oct, pp. 9-10}
}

@Misc{Sjoberg.2002.1,
  author = {Sj{\o}berg, Dag and S{\o}r{\r a}sen, Oddvar and Holden, Lars},
  title = {Nekrolog - Kristen Nygaard, Aftenposten og Uniforum 13.8.2002},
  year = {2002},
  abstract = {},
  howpublished = {Misc}
}

@Inbook{Karahasanovic.2006.2,
  author = {Karahasanovic, Amela and Fjuk, A and Sj{\o}berg, Dag and Thomas, R},
  title = {Revealing object-oriented comprehension by feedback collection},
  year = {2006},
  abstract = {},
  booktitle = {Comprehensive Object-Oriented Learning. The Learner{\textquoteright}s Perspective},
  editor = {A. Fjuk, A. Karahasanovic, J. Kaasb{\o}ll},
  publisher = {Informing Science Press},
  address = {California, USA},
  chapter = {7},
  pages = {111--130},
  isbn = {83-922337-4-3}
}

@Misc{Sjoberg.2006.1,
  author = {Sj{\o}berg, Dag I. K},
  title = {The Simula approach to experimentation in software engineering},
  year = {2006},
  abstract = {The ultimate goal of software engineering research is to support the private and public software industry in developing higher quality systems with improved timeliness in a more cost-effective and predictable way. One contribution of the empirical software engineering community to this overall goal is the conducting of experiments to evaluate and compare technologies (processes, methods, techniques, languages and tools) for planning, building and maintaining software. However, the applicability of the experimental results to industrial practice is, in most cases, hampered by the experiments{\textquoteright} lack of realism and scale regarding subjects, tasks, systems and environments. In this talk, I will discuss Simula Research Laboratory{\textquoteright}s strategy for addressing this challenge: (1) About 25\% of our budget is used for hiring software consultants as experimental subjects, mainly at the expense of employing a larger number of researchers. In the last five years, about 800 professionals from 60 companies in several countries have participated in 25 experiments (some of them very large, in order to identify the variances between sub-populations) in which the professionals worked under various controlled circumstances, such as the complexity of tasks and systems, the tools used, whether they worked in pairs, and so on. (2) A large investment in infrastructures and apparatus has been made to support the logistics of running large experiments and surveys, and to collect and organise data with minimal overhead. (3) A senior project manager has been employed to organise the experiments and the resulting data. To increase flexibility and save administrative overhead, Simula hires people on a short-term basis for assistance with, for example, particularly large or complex experiments. They could be students for clerical work or consultants who are particularly qualified for certain tasks, for example, a statistician. (4) Active collaboration with industry (in addition to hiring consultants), such as taking part in industry-managed research projects on software process improvement, and giving seminars and courses, has been considered important. The focus on publicising our research in the media and disseminating it through teaching has also resulted in Simula becoming well known in the Norwegian software industry. (5) Software engineering is typically performed by humans in organisations. Hence, we have established research collaborations with other disciplines, such as psychology, sociology and management.},
  howpublished = {Keynote address at EASE'2006}
}

@Book{Conradi.2006.3,
  editor = {Conradi, Reidar and Dyb{\r a}, Tore and Sj{\o}berg, Dag I. K and Ulsund, Tor},
  title = {Software Process Improvement: Results and Experience from the Field},
  year = {2006},
  abstract = {For over a decade, software process improvement (SPI) has been promoted as an approach to improve systematically the way software is developed and managed. Mostly this research and the relevant experience reports have been focussed on large software companies.

Conradi and his co-authors have collected the main results from four Norwegian industrial research and development projects on SPI carried out between 1996 and 2005, which, in contrast to other treatments, concentrated on small- and medium-sized companies, typically characterized by fast-changing environments and processes. The presentation is organized in five sections: general principles and methods of SPI, knowledge management for SPI, process modelling and electronic process guides, estimation methods, and object-oriented and component-based systems. A spectrum of empirical methods has been used, e.g. case studies, large-scale experiments, surveys and interviews, and action research.

The book mainly targets researchers and graduate students in (empirical) software engineering, and software professionals working in development or quality assurance
},
  publisher = {Springer},
  isbn = {3-540-32178-0}
}

@Inbook{Arisholm.2006.1,
  author = {Arisholm, Erik and Sj{\o}berg, Dag},
  title = {Evaluating the Effect of a Delegated versus Centralized Control Style on the Maintainability of Object-Oriented Software},
  year = {2006},
  abstract = {},
  booktitle = {Software Process Improvement: Results and Experience from the Field},
  editor = {Conradi, Dyb{\r a}, Sj{\o}berg \& Ulsund},
  publisher = {Springer},
  chapter = {20},
  pages = {379-406}
}

@Inbook{Sjoberg.2003.1,
  author = {Sj{\o}berg, Dag I. K and Anda, Bente Cecilie Dahlum and Arisholm, Erik and Dyb{\r a}, Tore and J{\o}rgensen, Magne and Karahasanovic, Amela and Vok{\a\'a}c, Marek},
  title = {Challenges and Recommendations When Increasing the Realism of Controlled Software Engineering Experiments},
  year = {2003},
  abstract = {An important goal of most empirical software engineering experiments is the transfer of the research results to industrial applications. To convince industry about the validity and applicability of the results of controlled software engineering experiments, the tasks, subjects and the environments should be as realistic as practically possible. Such experiments are, however, more demanding and expensive than experiments involving students, small tasks and pen-and-paper environments. This chapter describes challenges of increasing the realism of controlled experiments and lessons learned from the experiments that have been conducted at Simula Research Laboratory. },
  booktitle = {Empirical Methods and Studies in Software Engineering: Experiences from ESERNET},
  editor = {Reidar Conradi and Alf Inge Wang},
  publisher = {Springer},
  address = {Berlin / Heidelberg},
  series = {Lecture Notes in Computer Science, Volume 2765 },
  chapter = {Part II: Method Chapters},
  pages = {24--38},
  isbn = {978-3-540-40672-3}
}

@Article{Arisholm.2006.2,
  author = {Arisholm, Erik and Gallis, Hans Enger and Dyb{\r a}, Tore and Sj{\o}berg, Dag I. K},
  title = {Evaluating Pair Programming with Respect to System Complexity and Programmer Expertise},
  year = {2007},
  abstract = {A total of 295 junior, intermediate and senior professional Java consultants (99 individuals and 98 pairs) from 29 international consultancy companies in Norway, Sweden and the UK were hired for one day to participate in a controlled experiment on pair programming. The subjects used professional Java tools to perform several change tasks on two alternative Java systems with different degrees of complexity.
The results of this experiment do not support the hypotheses that pair programming in general reduces the time required to solve the tasks correctly or increases the proportion of correct solutions. On the other hand, there is a significant 84 percent increase in effort to perform the tasks correctly. However, on the more complex system, the pair programmers had a 48 percent increase in the proportion of correct solutions, but no significant differences in the time taken to solve the tasks correctly. For the simpler system, there was a 20 percent decrease in time taken but no significant differences in correctness. However, the moderating effect of system complexity depends on the expertise of the subjects. The observed benefits of pair programming in terms of correctness on the complex system apply mainly to juniors, whereas the reductions in duration to perform the tasks correctly on the simple system apply mainly to intermediates and seniors. 
},
  journal = {IEEE Transactions on Software Engineering},
  volume = {33},
  number = {2},
  pages = {65-86}
}

@Article{Dyba.2006.1,
  author = {Dyb{\r a}, Tore and Kampenes, Vigdis By and Sj{\o}berg, Dag I. K},
  title = {A systematic review of statistical power in software engineering experiments},
  year = {2006},
  abstract = {Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993-2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study{\textquoteright}s findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.},
  journal = {Journal of Information \& Software Technology},
  volume = {48},
  number = {8},
  pages = {745-755}
}

@Article{Hannay.2006.1,
  author = {Hannay, Jo E and Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore},
  title = {A Systematic Review of Theory Use in Software Engineering Experiments},
  year = {2007},
  abstract = {Empirically-based theories are generally perceived as foundational to
science. However, in many disciplines, the nature, role and even the
necessity of theories remain matters for debate, particularly in young or
practical disciplines such as software engineering.  This article
reports a systematic review of the explicit use of theory in a comprehensive
set of 103 articles reporting controlled experiments, from of a total
of 5,453 articles published in major software engineering journals and
conferences in the decade 1993-2002.  Of the 103 articles, 24 use a
total of 39 theories in various ways to explain the cause-effect
relationship(s) under investigation. The majority of these use theory in the
experimental design to justify research questions and hypotheses;
some use theory to provide \emph{post-hoc} explanations of their results;
while a few test or modify theory. A third of the theories are
proposed by authors of the reviewed articles. The interdisciplinary nature
of the theories used is greater than that of research in software
engineering in general. We found that theory use and awareness of
theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering.  Several
articles comment explicitly on the lack of relevant theory. We
call for an increased awareness of the potential benefits of involving
theory, when feasible. To support software engineering researchers who
wish to use theory, we give an overview that shows which of the reviewed
articles on which topics use which theories for what purposes, as well
as details of the theories' characteristics.},
  journal = {IEEE Transactions on Software Engineering},
  volume = {33},
  number = {2},
  pages = {87--107}
}

@Inbook{Jorgensen.2006.9,
  author = {J{\o}rgensen, Magne and Sj{\o}berg, Dag},
  title = {Expert Estimation of Software Development Work},
  year = {2006},
  abstract = {},
  booktitle = {Software Evolution and Feedback: Theory and Practice},
  editor = {Nazim H.Madhavji, Juan Fernandez-Ramli, Dewayne Perry},
  publisher = {Wiley, ISBN-0-470-87180-6},
  chapter = {25},
  pages = {489-503}
}

@Article{Basili.2006.1,
  author = {Basili, Victor and Zelkowitz, Marvin and Sj{\o}berg, Dag I. K and Johnson, Philip and Cowling, Tony},
  title = {Protocols in the use of Empirical Software Engineering Artifacts},
  year = {2007},
  abstract = {If empirical software engineering is to grow as a valid scientific endeavor, the
ability to acquire, use, share, and compare data collected from a variety of sources must be
encouraged. This is necessary to validate the formal models being developed within
computer science. However, within the empirical software engineering community this has
not been easily accomplished. This paper analyses experiences from a number of projects,
and defines the issues, which include the following: (1) How should data, testbeds, and
artifacts be shared? (2) What limits should be placed on who can use them and how? How
does one limit potential misuse? (3) What is the appropriate way to credit the organization
and individual that spent the effort collecting the data, developing the testbed, and building
the artifact? (4) Once shared, who owns the evolved asset? As a solution to these issues, the
paper proposes a framework for an empirical software engineering artifact agreement. Such
an agreement is intended to address the needs for both creator and user of such artifacts and
should foster a market in making available and using such artifacts. If this framework for sharing software engineering artifacts is commonly accepted, it should encourage artifact
owners to make the artifacts accessible to others (gaining credit is more likely and misuse is
less likely). It may be easier for other researchers to request artifacts since there will be a
well-defined protocol for how to deal with relevant matters.},
  journal = {Journal of Empirical Software Engineering},
  volume = {12},
  number = {1},
  pages = {107{\textendash}119}
}

@Article{Krogstie.2006.1,
  author = {Krogstie, John and Jahr, Arthur and Sj{\o}berg, Dag I. K},
  title = {A Longitudinal Study of General and Functional  Maintenance in Norway},
  year = {2006},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {48},
  number = {11},
  pages = {993-1005}
}

@Article{Karahasanovic.2006.5,
  author = {Karahasanovic, Amela and Hinkel, Unni Nyhamar and Sj{\o}berg, Dag I. K and Thomas, Richard},
  title = {Comparing of Feedback Collection and Think-Aloud Methods in Program Comprehension Studies},
  year = {2007},
  abstract = {This paper reports an explorative experimental comparison of (i) an experience-sampling method called feedback collection and (ii) the think-aloud methods with respect to their usefulness in studies on program comprehension. Think-aloud methods are widely used in studies of cognitive processes, including program comprehension. Alternatively, as in the feedback-collection method (FCM), cognitive processes can be traced by collecting written feedback from the subjects at regular intervals. We compare FCM with concurrent think-aloud (CTA) and retrospective think-aloud (RTA) regarding type and usefulness of the collected information, costs related to analysis of the collected information, and effects of the data collection methods on the subjects{\textquoteright} performance. FCM allowed us to identify a greater number of comprehension problems that prevented progress or caused significant delay (FCM: 30 problems; CTA: 5; RTA: 15). It was less precise in identifying strategies for comprehension than CTA (92\% correctness for FCM; 100\% for CTA). FCM was less expensive in analysis (transcription and coding) than the other two methods (FCM: 0,7 hours of analysis per protocol; CTA: 31 hours; RTA: 7,9 hours). The results indicate that all three methods of data collection were intrusive and affected the performance of the subjects with respect to time and correctness (small to medium effect size). This research confirms that FCM can be used beneficially in studies that trace the cognitive processes involved in, and identify problems related to, the comprehension of software applications. Based on our experience, we recommend that FCM be used in studies that have a large number of subjects and as a complement to other methods for tracing cognitive processes, such as user log files. We recommend a design with two groups (verbalization and silent control) and a pretest task to be used in studies with FCM or CTA that focus on performances. },
  journal = {Journal of Behaviour \& Information Technology}
}

@Article{Kampenes.2006.1,
  author = {Kampenes, Vigdis By and Dyb{\r a}, Tore and Hannay, Jo Erskine and Sj{\o}berg, Dag I. K},
  title = {A Systematic Review of Effect Size in Software Engineering Experiments},
  year = {2007},
  abstract = {},
  journal = {Information and Software Technology},
  volume = {49},
  number = {11-12},
  pages = {1073-1086}
}

@Book{Shull.2007.1,
  editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K},
  title = {Advanced Topics in Empirical Software Engineering},
  year = {2008},
  abstract = {Empirical studies have become an integral element of software engineering
research and practice. This unique text/reference includes chapters from
some of the top international empirical software engineering researchers and
focuses on the practical knowledge necessary for conducting, reporting and
using empirical methods in software engineering.},
  publisher = {Springer-Verlag London},
  isbn = {13:978-1-84800-043-8}
}

@Inbook{Sjoberg.2007.1,
  author = {Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore and Anda, Bente Cecilie Dahlum and Hannay, Jo Erskine},
  title = {Building Theories in Software Engineering},
  year = {2008},
  abstract = {Empirical studies have become an integral element of software engineering
research and practice. This unique text/reference includes chapters from
some of the top international empirical software engineering researchers and
focuses on the practical knowledge necessary for conducting, reporting and
using empirical methods in software engineering.},
  booktitle = {Advanced Topics in Empirical Software Engineering},
  editor = {Forrest Shull, Janice Singer, Dag I.K. Sj{\o}berg},
  publisher = {Springer-Verlag London},
  isbn = {13:978-1-84800-043-8}
}

@Misc{Sjoberg.2002.2,
  author = {Sj{\o}berg, Dag},
  title = {Obituary - Kristen Nygaard},
  year = {2002},
  abstract = {}
}

@Inbook{Simula.SE.10,
  author = {Sj{\o}berg, Dag I. K},
  title = {Documenting Theories},
  year = {2007},
  abstract = {},
  booktitle = {Empirical Software Engineering Issues: Critical Assessment and Future Directions},
  editor = {Basili, V.R., Rombach, D., Schneider, K., Kitchenham, B., Pfahl, D. and Selby, R.},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  series = {LNCS 4336},
  pages = {111-114},
  isbn = {0302-9743}
}

@Inbook{Simula.SE.11,
  author = {Sj{\o}berg, Dag I. K},
  title = {Knowledge Acquisition in Software Engineering Requires Sharing of Data and Artifacts},
  year = {2007},
  abstract = {},
  booktitle = {Empirical Software Engineering Issues: Critical Assessment and Future Directions},
  editor = {Basili, V.R., Rombach, D., Schneider, K., Kitchenham, B., Pfahl, D. and Selby, R.},
  publisher = {Springer-Verlag},
  address = {Berlin Heidelberg},
  series = {LNCS 4336},
  pages = {77-82},
  isbn = {0302-9743}
}

@Inproceedings{Simula.SE.13,
  author = {Sj{\o}berg, Dag I. K and Dyb{\r a}, Tore and J{\o}rgensen, Magne},
  title = {The Future of Empirical Methods in Software Engineering Research},
  year = {2007},
  abstract = {We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research. },
  booktitle = {Future of Software Engineering (FOSE '07)},
  editor = {Briand L. and Wolf A.},
  pages = {358-378},
  publisher = {IEEE-CS Press},
  isbn = {432663526}
}

@Article{Simula.SE.305,
  author = {Hannay, Jo and Arisholm, Erik and Engvik, Harald and Sj{\o}berg, Dag},
  title = {Effects of Personality on Pair Programming},
  year = {2008},
  abstract = {Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting programming and team performance in software engineering. However, research suggests that other human-related factors, such as motivation, general mental ability, expertise and task complexity also affect performance in general. This paper reports on a study of the impact of Big-Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals from three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part, and the results show that (1) our data does not confirm a meta-analysis-based model of the impact of certain personality traits on performance, and (2) personality traits in general have modest predictive value on pair programming performance compared with expertise, task complexity, and even country. Rather than focusing on direct effects of personality on pair programming performance, we conclude that (a) effort should be spent on elaborating on personality{\textquoteright}s (and other factors{\textquoteright}) indirect effects on performancemediated by formalized scales of collaboration, and (b) more effort should be spent on investigating other performance-related predictors such as programming skill, learning, motivation, expertise, and task complexity.},
  journal = {Transaction on Software Engineering}
}

@Article{Simula.SE.195,
  author = {Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag and Mockus, Audris},
  title = {Variability and Reproducibility in Software Engineering: A Study of four Companies that Developed the same System},
  year = {2008},
  abstract = {The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. However, reproducibility in software engineering (SE) has not been thoroughly investigated, although generalizing the results of SE studies depend on SE phenomena being reproducible. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirements specification. In a call for tender to 81 consultancy companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, {\textquotedblleft}low{\textquotedblright}, {\textquotedblleft}low{\textquotedblright}, and {\textquotedblleft}medium{\textquotedblright} reproducibility. The contractor{\textquoteright}s internal costs, actual lead time, and schedule overrun of the projects had, respectively, {\textquotedblleft}medium{\textquotedblright}, {\textquotedblleft}high{\textquotedblright}, and {\textquotedblleft}low{\textquotedblright} reproducibility. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, {\textquotedblleft}low{\textquotedblright}, {\textquotedblleft}high{\textquotedblright}, and {\textquotedblleft}low{\textquotedblright} reproducibility. We proposed a coarse-grained model that uses software process inputs to predict key project outcomes. The comparisons of the model{\textquoteright}s predictions with the actual outcomes indicate some reproducibility. This initial work may contribute to developing more accurate models, but making SE more reproducible remains a great challenge for SE research, education, and industry.},
  journal = {IEEE Transactions on Software Engineering}
}

@Article{Simula.SE.197,
  author = {F{\o}lstad, Asbj{\o}rn and Anda, Bente Cecilie Dahlum and Sj{\o}berg, Dag},
  title = {Usability Inspection: A Comparative Study of the Evaluator Performance of Work-Domain Experts vs. Usability Experts},
  year = {2008},
  abstract = {When applications that are tailored to work domains of which usability experts have little knowledge are evaluated, the usefulness of usability inspection methods is challenged. To counter this challenge, usability inspections with work-domain experts have been explored, but no empirical research has been reported on such experts{\textquoteright} performance as evaluators. The present study compares the performance of work-domain experts and usability experts, with respect to the validity and thoroughness of their evaluations. Fifteen work-domain experts and 12 usability experts participated. The applied inspection method was group-based expert walkthrough. The basis for comparison was user test results. The work-domain experts produced equally valid but less thorough evaluation results than the usability experts. On average, two work-domain experts produced results of similar quality to those of one usability expert, which implies that work-domain experts may substitute for usability experts if the number of evaluators is increased. These results should be taken into account when developing future usability inspection methods with work-domain experts as evaluators.},
  journal = {a journal}
}

@Article{Simula.SE.124,
  author = {Kampenes, Vigdis By and Dyb{\r a}, Tore and Hannay, Jo Erskine and Sj{\o}berg, Dag I. K},
  title = {A systematic review of quasi-experiments in software engineering},
  year = {2008},
  abstract = {},
  journal = {Information \& Software Technology}
}
